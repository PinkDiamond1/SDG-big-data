# Geocode

The first step to analyse GPS data is to geocode the individual information units. In this case those units are pings
which are packages of metadata sent from a device to the Veraset servers. These pings are the ones carrying the
geographical position of the device when in use and are sent every given a predifined time interval while the
partner app is open.

## Geospatial Indexing

An important step when working with geospatial data (specially at big data levels) is indexing, this helps
reduce the computations by only querying nearby points and speeding them up as a result. This type of indices are usually
hierarchical which allows to browse through different query radii optimally. We use the [H3](https://eng.uber.com/h3/) indexing given by Uber
which partitions the World in hexagons (and some pentagons in the ocean) at different sizes.

```{r nice-fig, fig.cap='H3 indexing representation. Figure taken from the H3 website referenced above.', out.width='80%', fig.asp=.75, fig.align='center', echo=FALSE}
img <- png::readPNG("figures/h3.png")
grid::grid.raster(img)
```

## Code

The following code is a Scala solution for the geocoding process of the pings.

### Loading administrative units

We first load the administrative files containing the polygons and the wealth index information, then using *Apache Sedona* we
can transform the Well-Known Text representation of the polygons (WKT) into geometries and create a buffer around them.

```{scala eval=FALSE}
var admin = spark.read.option("header","true").csv(c("admin_path") + "/" + c("country") + "/admin.csv")
admin.createOrReplaceTempView("admin")


var query = "SELECT geom_id AS geom_id, ST_GeomFromText(geometry) as polygon FROM admin"
admin = spark.sql(query)
admin.createOrReplaceTempView("admin_with_polygons")


query = "SELECT *, ST_Buffer(polygon, 0.005) as polygon_buff FROM admin_with_polygons"
admin = spark.sql(query)
```

### Indexing and Geocoding

We will also be needing the ping data from *Veraset*. In our case we'll load them from the following table depending the country.

```{scala eval=FALSE}
val table = s"pings.${c("source")}_${c("country")}"
```

Then we're able to proceed indexing the pings and the buffered geometries.
In order to do this we only take those pings that have correct longitude and latitude
values and threshold the accuracy (provided in the data) of the GPS point location. That last part is a confidence
estimate of the real location of the device, we can usually see this in our phone when using a geolocation app which
gives you information such as "accuracy < 3m" which means that the actual position could be anywhere in a 3m radius circle with
center at the position given.

```{scala eval=FALSE}
var pings = spark.sql(s"SELECT lat, lon, accuracy, timestamp AS time, device_id AS user_id FROM ${c("input_table")} WHERE country = '${c("country")}'")
pings.filter($"lat" > -90 && $"lat" < 90 && $"lon" > -180 && $"lon" < 180 && $"accuracy" >= 0 && $"accuracy" <= 1000)
if (c("country") == "CO"){
  pings = pings.withColumn("offset", lit(-5*60*60)) //colombia offset
  pings.printSchema()
}

val res = 10
pings = pings.withColumn("time", col("time") + col("offset")).withColumn("h3index", geoToH3(col("lat"), col("lon"), lit(res)))

val adminH3 = admin.withColumn("h3index", multiPolygonToH3(col("polygon_buff"), lit(res))).select("geom_id", "h3index").withColumn("h3index", explode($"h3index"))

pings.createOrReplaceTempView("pingsH3")
adminH3.createOrReplaceTempView("adminH3")
```

