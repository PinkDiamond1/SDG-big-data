# Mobility Patterns {#mobility}
Individual mobility analysis represent a key ingredient to understand and timely follow changes in their behavioural patterns. In this book we relie on heavily tested and robust GPS traces preprocessing to measure and quantify individual mobility patterns.

## Socio-economic groups
Particular attention is devoted to different socio-economig groups. Groups are assigned to individuals based on the administrative unit where their principal pre-pandemic home-location is detected. Three main socio-economic groups are analysed here: the 40% with lowest index value (the 0% to 40% administrative units with lowest wealth index value), the middle ones (from 40% to 80%), and the top 20% (the wealthiest 80 to 100 percentiles). 
Socio-economic groups are drawn both by considering the entire country wealth index percentiles and by considering percentiles relatively to the metropolitan area administrative units are part of.

The computation is performed by loading and grouping administrative information "admin.csv" files for each country using pyspark.


```{python eval=FALSE}
def process_admin(country,admin_path):
  '''
  INPUT:  country ISO code, path to the admin files (saved as "{country}/admin.csv" files)
  OUTPUT: three pandas dataframes
          - "admins_by_country": all country administrative units with socio-economic group assigned based on entire country 
          - "admins_by_metro_area": wealth groups computed relatively to the emtropolitan area of each unit
          - total population of the metropolitan areas
  '''
  cols = ['geom_id', 'metro_area_name', 'pop', 'wealth_index']
  admin = spark.read.option('header', 'true').csv(admin_path+f'{country}/admin.csv').toPandas()
  admins = admins[[cols]]

  admins = admins.rename(columns={'metro_ar_1': 'metro_area_name', 'wealth_ind': 'wealth_index'})

  admins_by_country = admins[['geom_id', 'pop', 'wealth_index']].dropna(
  ).sort_values(by=['wealth_index'], ascending=[False]).reset_index(drop=True)
  admins_by_country['pct_wealth'] = admins_by_country['pop'].cumsum().divide(
      admins_by_country['pop'].sum())

  admins_by_metro_area = admins[['geom_id', 'metro_area_name', 'pop', 'wealth_index']].dropna(
  ).sort_values(by=['metro_area_name', 'wealth_index'], ascending=[True, False]).reset_index(drop=True)
  admins_by_metro_area['pct_wealth'] = admins_by_metro_area.groupby(
      'metro_area_name')['pop'].apply(lambda x: x.cumsum()/x.sum())

  pop_metro_areas = admins_by_metro_area.groupby(
      'metro_area_name')['pop'].sum().sort_values(ascending=False)

  return admins_by_country, admins_by_metro_area, pop_metro_areas
```

## Individual's selection
Following the preprocessing pipeline description presented in the previous sections of the book, here we introduce an additional step aiming at refinining the pool of individuals analysed. Our intent is to measure mobility patterns only for individuals whose activity level is sufficiently high to ensure a minimum level of representativeness of the data. Individuals with recods over only a small number of days cannot be considered as a complete and correct representation fairly proxying the mobility features of an individual.

To this extent we use the duration of each individuals records, which have been computed during the pipeline run.
The selection imposes two requirements on individuals: i) each individual should be active (i.e. should have at least one stop record) for a minimum number of days during the pre-pandemic period (which is choose to run from the beginning of January 2020 until March 15, 2020; ii) each individual should be active for a minimum fraction of days after the pre-pandemic period until the end of 2020. This selection process is obtained invoking the following python function which returns a list of the users to be included during the analyses.

```{python eval=FALSE}
def get_active_list(durations, country, activity_level):
  '''
  For each country invoke the following function to get a list of all active individuals.
    INPUT: spark dataframe with precomputed individual stops' durations; country ISO code; minimum activity level required
    OUTPUT: list of active individuals "user_id"
  '''
  
  # Indonesia experiences a major dropout from the service during January 2020. For this reason, a specific pre-pandemic period was adopted
  if country == 'ID':
      durations_2 = durations.where(col('date_trunc') >= '2020-02-01')
  else:
      durations_2 = durations
  durations_2 = durations_2.where(col('date_trunc') < '2021-01-01')

  active_days = (durations_2
                 .withColumn('pandemic', F.when(col('date_trunc') < '2020-03-15', 'pre').otherwise('post'))
                 .groupby('user_id', 'pandemic')
                 .agg(F.countDistinct('date_trunc').alias('n_days')))
  active_days.cache()

  max_days_pre = (active_days
                  .where(col('pandemic') == 'pre')
                  .agg(F.max('n_days').alias('max_days_pre'))
                  .toPandas().loc[0, 'max_days_pre'])

  max_days_all = (active_days
                  .groupby('user_id')
                  .agg(F.sum('n_days').alias('n_days'))
                  .agg(F.max('n_days').alias('max_days_all'))
                  .toPandas().loc[0, 'max_days_all'])

  active_users = (active_days
                  .groupby('user_id')
                  .pivot('pandemic')
                  .agg(F.first('n_days'))
                  .fillna(0)
                  .withColumn('tot', col('pre')+col('post'))
                  .where(col('pre') >= activity_level*max_days_pre)
                  .where(col('tot') >= activity_level*max_days_all))
  active_days.unpersist()
  
  return active_users
```

## Measures
The focus on socio-economic groups require an aggregation process which is performed for each individuals based on their home-location administrative unit. Both stop-locations and duration data processed during the pipeline run are used here.
```{python eval=FALSE}
def compute_durations_and_admins(country, data_date, stop_path, activity_level=0, hw=28, ww=28, wa=900, mph=10, mpw=7):
  '''
  INPUT: country ISO code, date of the data to consider, path to the stop locations data,  
         minimum activity level for individuals to be included, **{pipeline computation specific parameters}
  OUTPUT: spark dataframe containing "date" (with daily frequency), "user_id", "H" (time spent at home), "W" (time spent at work), 
          "R" (1 if not leaving home), "C" (1 if commuting to work), "O" (time spent in locations which are neither home nor work)
  '''
  personal_nf = f"personal_stop_location_hw{hw}_ww{ww}_wa{wa}_mph{mph}_mpw{mpw}"
  stops = spark.read.parquet(f"{stop_path}{country}/accuracy100_maxtimestop3600_staytime300_radius50_dbscanradius50/date{data_date}/" + personal_nf)

  fname_nf = f'durations_window_hw{hw}_ww{ww}_wa{wa}_mph{mph}_mpw{mpw}'
  durations_path_nf = f'{stop_path}{country}/accuracy100_maxtimestop3600_staytime300_radius50_dbscanradius50/date{data_date}/' + fname_nf
  durations = spark.read.parquet(durations_path_nf)

  # aggregate day/night
  durations = (durations
               .groupby('date_trunc', 'user_id')
               .agg(F.sum('H').alias('H'),
                    F.sum('W').alias('W'),
                    F.sum('O').alias('O')))

  active_users = get_active_list(durations, country, activity_level)

  durations = durations.join(active_users.select(
      'user_id'), on='user_id', how='inner')

  # create binary column for commuters
  durations = durations.withColumn(
      'C', F.when(col('W').isNull(), 0).otherwise(1))

  # create binary column for people who don't leave home, aka recluse
  durations = durations.withColumn('R', F.when(
      (col('W').isNull()) & (col('O').isNull()), 1).otherwise(0))

  # compute H and W id for wealth labels
  w = Window.partitionBy('user_id')
  user_h_id = (stops
               .where(col('location_type') == 'H')
               .where(col('date_trunc') <= '2020-03-15')
               .groupby('user_id', 'geom_id')
               .agg(F.countDistinct('date_trunc').alias('n_days'))
               .withColumn('max_days', F.max('n_days').over(w))
               .where(col('n_days') == col('max_days'))
               .groupby('user_id')
               .agg(F.first('geom_id').alias('geom_id_home')))
  user_w_id = (stops
               .where(col('location_type') == 'W')
               # .where(col('date_trunc') <= '2020-03-15')
               .groupby('user_id', 'geom_id')
               .agg(F.countDistinct('date_trunc').alias('n_days'))
               .withColumn('max_days', F.max('n_days').over(w))
               .where(col('n_days') == col('max_days'))
               .groupby('user_id')
               .agg(F.first('geom_id').alias('geom_id_work')))

  durations_and_admins = (durations
                          .withColumnRenamed('date_trunc', 'date')
                          .select('date', 'user_id', 'H', 'R', 'W', 'C', 'O')
                          .join(user_h_id, on='user_id', how='left')
                          .join(user_w_id, on='user_id', how='left'))

  return durations_and_admins
```

The aggregation process takes the output of "compute_durations_and_admins" and compute the weighted average of the different metrics of interest.
Before focusing on the aggregated metrics produced, it is important to highlight that during this process metrics are reweighted based on the fraction of population living in administrative units in which at least one individual in our dataset is living. This step rebalance the data satial-coverage biases giving more importance to individuals living in administrative units with larger population.

```{python eval=FALSE}
def compute_durations_normalized_by_wealth_home(durations_and_admins, admins, labels_wealth, bins_wealth):
  '''
    INPUT:  df output of "compute_durations_and_admins", df (first) output of "process_admins", wealth labels, wealth bins
    OUTPUT: df with weight column dataframe
  '''
  admins['wealth_label'] = pd.cut(
      admins['pct_wealth'], bins_wealth, labels=labels_wealth)
  admins['geom_id'] = admins['geom_id'].astype(str)
  admins['wealth_label'] = admins['wealth_label'].astype(str)
  # get admin info for home and work location
  tmp1 = spark.createDataFrame(
      admins[['geom_id', 'pop', 'pct_wealth', 'wealth_label']].rename(columns=lambda x: x+'_home'))
  out1 = (durations_and_admins
          .join(tmp1, on='geom_id_home', how='inner'))

  geom_users = (out1
                .groupby('geom_id_home')
                .agg(F.countDistinct('user_id').alias('n_users')))

  out = (out1
         .join(geom_users, on='geom_id_home', how='inner')
         .withColumn('weight', col('pop_home')/col('n_users')))
  return out


def output(out, column):
  '''
    INPUT:  df output of "compute_durations_normalized_by_wealth_home", measure to aggregate
    OUTPUT: aggregated and normalized dataframe
  '''
  # compute aggregate measures
  out = (out
         .fillna(0, subset=column)
         .groupby('date', 'wealth_label_home')
         .agg((F.sum(col(column)*col('weight'))/F.sum(col('weight'))).alias('mean'),
              F.stddev(column).alias('std'),
              F.count(column).alias('n'),
              F.countDistinct('user_id').alias('n_unique'))
         .withColumn('sem', col('std')/F.sqrt(col('n')))
         .drop('std'))

  durations_normalized_by_wealth_home = out.toPandas(
  ).set_index(['wealth_label_home', 'date'])
  return durations_normalized_by_wealth_home
```

### Measuring mobility patterns
We can now go back and discuss the metrics coded so far which we selected as proxies of different aspect of individuals mobility behaviour.
"H" (time spent at home"), "W" (time spent at work), "O" (time spent in locations which are neither home nor work) are quantities directly computed as the weighted average of the time spent in three diferent location categories, namely, the home-locations of an individuals, its work locations, and all the other locations which are neither home nor work. Additionally, we also compute the fraction of people not leaving their home-location on an entire day, "R", and the fraction of commuters going to their workplace on a specific date, "C". All these quantities are computed and aggregated following the wealth bins provided (e.g. "low-income": 0-40% wealth-index percentile,  "middle income": 40-80% wealth-index percentile,  "high-income": 80-100% wealth-index percentile). To further specialize on most fragile individuals' groups, we focus our attention on i) the "low-income" individuals and their fraction of commuters, and ii) the "low-income" individuals and thir time spent at their work places. These two quantities aims at providing in-dept insights over the working behaviour and its changes during the pandeimc.


Here we report an example code using the functions presented in the previous sections and the data computed by running the preprocessing pipeline.
```{python eval=FALSE}
import os
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import numpy as np
import seaborn as sns

import pyspark.sql.functions as F
from pyspark.sql.functions import col, desc, lit
from pyspark.sql import Window
spark.conf.set("spark.sql.shuffle.partitions", 1500)

start_baseline = '2020-02-01'
end_baseline = '2020-03-01'

start_date = '2020-02-01'
end_date = '2021-04-15'

results_dir = "SAVE_RESULTS_HERE"
admin_path = "FIND_ADMIN_DATA_HERE" # country specific admin files should be stored as "{admin_path}/{country}/admin.csv"
stop_path = "FIND_PIPELINE_STOP_DATA_HERE"

# define wealth groups
bins_wealth = [0, 0.4, 0.8, 1]
bins_dist = [0, 0.4, 0.8, 1]

# select countries for which produce metric results
countries = ['ID', 'PH', 'BR', 'CO', 'MX', 'ZA']

c_dates = {'BR': '2021-05-16',
           'CO': '2021-05-16',
           'ID': '2021-05-16',
           'MX': '2021-05-16',
           'PH': '2021-05-16',
           'ZA': '2021-05-16',
           'AR': '2021-05-16'}

activity_level = 0.2

hw = 49
ww = 49
wa = 3600
mph = 0.2
mpw = 0.2

# smoothing window size
ma = 28

# weath-group names
labels_wealth = [x+' ('+str(int(bins_wealth[i]*100))+'%-'+str(int(bins_wealth[i+1]*100)) +'%)' for i, x in zip(range(len(bins_wealth)-1), ['Low', 'Medium', 'High'])]
labels_dist = [x+' ('+str(int(bins_dist[i]*100))+'%-'+str(int(bins_dist[i+1]*100)) +'%)' for i, x in zip(range(len(bins_dist)-1), ['Low', 'Medium', 'High'])]
# largest metropolis names
country_capitals = {'MX': 'Mexico City', 'BR': 'São Paulo', 'CO': 'Bogota', 'PH': 'Quezon City [Manila]', 'ID': 'Jakarta', 'ZA': 'Johannesburg', 'AR': 'GBuenos Aires'}


# initialize dictionary where to store results
results = {}
for country in countries:
    print(country)

    admins_by_country, admins_by_metro_area, pop_metro_areas = process_admin(country,admin_path)
    capital_geomid = admins_by_metro_area[admins_by_metro_area.metro_area_name == country_capitals[country]].geom_id.unique().tolist()
    durations_and_admins = compute_durations_and_admins(country, c_dates[country], stop_path, activity_level=activity_level, hw=hw, ww=ww, wa=wa, mph=mph, mpw=mpw)
    durations_and_admins.cache()
    out1 = compute_durations_normalized_by_wealth_home(durations_and_admins, admins_by_country, labels_wealth, bins_wealth)
    out2 = compute_durations_normalized_by_wealth_home_wealth_work(durations_and_admins, admins_by_country, labels_wealth, bins_wealth)
    results[(country, 't_home')] = output(out1, column='H')
    results[(country, 't_work')] = output(out1, column='W')
    results[(country, 't_other')] = output(out1, column='O')
    results[(country, 'rec')] = output(out1, column='R')
    results[(country, 'comms')] = output(out1, column='C')
    results[(country, 'comms_hw')] = output_hw(out2, column='C')
    results[(country, 't_work_hw')] = output_hw(out2, column='W')

    durations_and_admins.unpersist()
    
# save all countries results in a single .csv file
res2 = pd.DataFrame(columns=['state', 'measure',
                             'wealth_label_home', 'date', 'mean', 'sem'])
for key, res in results.items():
    res_tmp = res.reset_index().copy()
    res_tmp['state'], res_tmp['measure'] = key
    res2 = res2.append(res_tmp, ignore_index=True)

res2.to_csv(results_dir+'all_hw_weighted.csv')
```

### Measuring change
It is of particular interest, due to the dramatic societal impact the pandemic had at a global scale, to study these quantities in relation to the pre-pandemic periods. A comparison bethween individuals mobility behaviour from the months preceding the explosive diffusion of the SARS-ncov virus represent an important indicator of how much each socio-economic group was affected by the local regulations and how individuals were forced to reshape their daily mobility patterns in response to the pandemic threat. To this extent, we implemented a change metric which directly compare the levels of the metrics discussed in terms of the period going from the beginning of January 2020 and March 15, 2020. Change is computed on a daily basis, comparing the level of a specific date with the day-of-the-week median value of the same metric. This accounts for weekly periodical effects, accounting, for example, for the physiological reduction in the number of commuters during weekends. 


```{python eval=FALSE}
def google_change_metric(df_original, start_baseline, end_baseline,other_groups=[]):
  '''
  INPUT:  dataframe with (at least) 2 columns named "mean" and "sem"
  OUTPUT: dataframe with the values of the two columns converted to google change metric
  NOTE: Google uses as baseline period the 5-weeks period from Jan 3 to Feb 6
  '''
  df = df_original.copy()
  
  # compute weekday baseline values
  baseline = df.loc[start_baseline:end_baseline,['mean','sem']+other_groups].copy()
  baseline['weekday'] = list(baseline.index.dayofweek.values)
  baseline = baseline.groupby(['weekday']+other_groups,dropna=False,as_index=False).mean()
  df['weekday'] = list(df.index.dayofweek.values)

  date = df.index.copy()
  df = df.merge(baseline, on=['weekday']+other_groups, how='left', =('', '_baseline'))
  
  # compute "mean" change with respect to weekday baseline values
  df['mean'] = (df['mean']- df['mean_baseline']) / np.abs(df['mean_baseline'])
  df['sem'] = np.abs(df['sem']/df['mean_baseline'])
  df.index = date
  
  # return input dataframe with "mean" and "sem" column now expressing the relative change and its error
  return df.drop(['weekday','mean_baseline'],axis=1,errors='ignore')
```


### Home-isolated, commuters and low-wealth commuters
In this section we present a summary of the results for three different metrics computed for six different countries. Countries are selected among middle-low income countries to include at least one state on three different continents: America, Asia and Africa. Similar pattern changes are found between socio-economics groups across the globe.

```{r nice-fig, fig.cap='Results for six different countries (rows): Brazil, Colombia, Indonesia, Mexico, Philippines, and South Africa. Measuring (columns): relative change in the fraction of people not leaving home, relative change of the number of people commuting from home to workplace, relative change of individuals from living in low-wealth admin units. The three coloured lines distinguishes the metric changes for the three different socio-economic groups.', out.width='80%', fig.asp=.75, fig.align='center', echo=FALSE}
# img <- png::readPNG("figures/mobility_figure.png")
# grid::grid.raster(img)
```
