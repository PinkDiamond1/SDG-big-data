## Training data preparation

### Sampling

The first step in order to train classifiers to detect disclosures of labor market situation is to sample informative tweets and have them labelled. Owing to the very low share of tweets containing these disclosures, a random sampling would yield very few positive examples which would not allow to train a good-performing classifier. We instead decided to opt for stratified sampling, namely by defining a list of n-grams, both specific to the labor market context and frequent enough, and sampling tweets containing these n-grams.

In practice, we wrote code in PySpark as we had to handle big amounts of data. Here's a snippet of the code we used: after loading the data in the dataframe `df` and having `text_lowercase` as the column where the lowercased text is stored, we define the list of ngrams to sample from and then sample from it.

```{python eval=FALSE}
ngram_list = [[' i ', 'fired '], ['fired me'], ['laid off'], ['lost my job']]
for ngram in ngram_list:
    if len(ngram) == 1:
        df_ngram = df.filter(df.text_lowercase.contains(ngram[0]))
    elif len(ngram) == 2:
        regex = f"{ngram[0]}[.\w\s\d]*{ngram[1]}"
        df_ngram = df.filter(df.text_lowercase.rlike(regex))
    share = min(float(150 / df_ngram.count()), 1.0)
    df_ngram_sample = df_ngram.sample(False, share, seed=0)
```
We sample 150 tweets per n-gram for each class and language. In total, we end up with approximately 5000 samples for each language. A detailed list of n-grams for each language and class can be found in Tonneau et al. (2021).

### Labelling

After sampling informative tweets, we have them labelled by crowdworkers. We use the crowdsourcing platform Amazon Mechanical Turk. This platform has the advantage of having an international workforce speaking several languages, including Spanish and Brazilian Portuguese on top of English.

#### Creating a Qualtrics survey

The first step was to create a survey we would then send to crowdworkers. For this, we use Qualtrics which implies having a Qualtrics API token and a datacenter ID. We build beforehand a survey template one can create manually on Qualtrics that we can then load questions from for the different labelling sessions.

With this information, we are able to create a new blank survey for which we can define the name `SurveyName` and the `language`:

```{python eval=FALSE}
def create_survey(SurveyName, apiToken, dataCenter, language):
    baseUrl = "https://{0}.qualtrics.com/API/v3/survey-definitions".format(
        dataCenter)

    headers = {
        "x-api-token": apiToken,
        "content-type": "application/json",
        "Accept": "application/json"
    }

    data = {
        "SurveyName": SurveyName,
        "Language": language,
        "ProjectCategory": "CORE"
    }

    response = requests.post(baseUrl, json=data, headers=headers)

    if json.loads(response.text)["meta"]["httpStatus"] != '200 - OK':
        print(json.loads(response.text)["meta"]["httpStatus"])

    SurveyID = json.loads(response.text)['result']['SurveyID']
    DefaultBlockID = json.loads(response.text)['result']['DefaultBlockID']

    return SurveyID, DefaultBlockID
```
We can then retrieve questions from our template, by specifying the `QuestionID` from the question we want to retrieve and the `SurveyID` from the template.

```{python eval=FALSE}
def get_question(QuestionID, SurveyID, apiToken, dataCenter):
    baseUrl = "https://{0}.qualtrics.com/API/v3/survey-definitions/{1}/questions/{2}".format(
        dataCenter, SurveyID, QuestionID)

    headers = {
        "x-api-token": apiToken,
    }

    response = requests.get(baseUrl, headers=headers)

    if json.loads(response.text)["meta"]["httpStatus"] != '200 - OK':
        print(json.loads(response.text)["meta"]["httpStatus"])

    return json.loads(response.text)["result"]
```

The survey template contains questions that don't need to be modified, such as asking for a participant's MTurk ID. For the labelling question though, we need to update the question's text with the tweet to be labelled. We do this the following way, with `tweet` being the tweet in string format:

```{python eval=FALSE}
def update_question(QuestionData, QuestionID, SurveyID, apiToken, dataCenter):
    baseUrl = "https://{0}.qualtrics.com/API/v3/survey-definitions/{1}/questions/{2}".format(
        dataCenter, SurveyID, QuestionID)

    headers = {
        'accept': "application/json",
        'content-type': "application/json",
        "x-api-token": apiToken,
    }

    response = requests.put(baseUrl, json=QuestionData, headers=headers)

    if json.loads(response.text)["meta"]["httpStatus"] != '200 - OK':
        print(json.loads(response.text)["meta"]["httpStatus"])

QuestionData = get_question(QuestionID=QuestionID, SurveyID=SurveyID, apiToken=apiToken, dataCenter=dataCenter)
QuestionData['QuestionText'] = tweet
update_question(QuestionData=QuestionData, QuestionID=QuestionID, SurveyID=SurveyID, apiToken=apiToken,
                dataCenter=dataCenter)
```

The entire code we used to create a Qualtrics survey using Python and the Qualtrics API is available at `src/1-training_data_preparation/qualtrics/get_training_set_to_qualtrics_API_classification.py`. After loading the questions and embedding the data into them, the survey is ready to be sent to crowdworkers.

#### Sharing the survey on MTurk
To share the Qualtrics survey on MTurk, we use the Python package `boto3`. After collecting our access and secret access keys (respectively `access_key_id` and `secret_access_key`), we initiate the client:


```{python eval=FALSE}
mturk = boto3.client('mturk',
                     aws_access_key_id=access_key_id,
                     aws_secret_access_key=secret_access_key,
                     region_name='us-east-1',
                     endpoint_url='https://mturk-requester.us-east-1.amazonaws.com'
                     )
```

To create an MTurk task, usually called a HIT, we need to provide an XML file containing all of the HIT's information and content. We therefore prepare dictionaries containing specific content, such as HIT title or description, for each language. For instance, for the HIT title, we create this dictionary with `ntweets` being the number of tweets:

```{python eval=FALSE}
title_dict = {
    'US': 'Read %d English Tweets and answer a few questions' % (ntweets),
    'MX': 'Lea %d Tweets en español mexicano y responda algunas preguntas' % (ntweets),
    'BR': 'Leia %d tweets em português e responda algumas perguntas' % (ntweets)
}
```

After preparing all of these dictionaries, we can then feed them to the `question_generator` function which will load a template previously created in HTML format (`template.html`), adapt it to the related survey and prepare the content of the related XML file in string format:

```{python eval=FALSE}
def question_generator(country_code, survey_link, instructions_dict, survey_link_text_dict, worker_input_text_dict,
                       submit_dict):
    xml_wrapper_begin = """
    <HTMLQuestion xmlns="http://mechanicalturk.amazonaws.com/AWSMechanicalTurkDataSchemas/2011-11-11/HTMLQuestion.xsd">
    <HTMLContent><![CDATA[
    <!-- YOUR HTML BEGINS -->
    <!DOCTYPE html>
    """
    xml_wrapper_end = """
    <!-- YOUR HTML ENDS -->
    ]]>
    </HTMLContent>
    <FrameHeight>0</FrameHeight>
    </HTMLQuestion>
    """

    with open(
            "template.html",
            "r") as f:
        content = f.read()

    content = content.replace("${INSTRUCTIONS}", instructions_dict[country_code])
    content = content.replace("${SURVEY_LINK}", survey_link)
    content = content.replace("${SURVEY_LINK_TEXT}", survey_link_text_dict[country_code])
    content = content.replace("${WORKER_INPUT_TEXT}", worker_input_text_dict[country_code])
    content = content.replace("${SUBMIT}", submit_dict[country_code])

    return xml_wrapper_begin + content + xml_wrapper_end
```

Afterwards, when the HIT content is ready, we need to preselect the crowdworkers to make sure they live in the same country as the authors from the tweets that they will label. That way, we make sure they speak the same language but also can better understand linguistic subtleties, such as humor or specific words from the region of origin. For example, to make sure the crowdworkers working on our HIT are based in the U.S., we create this `QualificationRequirements_list`:

```{python eval=FALSE}
QualificationRequirements_list = [
    {
        'QualificationTypeId': '00000000000000000071',  # Worker_Locale
        'Comparator': 'EqualTo',
        'LocaleValues': [{
            'Country': 'US'}],
        'RequiredToPreview': True,
        'ActionsGuarded': 'PreviewAndAccept'
    }]
```

Finally, we can create the HIT. At this step, we can specify specific parameters, such as the maximum number of workers allowed to take the survey `n_workers` or the amount of money they get to complete the survey `money_for_hit`.

```{python eval=FALSE}
new_hit = mturk.create_hit(
    MaxAssignments=n_workers,
    AutoApprovalDelayInSeconds=172800,
    LifetimeInSeconds=259200,
    AssignmentDurationInSeconds=10800,
    Reward=str(money_for_hit),
    Title=f'{title_dict[args.country_code]} v{args.version_number}',
    Description=description_dict[args.country_code],
    Keywords=keywords_dict[args.country_code],
    QualificationRequirements=QualificationRequirements_list if create_hits_in_production else list(),
    Question=question
)
```

The HIT is now online and crowdworkers are able to take it. You can check the progress on Qualtrics by looking at the number of replies to a given survey. When the number of replies reaches `n_workers`, the HIT is finished. The labelled data can then be extracted from Qualtrics.









