[["index.html", "GPS Analytics Chapter 1 Introduction", " GPS Analytics GPS Analytics Group 2021-08-25 Chapter 1 Introduction In latest years new sources of data have been proven to be valuable assets to understand human behavior and thus help in the creation of public policies based on evidence. Among those lies mobile phone data which is commonly used to study spatio-temporal patterns in the form of mobility or social connections. This type of data usually comes in two flavors; Call Detail Record (CDR), which is basicaly the information generated by a telecom transaction between at least two devices, i.e. phone calls or SMS and it can be used to determine connections between users and a location proxy given the connection of the device to the nearest antenna. The second type is GPS data. In present times mobile phones, particularely smart phones, which global penetration is estimated to be almost 50%, are used for much more than Pier to Pier (P2P) transactions as they are capable of connecting to the internet through apps. There are companies that partner up with app creators in order to store information about the usage of their products, this way when used in a phone they store a token with a unique device ID and time stamp plus any extra information that the user has consent to share, such as geolocation which is obtained via the high precision GPS anthenas the phones are equiped with plus WiFi triangulation in some cases to upscale it. We are interested in the latter source of information as the aim of our study is to untangle the universal disparities, if any, in mobility reduction during the COVID-19 pandemic between socioeconomic groups. We focused our study in 6 middle-income countries (Brazil, Colombia, Indonesia, Mexico, Philippines and South Africa). A summary plot of our results is displayed in the interactive chart below. The first panel represents the percentage change of the total time spent at home by socioeconomic group, the second one is the percentage change of daily users commuting to work also by socioeconomic group and the last one shows the percentage change of work commutes in the low-income group by the wealth of the neighborhood where the work is located. A more detailed explanation of these results and the procedure to obtained them is described in the following chapters. "],["data.html", "Chapter 2 Data 2.1 GPS Data 2.2 Wealth Index Data 2.3 Administrative Boundaries Data", " Chapter 2 Data The complex task of analysing mobility data given the socioeconomic background of the user requieres multiple data sources because we need a type of data that allow us to study individual spatio-temporal behavior, another one with demographic and economic information and one to serve as a bridge to the other two. 2.1 GPS Data Anonymous raw GPS points are provided at a global scale by the Veraset data company in the framework of the World Bank “Monitoring COVID-19 policy response through human mobility data” project. Veraset Movements data consists of raw GPS point information from devices all around the World. In contrast to the majority of GPS data providers, Verasets Movements are sourced from thousand of Software Development Kit (SDK) to reduce sample biases. It features a coverage of about 5% of the global population. In our work, unique-device GPS points are processed and analysed as a proxy of individuals mobility and patterns of visits. The data at our disposal spans over a period of 17 months, starting from the start of Jan 2020 until the end of May 2021. In this work multiple countries were analysed to assess the impact of the pandemic and the policy responses across a vast spectrum of potentially different behaviours. 2.2 Wealth Index Data Aiming for universal behaviors is a hard endevour as there are cultural and economic differences between every country. This is why we need an homologated number or index that can describe the level of marginalization or depravation of goods that comprise the universal definition of poverty. In order to do this we gathered the latest census data from each country and summarized the level of education, the access to health services and household characteristics and the finnest administrative unit level available. Then we take all of these values and embed them in a new coordinate system given by a Principal component Analysis (PCA) and take the normalized value of the first componen as a proxy to wealth. This approach yields a continuous variable which we later discretize by quantiles. 2.3 Administrative Boundaries Data The country-wise geographical information is given in the form of spatial databases that represent certain areas with polygons made of vertices in a given cartographic projection, commonly represented by their longitude and latitude coordinate pair. This areas have labels that can be linked to the census data allowing us to get a spatial representation of the previously mentioned Wealth Index. This files are usually given by the Statistics or Geography Institute of each nation. Using Geographical Information Software (GIS), in this case Sedona for Spark, we are able to assign a geographical area to each one of the GPS points given by the Veraset dataset and which will allow us to determine a probable home location inside an administrative polygon to each user and assign a wealth cathegory to them. "],["geocode.html", "Chapter 3 Geocode 3.1 Efficient spatial joining and Geospatial Indexing 3.2 TODO: put an image or create one 3.3 Code 3.4 Initial coarse geo-spatial join with shapefiles 3.5 Final exact join with a subset of the shapefiles", " Chapter 3 Geocode As a first step in our pipeline we want to associate to each GPS data point (ping) the smallest administrative unit available in our shapefiles for one country. This will allow us to join the gps data with the census and the stringency index are indices based on the same shapefiles, and to group by larger administrative units (such as city or region for more general analysis. We will refer to the process of associating a ping to a shape in the county’s shapefile as geocoding. The data we are utilizing was provided by Veraset. If a user has installed one of the Veraset’s partners’ apps on his or her mobile phone, the users’ position and postion’s accuracy is sent to the Veraset server where it is recorded at regular intervals. 3.1 Efficient spatial joining and Geospatial Indexing To be able to deal with billions of points, we need an efficient way to perform spatial queries. Since it is not feasible to join directly thousands of shapes with the billions of points, for computational reasons, we need an efficient way to reduce the computational cost by checking only few shapes that are in the surroundings of the point we want to join. As geo-spatial index we use the H3 indexing created by Uber which partitions the World in hexagons (and some pentagons in the ocean) at different sizes. Since the shape-files cannot be exactly tasseled by hexagons of a size that is reasonable for a quick spatial join, we run the risk of mis-join pings that end up close to the border: since the hexagon tesselation is fixed, we might end up with points close to the borders that belongs to the hexagon of a wrong adjacent shape. 3.2 TODO: put an image or create one To overcome this problem we perform two joins: the first one will use a geospatial index, and will get the shapes that are close enough to the points we want to geocode, after buffering the shapes to be sure that any hexagon that is around the border of a shape is included in the join. The second join will be only between these subsets of shapes selected and the actual point. This greatly reduces the computational cost and makes this problem tractable. Figure 3.1: H3 indexing representation. Figure taken from the H3 website referenced above. 3.3 Code 3.3.1 Loading administrative units We first load the administrative files containing the polygons and the wealth index information, then, using Apache Sedona (previously named GeoSpark) we can transform the Well-Known Text representation of the polygons (WKT) into geometries and create a buffer around them. var admin = spark.read.option(&quot;header&quot;,&quot;true&quot;).csv(c(&quot;admin_path&quot;) + &quot;/&quot; + c(&quot;country&quot;) + &quot;/admin.csv&quot;) admin.createOrReplaceTempView(&quot;admin&quot;) var query = &quot;SELECT geom_id AS geom_id, ST_GeomFromText(geometry) as polygon FROM admin&quot; admin = spark.sql(query) admin.createOrReplaceTempView(&quot;admin_with_polygons&quot;) query = &quot;SELECT *, ST_Buffer(polygon, 0.005) as polygon_buff FROM admin_with_polygons&quot; admin = spark.sql(query) 3.3.2 Loading ping data and h3 indexing We will also be needing the ping data from Veraset. In our case we’ll load them from the following table depending on the country. val table = s&quot;pings.${c(&quot;source&quot;)}_${c(&quot;country&quot;)}&quot; Then we’re able to proceed indexing the pings and the buffered geometries. In order to do this we only take those pings that have correct longitude and latitude values and threshold the accuracy (provided in the data) of the GPS point location. That last part is a confidence estimate of the real location of the device, we can usually see this in our phone when using a geolocation app which gives you information such as “accuracy &lt; 3m” which means that the actual position could be anywhere in a 3m radius circle with center at the position given. When reading the pings, we first filter the ones having wrong coordinates or an accuracy &gt; than 1km, and add to the spark table a column containing the county’s offset (if a country has only one timezone), since we want to compare different countries for the same time of the days, we will use this offset to align all the timestamps. We then index each buffered admin shape and ping using h3. var pings = spark.sql(s&quot;SELECT lat, lon, accuracy, timestamp AS time, device_id AS user_id FROM ${c(&quot;input_table&quot;)} WHERE country = &#39;${c(&quot;country&quot;)}&#39;&quot;) pings.filter($&quot;lat&quot; &gt; -90 &amp;&amp; $&quot;lat&quot; &lt; 90 &amp;&amp; $&quot;lon&quot; &gt; -180 &amp;&amp; $&quot;lon&quot; &lt; 180 &amp;&amp; $&quot;accuracy&quot; &gt;= 0 &amp;&amp; $&quot;accuracy&quot; &lt;= 1000) if (c(&quot;country&quot;) == &quot;CO&quot;){ pings = pings.withColumn(&quot;offset&quot;, lit(-5*60*60)) //colombia offset pings.printSchema() } val res = 10 pings = pings.withColumn(&quot;time&quot;, col(&quot;time&quot;) + col(&quot;offset&quot;)).withColumn(&quot;h3index&quot;, geoToH3(col(&quot;lat&quot;), col(&quot;lon&quot;), lit(res))) val adminH3 = admin.withColumn(&quot;h3index&quot;, multiPolygonToH3(col(&quot;polygon_buff&quot;), lit(res))).select(&quot;geom_id&quot;, &quot;h3index&quot;).withColumn(&quot;h3index&quot;, explode($&quot;h3index&quot;)) pings.createOrReplaceTempView(&quot;pingsH3&quot;) adminH3.createOrReplaceTempView(&quot;adminH3&quot;) 3.4 Initial coarse geo-spatial join with shapefiles We then perform the first approximated spatial join using the spatial index and write it on an intermediate table val query = &quot;&quot;&quot;SELECT p.time , p.user_id , p.lat , p.lon , p.accuracy , s.geom_id FROM pingsH3 AS p INNER JOIN adminH3 AS s ON (p.h3index = s.h3index)&quot;&quot;&quot; var pings_geocoded = spark.sql(query) val out_table_temp = s&quot;pings.${c(&quot;source&quot;)}_${c(&quot;country&quot;)}_geocoded_temp&quot; pings_geocoded.write.format(&quot;delta&quot;).mode(&quot;overwrite&quot;).saveAsTable(out_table_temp) 3.5 Final exact join with a subset of the shapefiles Finally perform the accurate spatial join on the reduced subset of shapes and pings without the spatial index and save it. val out_table_temp = s&quot;pings.${c(&quot;source&quot;)}_${c(&quot;country&quot;)}_geocoded_temp&quot; val admin = spark.sql(s&quot;select geom_id, polygon from admin_with_polygon&quot;) val pings_geo = spark.read.table(out_table_temp) var pings_geo_pol = pings_geo.join(broadcast(admin), on=&#39;geom_id&#39;) pings_geo_pol.createOrReplaceTempView(&quot;pings_geo&quot;) val query = &quot;SELECT *, ST_Point(cast(lon as Decimal(13,10)), cast(lat as Decimal(13,10))) as point FROM pings_geo&quot; pings_geo_pol = spark.sql(query) pings_geo_pol.createOrReplaceTempView(&quot;pings_geo&quot;) val query = &quot;SELECT * FROM pings_geo WHERE ST_Intersects(point, polygon)&quot; pings_geo_pol = spark.sql(query).drop(&quot;polygon&quot;, &quot;point&quot;, &quot;valid&quot;) val out_table_temp = s&quot;pings.${c(&quot;source&quot;)}_${c(&quot;country&quot;)}_geocoded&quot; pings_geo_pol.write.format(&quot;delta&quot;).mode(&quot;overwrite&quot;).saveAsTable(out_table) "],["stops.html", "Chapter 4 Finding Stops 4.1 Definition 4.2 Infostop 4.3 Code 4.4 Clustering recurrent stops 4.5 Appending pings from recent dates 4.6 Stops geocoding", " Chapter 4 Finding Stops 4.1 Definition A critical endeavor to perform the sugested analysis is to give sense to indiviual pings by providing them with structure. In order to do this we need to derive a way to discern wether a person is at a specific place or traveling between two locations. 4.2 Infostop In order to do this we need both the spatial and the temporal data from each ping. This is, how far are consecutive pings from eachother in time and space. We rely on the firtst part of the infostop algorithm to detect stop-locations or stays from pings using the time stamp and the coordinates where it was created. Infostop is based in some heuristics: \\(r_1\\): The maximum distance between any pair of points within the same stop. \\(n_{min}\\): The minimum number of points that make up a stop. \\(t_{min}\\): The minimum duration of a stop. \\(t_{max}\\): The maximum time difference between two consecutive pings. Then in order to have a stop we would need to satisfy that some consecutive pings are all less than \\(r_1\\) meters apart, the time span between the first and the last pings is more than \\(t_{min}\\) seconds, there’s no more than \\(t_{max}\\) seconds between consecutive pairs and each stop cannot be composed of less than \\(n_{min}\\) pings. Particularely we use the Python function get_stationary_events() in the Infostop package. 4.3 Code 4.3.1 Data preparation Once we load the required data, we make make sure that they meet the timezone metadata requirements and that their coordinates are valid geographical points. filter_string = f&quot;accuracy &gt;=0 AND accuracy &lt;= 200 AND lat &gt; -90 AND lat &lt; 90 AND lon &gt; -180 AND lon &lt; 180&quot; if not tz: # add a check on TZ_OFFSET pings = spark.sql(f&quot;SELECT device_id AS user_id, lat, lon, accuracy, timestamp, TZ_OFFSET_SEC FROM default.veraset_{c.country}_tz WHERE country = &#39;{c.country}&#39; AND {filter_string}&quot;) pings = (pings .withColumn(&#39;epoch_time&#39;, col(&quot;timestamp&quot;) + col(&quot;TZ_OFFSET_SEC&quot;).cast(&quot;long&quot;)) .drop(&quot;TZ_OFFSET_SEC&quot;, &quot;timestamp&quot;)) elif tz: pings = spark.sql(f&quot;SELECT device_id AS user_id, lat, lon, accuracy, timestamp FROM default.veraset_primary_1 WHERE country = &#39;{c.country}&#39; AND {filter_string}&quot;) pings = (pings .withColumn(&#39;time&#39;, F.to_timestamp(&#39;timestamp&#39;)) .withColumn(&#39;new_time&#39;, F.from_utc_timestamp(&#39;time&#39;, tz)) .withColumn(&#39;epoch_time&#39;, F.unix_timestamp(&#39;new_time&#39;)) .drop(&#39;timestamp&#39;, &#39;time&#39;, &#39;new_time&#39;)) else: raise Exception (&quot;Undefined time zone in config or tz_offset in input table&quot;) 4.3.2 Get stop locations Now we have the data ready to identify stops, we only need to sort pings by their timestamp for each user. sl = (pings .orderBy(&quot;epoch_time&quot;) .groupBy(&quot;user_id&quot;) .apply(get_stop_location, args=(radius, stay_time, min_pts_per_stop_location, max_time_stop_location, max_accuracy, db_scan_radius)) .dropna()) Where the user defined function get_stop_location() composed by other functions which at the end will yield a data frame with user id, stop beginning timestamp, stop end timestamp, a centroid coordinate pair from the original pings, a cluster label from a DBSCAN clustering algorithm, the median accuracy of the pings and the total number of pings that compose the given stop. For DBSCAN we also use \\(\\epsilon=50m\\) and the minimum accuracy for us to keep a ping is \\(100m\\) def compute_intervals(centroids, labels, timestamps, accuracy, input_data): # if the label is -1 it means that the point doesn&#39;t belong to any cluster. Otherwise there should be at least 2 points for a stop locations # and they should # assert (len(centroids) == len(community_labels)) i = 0 seen = 0 trajectory = [] while i &lt; len(labels): if labels[i] == -1: i += 1 else: start_index = i while (i + 1 &lt; len(labels)) and (labels[i] == labels[i + 1]): i += 1 trajectory.append((timestamps[start_index], timestamps[i], *centroids[seen], np.median(accuracy[start_index: i]), i - start_index + 1)) seen += 1 i += 1 return trajectory def run_infostop(data, r1, min_staying_time, min_size, max_time_between, distance_metric): data_assertions(data) centroids, stat_labels = get_stationary_events( data[:, :3], r1, min_size, min_staying_time, max_time_between, distance_metric) return compute_intervals(centroids, stat_labels, data[:, 2], data[:, 3], data) schema_df = StructType([ StructField(&#39;user_id&#39;, StringType(), False), StructField(&#39;t_start&#39;, LongType(), False), StructField(&#39;t_end&#39;, LongType(), False), StructField(&#39;lat&#39;, DoubleType(), False), StructField(&#39;lon&#39;, DoubleType(), False), StructField(&#39;cluster_label&#39;, LongType(), True), StructField(&#39;median_accuracy&#39;, DoubleType(), True), StructField(&#39;total_pings_stop&#39;, LongType(), True), ]) @pandas_udf(schema_df, PandasUDFType.GROUPED_MAP) def get_stop_location(df, radius, stay_time, min_pts_per_stop_location, max_time_stop_location, max_accuracy, db_scan_radius): identifier = df[&#39;user_id&#39;].values[0] df.sort_values(by=&#39;epoch_time&#39;, inplace=True) # shouldnt be necessary data = df[[&quot;lat&quot;, &quot;lon&quot;, &#39;epoch_time&#39;, &quot;accuracy&quot;]].values res = run_infostop(data, r1=radius, min_staying_time=stay_time, min_size=min_pts_per_stop_location, max_time_between=max_time_stop_location, distance_metric=&#39;haversine&#39;) df = pd.DataFrame(res, columns=[ &quot;t_start&quot;, &quot;t_end&quot;, &quot;lat&quot;, &quot;lon&quot;, &quot;median_accuracy&quot;, &quot;total_pings_stop&quot;]) # new filtering step based on median accuracy df = df[df[&#39;median_accuracy&#39;] &lt; max_accuracy] df[&#39;user_id&#39;] = identifier if not df.empty: # df[&#39;cluster_label&#39;] = get_labels(df[[&#39;lat&#39;, &#39;lon&#39;]]) # notice that we don&#39;t have noise here, since any point that we consider is a stop location and hence has been already pre filtered by run_infostop (min_samples = 1 =&gt; no label =-1) db = DBSCAN(eps=db_scan_radius, min_samples=1, metric=&#39;haversine&#39;, algorithm=&#39;ball_tree&#39;).fit(np.radians(df[[&#39;lat&#39;, &#39;lon&#39;]].values)) df[&#39;cluster_label&#39;] = db.labels_ else: df[&#39;cluster_label&#39;] = None return df 4.4 Clustering recurrent stops The clustering part is where our approach differs from the Infostop one, as the assign cluster labels by building a network of stops if they’re located closer than a distance \\(r_2\\) and then use the Infomap community detection algorithm while we proceed by clustering with DBSCAN as previously discussed. It is important to notice that these clusters don’t depend on the time but only on the spatial distribution as we are interested in finding recurrent visited locations. Finally we split those stops that span multiple days into single ones sl = (sl .withColumn(&quot;total_duration_stop_location&quot;, F.col(&quot;t_end&quot;) - F.col(&quot;t_start&quot;)) .withColumn(&#39;my_list&#39;, make_list(F.to_timestamp(col(&#39;t_start&#39;)), F.to_timestamp(col(&#39;t_end&#39;)))) .drop(&#39;t_start&#39;, &#39;t_end&#39;) .withColumn(&quot;tmp&quot;, F.explode(&quot;my_list&quot;)) .withColumn(&quot;t_start&quot;, F.col(&quot;tmp&quot;).t_start) .withColumn(&quot;t_end&quot;, F.col(&quot;tmp&quot;).t_end) .drop(&quot;tmp&quot;, &quot;my_list&quot;) .withColumn(&quot;duration&quot;, F.col(&quot;t_end&quot;) - F.col(&quot;t_start&quot;))) 4.5 Appending pings from recent dates GPS data is passively gathered and as so it’s continiously growing, this translates in the need for contiously find stops and assign cluster labels to them. This whole process is very computationally expensive and running it on the whole data would be highly inefficient, this is why we keep old stops and only compute new ones, but there’s no way around the clustering part as the labels can change every time. Fortunately the number of stops is several order magnitudes lower than the pings as the smallest stop has at least two of them and we drop those that didn’t make up a stop. 4.6 Stops geocoding Now that our data is composed by new aggregated points we replicate the indexing from 3 but with the stops instead of the pings. "],["defining-home-and-work-locations.html", "Chapter 5 Defining Home and Work Locations 5.1 Seasonal patterns 5.2 Code", " Chapter 5 Defining Home and Work Locations From the census data we have socioeconomic information for every geographical region in each country and we used that to assign a Wealth Index value to them. A good proxy of the demographic of the users can be obtained by the location of their home which we can infeer from the recurrent stop locations we did in the previous chapter 4 given some regularity patterns and use the administrative information of the geometry where it falls into. Moreover we can do this for every stops’ cluster. Our focus lies on the home and the work locations as they’re likely to be the places where indiviuals spend more time and define the largest bulk of their mobility patterns. 5.1 Seasonal patterns Figure 5.1: Number of stops by weekday and start hour at home location Figure 5.2: Number of stops by weekday and start hour at work location As we can see from 5.1 and 5.2 usually a person’s mobility patterns have seasonal structure so we label each stop with it’s corresponding day of the week. For this reason we need to label each stop with the weekday value when it was created. 5.2 Code df = df.withColumn(&quot;t_start_hour&quot;, F.hour( F.to_timestamp(&quot;t_start&quot;))) .withColumn(&quot;t_end_hour&quot;, F.hour( F.to_timestamp(&quot;t_end&quot;))) .withColumn(&#39;weekday&#39;, F.dayofweek( F.to_timestamp(&quot;t_start&quot;))) .withColumn(&quot;date&quot;, F.to_timestamp(&quot;t_start&quot;)) .withColumn(&quot;date_trunc&quot;, F.date_trunc(&quot;day&quot;, F.col(&quot;date&quot;))) Then we use the following function to label the home and work candidates. Notice that as this is a user defined function, the schema at the top contains the output variables. schema_df = StructType([ StructField(&#39;user_id&#39;, StringType(), False), StructField(&#39;t_start&#39;, LongType(), False), StructField(&#39;t_end&#39;, LongType(), False), StructField(&#39;duration&#39;, LongType(), False), StructField(&#39;lat&#39;, DoubleType(), False), StructField(&#39;lon&#39;, DoubleType(), False), StructField(&#39;total_duration_stop_location&#39;, LongType(), False), StructField(&#39;total_pings_stop&#39;, LongType(), False), StructField(&#39;cluster_label&#39;, LongType(), False), StructField(&#39;median_accuracy&#39;, DoubleType(), False), StructField(&#39;location_type&#39;, StringType(), True), StructField(&#39;home_label&#39;, LongType(), True), StructField(&#39;work_label&#39;, LongType(), True), StructField(&#39;geom_id&#39;, StringType(), False), StructField(&#39;date&#39;, TimestampType(), True), StructField(&#39;t_start_hour&#39;, IntegerType(), True), StructField(&#39;t_end_hour&#39;, IntegerType(), True), StructField(&quot;date_trunc&quot;, TimestampType(), True) ]) @pandas_udf(schema_df, PandasUDFType.GROUPED_MAP) def compute_home_work_label_dynamic(user_df, start_hour_day, end_hour_day, min_pings_home_cluster_label, work_activity_average): # We start by assuming every cluster falls into the &quot;Other&quot; category. # Meaning they&#39;re not home nor work. user_df[&#39;location_type&#39;] = &#39;O&#39; user_df[&#39;home_label&#39;] = -1 user_df[&#39;work_label&#39;] = -1 # HOME # filter candidates as night-time stops home_tmp = user_df[(user_df[&#39;t_start_hour&#39;] &gt;= end_hour_day) | ( user_df[&#39;t_end_hour&#39;] &lt;= start_hour_day)].copy() # restrictive version of daytimes if home_tmp.empty: # if we don&#39;t have at least a home location, we return and not attemp to compute work location return remove_unused_cols(user_df) first_day = home_tmp[&#39;date_trunc&#39;].min() last_day = home_tmp[&#39;date_trunc&#39;].max() # work on clusters with &quot;duration&quot; attribute per day (&quot;date_trunc&quot;) home_tmp = home_tmp[[&#39;cluster_label&#39;, &#39;date_trunc&#39;, &#39;duration&#39;, &#39;total_pings_stop&#39;]].groupby( [&#39;cluster_label&#39;, &#39;date_trunc&#39;]).sum().reset_index().sort_values(&#39;date_trunc&#39;) # computer cumulative duration of candidates over &quot;period&quot; window home_tmp = home_tmp.merge(home_tmp[[&#39;date_trunc&#39;, &#39;cluster_label&#39;, &#39;duration&#39;, &#39;total_pings_stop&#39;]].groupby( [&#39;cluster_label&#39;]).apply(home_rolling_on_date).reset_index(), on=[&#39;date_trunc&#39;, &#39;cluster_label&#39;], suffixes=(&#39;&#39;, &#39;_cum&#39;)) print(home_tmp.columns) ###### home_tmp = home_tmp[home_tmp.total_pings_stop_cum &gt; min_pings_home_cluster_label].drop(&#39;total_pings_stop_cum&#39;, axis=1) # filter out nan rows, equivalent to filter on min_days home_tmp = home_tmp.dropna(subset=[&#39;duration_cum&#39;]) if home_tmp.empty: # if we don&#39;t have at least a home location, we return and not attemp to compute work location return remove_unused_cols(user_df) ##################### date_cluster = home_tmp.drop_duplicates([&#39;cluster_label&#39;, &#39;date_trunc&#39;])[ [&#39;date_trunc&#39;, &#39;cluster_label&#39;]].copy() date_cluster = date_cluster.drop_duplicates([&#39;date_trunc&#39;]) home_label = list(zip(date_cluster.cluster_label, date_cluster.date_trunc)) # creating a multinidex over which locating tuples of &quot;date_trunc&quot; and &quot;home_label&quot; idx = pd.MultiIndex.from_frame(user_df[[&#39;cluster_label&#39;, &#39;date_trunc&#39;]]) user_df.loc[idx.isin(home_label), &#39;home_label&#39;] = user_df.loc[idx.isin( home_label), &#39;cluster_label&#39;] ##################### base_dates = pd.date_range(start=first_day, end=last_day) date_cluster = date_cluster.sort_values( by=&#39;date_trunc&#39;).set_index(&#39;date_trunc&#39;) date_cluster = date_cluster.reindex(base_dates) if pd.notna(date_cluster[&#39;cluster_label&#39;]).sum() &gt; 1: date_cluster = date_cluster.interpolate( method=&#39;nearest&#39;).ffill().bfill() else: date_cluster = date_cluster.ffill().bfill() date_cluster.index.name = &#39;date_trunc&#39; date_cluster = date_cluster.reset_index() home_label = list(zip(date_cluster.cluster_label, date_cluster.date_trunc)) # creating a multindex over which locating tuples of &quot;date_trunc&quot; and &quot;home_label&quot; idx = pd.MultiIndex.from_frame(user_df[[&#39;cluster_label&#39;, &#39;date_trunc&#39;]]) user_df.loc[idx.isin(home_label), &#39;location_type&#39;] = &#39;H&#39; home_list = home_tmp.cluster_label.unique() if home_list.size == 0: return remove_unused_cols(user_df) ######## # WORK # ######## work_tmp = user_df[~(user_df[&#39;cluster_label&#39;].isin(home_list))].copy() if work_tmp.empty: # if we can&#39;t compute work location we return return remove_unused_cols(user_df) # if daytime: ######don&#39;t like it work_tmp = work_tmp[((work_tmp[&#39;t_start_hour&#39;] &gt;= start_hour_day+4) &amp; (work_tmp[&#39;t_end_hour&#39;] &lt;= end_hour_day-6)) &amp; (~work_tmp[&#39;weekday&#39;].isin([1, 7]))] # restrictive version of daytimes if work_tmp.empty: # if we can&#39;t compute work location we return return remove_unused_cols(user_df) first_day = work_tmp[&#39;date_trunc&#39;].min() # drop duplicates, smooth over &quot;period&quot; time window work_tmp = work_tmp[[&#39;date_trunc&#39;, &#39;cluster_label&#39;, &#39;duration&#39;]].groupby( [&#39;cluster_label&#39;, &#39;date_trunc&#39;]).sum().reset_index() work_tmp = work_tmp.merge(work_tmp[[&#39;date_trunc&#39;, &#39;cluster_label&#39;, &#39;duration&#39;]] .groupby([&#39;cluster_label&#39;]) .apply(work_rolling_on_date) .reset_index(), on=[&#39;date_trunc&#39;, &#39;cluster_label&#39;], suffixes=(&#39;&#39;, &#39;_average&#39;)) # filter out candidates which on average on the period do not pass the constraint work_tmp = work_tmp[(work_tmp.duration_average &gt;= work_activity_average)] # Select work clusters candidate: the clusters that passed the previous criteria are selected as work for the day if work_tmp.empty: # if we can&#39;t compute work location we return return remove_unused_cols(user_df) ##################### work_label = list(zip(work_tmp.cluster_label, work_tmp.date_trunc)) idx = pd.MultiIndex.from_frame(user_df[[&#39;cluster_label&#39;, &#39;date_trunc&#39;]]) work_list = work_tmp.cluster_label.unique() if work_list.size == 0: return remove_unused_cols(user_df) # add cluster label to work_label on the day on which it is found to be work_location only user_df.loc[idx.isin(work_label), &#39;work_label&#39;] = user_df.loc[idx.isin( work_label), &#39;cluster_label&#39;] ##################### # add work labels to all user dataset work_label = work_tmp[&#39;cluster_label&#39;].unique() idx = pd.Index(user_df[&#39;cluster_label&#39;]) user_df.loc[idx.isin(work_label), &#39;location_type&#39;] = &#39;W&#39; return remove_unused_cols(user_df) "],["references.html", "References", " References "],["migration.html", "Chapter 6 Migration Patterns", " Chapter 6 Migration Patterns Put mobility code and results here "],["optimization.html", "Chapter 7 Parameter Optimization", " Chapter 7 Parameter Optimization Explain the optimization here "],["index.html", "Chapter 8 Wealth Index", " Chapter 8 Wealth Index Explain how the wealth index is obtained. "],["voronoi.html", "Chapter 9 Voronoi", " Chapter 9 Voronoi Explain the polygon voronoi process and importance. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
