<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 13 Finetuning BERT-based models | Big Data Analytics</title>
  <meta name="description" content="SDG Bookdown" />
  <meta name="generator" content="bookdown 0.23 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 13 Finetuning BERT-based models | Big Data Analytics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="SDG Bookdown" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 13 Finetuning BERT-based models | Big Data Analytics" />
  
  <meta name="twitter:description" content="SDG Bookdown" />
  

<meta name="author" content="Chapter 13 Finetuning BERT-based models | Big Data Analytics Group" />


<meta name="date" content="2021-08-31" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="training-data-preparation.html"/>
<link rel="next" href="unemployment-indicators.html"/>
<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Big Data Analysis</a></li>

<li class="divider"></li>
<li class="part"><span><b>Cover</b></span></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Cover</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#gps-analytics"><i class="fa fa-check"></i><b>1.1</b> <span>GPS Analytics</span></a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#labor-market-analysis-with-twitter-data"><i class="fa fa-check"></i><b>1.2</b> <span>Labor market analysis with Twitter data</span></a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#news-analysis-with-twitter-data"><i class="fa fa-check"></i><b>1.3</b> <span>News analysis with Twitter data</span></a></li>
</ul></li>
<li class="part"><span><b>GPS Analytics</b></span></li>
<li class="chapter" data-level="2" data-path="gps.html"><a href="gps.html"><i class="fa fa-check"></i><b>2</b> Introduction - GPS</a></li>
<li class="chapter" data-level="3" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>3</b> Data</a>
<ul>
<li class="chapter" data-level="3.1" data-path="data.html"><a href="data.html#gps-data"><i class="fa fa-check"></i><b>3.1</b> GPS Data</a></li>
<li class="chapter" data-level="3.2" data-path="data.html"><a href="data.html#wealth-index-data"><i class="fa fa-check"></i><b>3.2</b> Wealth Index Data</a></li>
<li class="chapter" data-level="3.3" data-path="data.html"><a href="data.html#administrative-boundaries-data"><i class="fa fa-check"></i><b>3.3</b> Administrative Boundaries Data</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="geocode.html"><a href="geocode.html"><i class="fa fa-check"></i><b>4</b> Geocode</a>
<ul>
<li class="chapter" data-level="4.1" data-path="geocode.html"><a href="geocode.html#efficient-spatial-joining-and-geospatial-indexing"><i class="fa fa-check"></i><b>4.1</b> Efficient spatial joining and Geospatial Indexing</a></li>
<li class="chapter" data-level="4.2" data-path="geocode.html"><a href="geocode.html#code"><i class="fa fa-check"></i><b>4.2</b> Code</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="geocode.html"><a href="geocode.html#loading-administrative-units"><i class="fa fa-check"></i><b>4.2.1</b> Loading administrative units</a></li>
<li class="chapter" data-level="4.2.2" data-path="geocode.html"><a href="geocode.html#loading-ping-data-and-h3-indexing"><i class="fa fa-check"></i><b>4.2.2</b> Loading ping data and h3 indexing</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="geocode.html"><a href="geocode.html#initial-coarse-geo-spatial-join-with-shapefiles"><i class="fa fa-check"></i><b>4.3</b> Initial coarse geo-spatial join with shapefiles</a></li>
<li class="chapter" data-level="4.4" data-path="geocode.html"><a href="geocode.html#final-exact-join-with-a-subset-of-the-shapefiles"><i class="fa fa-check"></i><b>4.4</b> Final exact join with a subset of the shapefiles</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="stops.html"><a href="stops.html"><i class="fa fa-check"></i><b>5</b> Finding Stops</a>
<ul>
<li class="chapter" data-level="5.1" data-path="stops.html"><a href="stops.html#definition"><i class="fa fa-check"></i><b>5.1</b> Definition</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="stops.html"><a href="stops.html#get-stop-locations"><i class="fa fa-check"></i><b>5.1.1</b> Get stop locations</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="stops.html"><a href="stops.html#clustering-recurrent-stops"><i class="fa fa-check"></i><b>5.2</b> Clustering recurrent stops</a></li>
<li class="chapter" data-level="5.3" data-path="stops.html"><a href="stops.html#appending-pings-from-recent-dates"><i class="fa fa-check"></i><b>5.3</b> Appending pings from recent dates</a></li>
<li class="chapter" data-level="5.4" data-path="stops.html"><a href="stops.html#stops-geocoding"><i class="fa fa-check"></i><b>5.4</b> Stops geocoding</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="labeling.html"><a href="labeling.html"><i class="fa fa-check"></i><b>6</b> Defining Home and Work Locations</a>
<ul>
<li class="chapter" data-level="6.1" data-path="labeling.html"><a href="labeling.html#seasonal-patterns"><i class="fa fa-check"></i><b>6.1</b> Seasonal patterns</a></li>
<li class="chapter" data-level="6.2" data-path="labeling.html"><a href="labeling.html#code-1"><i class="fa fa-check"></i><b>6.2</b> Code</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="mobility.html"><a href="mobility.html"><i class="fa fa-check"></i><b>7</b> Mobility Patterns</a>
<ul>
<li class="chapter" data-level="7.1" data-path="mobility.html"><a href="mobility.html#socio-economic-groups"><i class="fa fa-check"></i><b>7.1</b> Socio-economic groups</a></li>
<li class="chapter" data-level="7.2" data-path="mobility.html"><a href="mobility.html#selection"><i class="fa fa-check"></i><b>7.2</b> Individual’s selection</a></li>
<li class="chapter" data-level="7.3" data-path="mobility.html"><a href="mobility.html#measures"><i class="fa fa-check"></i><b>7.3</b> Measures</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="mobility.html"><a href="mobility.html#measuring-mobility-patterns"><i class="fa fa-check"></i><b>7.3.1</b> Measuring mobility patterns</a></li>
<li class="chapter" data-level="7.3.2" data-path="mobility.html"><a href="mobility.html#measuring-change"><i class="fa fa-check"></i><b>7.3.2</b> Measuring change</a></li>
<li class="chapter" data-level="7.3.3" data-path="mobility.html"><a href="mobility.html#home-isolated-commuters-and-low-wealth-commuters"><i class="fa fa-check"></i><b>7.3.3</b> Home-isolated, commuters and low-wealth commuters</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="migration.html"><a href="migration.html"><i class="fa fa-check"></i><b>8</b> Migration Patterns</a>
<ul>
<li class="chapter" data-level="8.1" data-path="migration.html"><a href="migration.html#description"><i class="fa fa-check"></i><b>8.1</b> Description</a></li>
<li class="chapter" data-level="8.2" data-path="migration.html"><a href="migration.html#user-selection"><i class="fa fa-check"></i><b>8.2</b> User selection</a></li>
<li class="chapter" data-level="8.3" data-path="migration.html"><a href="migration.html#code-2"><i class="fa fa-check"></i><b>8.3</b> Code</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="optimization.html"><a href="optimization.html"><i class="fa fa-check"></i><b>9</b> Location labeling and parameter optimization</a>
<ul>
<li class="chapter" data-level="9.1" data-path="optimization.html"><a href="optimization.html#ground-truth"><i class="fa fa-check"></i><b>9.1</b> Ground truth</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="optimization.html"><a href="optimization.html#construction"><i class="fa fa-check"></i><b>9.1.1</b> Construction</a></li>
<li class="chapter" data-level="9.1.2" data-path="optimization.html"><a href="optimization.html#validation"><i class="fa fa-check"></i><b>9.1.2</b> Validation</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="optimization.html"><a href="optimization.html#parameter-optimization"><i class="fa fa-check"></i><b>9.2</b> Parameter optimization</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="optimization.html"><a href="optimization.html#performance-indicators"><i class="fa fa-check"></i><b>9.2.1</b> Performance indicators</a></li>
<li class="chapter" data-level="9.2.2" data-path="optimization.html"><a href="optimization.html#performance-error-estimates-and-bootstrapping"><i class="fa fa-check"></i><b>9.2.2</b> Performance error estimates and bootstrapping</a></li>
<li class="chapter" data-level="9.2.3" data-path="optimization.html"><a href="optimization.html#parallel-bootstrapping-and-optimization-result-storage"><i class="fa fa-check"></i><b>9.2.3</b> Parallel bootstrapping and optimization result storage</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="optimization.html"><a href="optimization.html#parameter-configuration-selection"><i class="fa fa-check"></i><b>9.3</b> Parameter configuration selection</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="windex.html"><a href="windex.html"><i class="fa fa-check"></i><b>10</b> Wealth Index</a>
<ul>
<li class="chapter" data-level="10.1" data-path="windex.html"><a href="windex.html#variables-to-estimate-the-social-gap-index-in-mexico"><i class="fa fa-check"></i><b>10.1</b> Variables to estimate the Social Gap Index in Mexico</a></li>
<li class="chapter" data-level="10.2" data-path="windex.html"><a href="windex.html#education"><i class="fa fa-check"></i><b>10.2</b> Education</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="windex.html"><a href="windex.html#health-services-access"><i class="fa fa-check"></i><b>10.2.1</b> Health Services Access</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="windex.html"><a href="windex.html#household-characteristics"><i class="fa fa-check"></i><b>10.3</b> Household Characteristics</a></li>
<li class="chapter" data-level="10.4" data-path="windex.html"><a href="windex.html#assets-ownership"><i class="fa fa-check"></i><b>10.4</b> Assets Ownership</a></li>
</ul></li>
<li class="part"><span><b>Twitter Analytics - Labor Market</b></span></li>
<li class="chapter" data-level="11" data-path="labor.html"><a href="labor.html"><i class="fa fa-check"></i><b>11</b> Introduction - Labor Market</a>
<ul>
<li class="chapter" data-level="11.1" data-path="labor.html"><a href="labor.html#labor-market-analysis-with-twitter-data-1"><i class="fa fa-check"></i><b>11.1</b> Labor market analysis with Twitter data</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="training-data-preparation.html"><a href="training-data-preparation.html"><i class="fa fa-check"></i><b>12</b> Training data preparation</a>
<ul>
<li class="chapter" data-level="12.1" data-path="training-data-preparation.html"><a href="training-data-preparation.html#sampling"><i class="fa fa-check"></i><b>12.1</b> Sampling</a></li>
<li class="chapter" data-level="12.2" data-path="training-data-preparation.html"><a href="training-data-preparation.html#labelling"><i class="fa fa-check"></i><b>12.2</b> Labelling</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="training-data-preparation.html"><a href="training-data-preparation.html#creating-a-qualtrics-survey"><i class="fa fa-check"></i><b>12.2.1</b> Creating a Qualtrics survey</a></li>
<li class="chapter" data-level="12.2.2" data-path="training-data-preparation.html"><a href="training-data-preparation.html#sharing-the-survey-on-mturk"><i class="fa fa-check"></i><b>12.2.2</b> Sharing the survey on MTurk</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="finetuning-bert-based-models.html"><a href="finetuning-bert-based-models.html"><i class="fa fa-check"></i><b>13</b> Finetuning BERT-based models</a>
<ul>
<li class="chapter" data-level="13.1" data-path="finetuning-bert-based-models.html"><a href="finetuning-bert-based-models.html#training-the-model"><i class="fa fa-check"></i><b>13.1</b> Training the model</a></li>
<li class="chapter" data-level="13.2" data-path="finetuning-bert-based-models.html"><a href="finetuning-bert-based-models.html#evaluation-on-the-random-set"><i class="fa fa-check"></i><b>13.2</b> Evaluation on the random set</a></li>
<li class="chapter" data-level="13.3" data-path="finetuning-bert-based-models.html"><a href="finetuning-bert-based-models.html#active-learning"><i class="fa fa-check"></i><b>13.3</b> Active learning</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="unemployment-indicators.html"><a href="unemployment-indicators.html"><i class="fa fa-check"></i><b>14</b> Unemployment indicators</a>
<ul>
<li class="chapter" data-level="14.1" data-path="unemployment-indicators.html"><a href="unemployment-indicators.html#building-unemployment-indicators-from-individual-tweets"><i class="fa fa-check"></i><b>14.1</b> Building unemployment indicators from individual tweets</a></li>
</ul></li>
<li class="part"><span><b>Twitter Analytics - News</b></span></li>
<li class="chapter" data-level="15" data-path="news.html"><a href="news.html"><i class="fa fa-check"></i><b>15</b> Introduction - News Analytics</a>
<ul>
<li class="chapter" data-level="15.1" data-path="news.html"><a href="news.html#news-articles"><i class="fa fa-check"></i><b>15.1</b> News articles</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="global-vs.-local-news-sentiment-indicators.html"><a href="global-vs.-local-news-sentiment-indicators.html"><i class="fa fa-check"></i><b>16</b> Global vs. local news sentiment indicators</a>
<ul>
<li class="chapter" data-level="16.1" data-path="global-vs.-local-news-sentiment-indicators.html"><a href="global-vs.-local-news-sentiment-indicators.html#local-news-sentiment-indicator"><i class="fa fa-check"></i><b>16.1</b> Local news sentiment indicator</a></li>
<li class="chapter" data-level="16.2" data-path="global-vs.-local-news-sentiment-indicators.html"><a href="global-vs.-local-news-sentiment-indicators.html#global-news-sentiment-indicator"><i class="fa fa-check"></i><b>16.2</b> Global news sentiment indicator</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="news-sentiment-measures.html"><a href="news-sentiment-measures.html"><i class="fa fa-check"></i><b>17</b> News-sentiment measures</a>
<ul>
<li class="chapter" data-level="17.1" data-path="news-sentiment-measures.html"><a href="news-sentiment-measures.html#bag-of-words-model"><i class="fa fa-check"></i><b>17.1</b> Bag-of-words model</a></li>
<li class="chapter" data-level="17.2" data-path="news-sentiment-measures.html"><a href="news-sentiment-measures.html#country-specific-news-sentiment-indicator"><i class="fa fa-check"></i><b>17.2</b> Country-specific news sentiment indicator</a></li>
<li class="chapter" data-level="17.3" data-path="news-sentiment-measures.html"><a href="news-sentiment-measures.html#stylized-facts"><i class="fa fa-check"></i><b>17.3</b> Stylized facts</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Big Data Analytics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="finetuning-bert-based-models" class="section level1" number="13">
<h1><span class="header-section-number">Chapter 13</span> Finetuning BERT-based models</h1>
<p>In the past few years, pretrained language models have revolutionized the field of NLP by achieving state-of-the-art results in a variety of natural language understanding tasks (Peters et al., 2018; Devlin et al., 2019). These models improve on existing word embedding methods, such as Word2Vec (Mikolov et al., 2013), by learning stable embedding representations from massive text corpora.</p>
<p>One of the models leading this revolution is the Bidirectional Encoder Representations from Transformers model (BERT, Devlin et al., 2019), which allowed for bi-directionality, through masked language modeling, and leveraged self-attention through its Transformer-based architecture (Vaswani et al., 2017). The model was pretrained on BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2.5B words) using two unsupervised tasks, namely masked language modeling and next-sentence prediction. This model can later be fine-tuned on a variety of downstream tasks, including text classification, achieving high performance.</p>
<p>In this part, we show how to fine-tune a BERT-based model for tweet classification. To do so, we mostly rely on the Python package <code>simpletransformers</code>, built on top of the famous <code>transformers</code> Python package developed by Hugging Face.</p>
<div id="training-the-model" class="section level2" number="13.1">
<h2><span class="header-section-number">13.1</span> Training the model</h2>
<p>After loading the set of labelled tweets in a dataframe <code>df</code>, we perform a train-test split (70-30) on it:</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="finetuning-bert-based-models.html#cb40-1" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> df.sample(frac<span class="op">=</span><span class="fl">0.7</span>,random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb40-2"><a href="finetuning-bert-based-models.html#cb40-2" aria-hidden="true" tabindex="-1"></a>val_df <span class="op">=</span> df.drop(train_df.index).reset_index(drop<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<p>We then define specific arguments for the fine-tuning such as the batch size <code>train_batch_size</code>, the number of epochs <code>num_train_epochs</code> or the output path <code>output_dir</code>. The models are evaluated at every epoch in terms of AUROC on the test set. The model with the highest AUROC on the test set is considered the best model and saved at <code>best_model_dir</code>. We also use early stopping which consists in stopping the training if the AUROC on the test set has not improved after <code>early_stopping_patience</code> epochs. A complete list of the training arguments can be found <a href="https://simpletransformers.ai/docs/usage/#configuring-a-simple-transformers-model">here</a>.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="finetuning-bert-based-models.html#cb41-1" aria-hidden="true" tabindex="-1"></a>    classification_args <span class="op">=</span> { <span class="st">&#39;train_batch_size&#39;</span>: <span class="dv">8</span>,</span>
<span id="cb41-2"><a href="finetuning-bert-based-models.html#cb41-2" aria-hidden="true" tabindex="-1"></a>                            <span class="st">&#39;overwrite_output_dir&#39;</span>: <span class="va">True</span>,</span>
<span id="cb41-3"><a href="finetuning-bert-based-models.html#cb41-3" aria-hidden="true" tabindex="-1"></a>                            <span class="st">&#39;evaluate_during_training&#39;</span>: <span class="va">True</span>,</span>
<span id="cb41-4"><a href="finetuning-bert-based-models.html#cb41-4" aria-hidden="true" tabindex="-1"></a>                            <span class="st">&#39;save_model_every_epoch&#39;</span>: <span class="va">True</span>,</span>
<span id="cb41-5"><a href="finetuning-bert-based-models.html#cb41-5" aria-hidden="true" tabindex="-1"></a>                            <span class="st">&#39;save_eval_checkpoints&#39;</span>: <span class="va">True</span>,</span>
<span id="cb41-6"><a href="finetuning-bert-based-models.html#cb41-6" aria-hidden="true" tabindex="-1"></a>                            <span class="st">&#39;output_dir&#39;</span>: path_to_store_model,</span>
<span id="cb41-7"><a href="finetuning-bert-based-models.html#cb41-7" aria-hidden="true" tabindex="-1"></a>                            <span class="st">&#39;best_model_dir&#39;</span>: path_to_store_best_model,</span>
<span id="cb41-8"><a href="finetuning-bert-based-models.html#cb41-8" aria-hidden="true" tabindex="-1"></a>                            <span class="st">&#39;evaluate_during_training_verbose&#39;</span>: <span class="va">True</span>,</span>
<span id="cb41-9"><a href="finetuning-bert-based-models.html#cb41-9" aria-hidden="true" tabindex="-1"></a>                            <span class="st">&#39;num_train_epochs&#39;</span>: num_train_epochs,</span>
<span id="cb41-10"><a href="finetuning-bert-based-models.html#cb41-10" aria-hidden="true" tabindex="-1"></a>                            <span class="st">&quot;use_early_stopping&quot;</span>: <span class="va">True</span>,</span>
<span id="cb41-11"><a href="finetuning-bert-based-models.html#cb41-11" aria-hidden="true" tabindex="-1"></a>                            <span class="st">&quot;early_stopping_delta&quot;</span>: <span class="dv">0</span>,</span>
<span id="cb41-12"><a href="finetuning-bert-based-models.html#cb41-12" aria-hidden="true" tabindex="-1"></a>                            <span class="st">&quot;early_stopping_metric&quot;</span>: <span class="st">&quot;auroc&quot;</span>,</span>
<span id="cb41-13"><a href="finetuning-bert-based-models.html#cb41-13" aria-hidden="true" tabindex="-1"></a>                            <span class="st">&quot;early_stopping_metric_minimize&quot;</span>: <span class="va">False</span>,</span>
<span id="cb41-14"><a href="finetuning-bert-based-models.html#cb41-14" aria-hidden="true" tabindex="-1"></a>                            <span class="st">&#39;early_stopping_patience&#39;</span>: <span class="dv">3</span>}</span></code></pre></div>
<p>Once the classification arguments are defined, we can initiate the fine-tuning. In our case, for English tweet classification, we use a version of BERT that was further pretrained on English tweets by <a href="https://deeppavlov.ai">Deep Pavlov</a>, therefore enhancing the classification performance in the Twitter context. We need to define the <code>model_name</code>, which is <code>bert</code> in our case but can also be other more sophisticated architectures such as <code>roberta</code>. The <code>model_type</code> refers to the model name on the <a href="https://huggingface.co/models">Hugging Face model hub</a>. As we only cover binary classification here, the number of labels <code>num_labels</code> is set to 2.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="finetuning-bert-based-models.html#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> simpletransformers.classification <span class="im">import</span> ClassificationModel</span>
<span id="cb42-2"><a href="finetuning-bert-based-models.html#cb42-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> ClassificationModel(model_name<span class="op">=</span><span class="st">&#39;bert&#39;</span>,</span>
<span id="cb42-3"><a href="finetuning-bert-based-models.html#cb42-3" aria-hidden="true" tabindex="-1"></a>                            model_type<span class="op">=</span><span class="st">&#39;DeepPavlov/bert-base-cased-conversational&#39;</span>,</span>
<span id="cb42-4"><a href="finetuning-bert-based-models.html#cb42-4" aria-hidden="true" tabindex="-1"></a>                            num_labels<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb42-5"><a href="finetuning-bert-based-models.html#cb42-5" aria-hidden="true" tabindex="-1"></a>                            use_cuda<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb42-6"><a href="finetuning-bert-based-models.html#cb42-6" aria-hidden="true" tabindex="-1"></a>                            args<span class="op">=</span>classification_args)</span></code></pre></div>
<p>Once the model has been loaded, we can launch the fine-tuning:</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="finetuning-bert-based-models.html#cb43-1" aria-hidden="true" tabindex="-1"></a>model.train_model(train_df<span class="op">=</span>train_df, eval_df<span class="op">=</span>eval_df, output_dir<span class="op">=</span>path_to_store_model)</span></code></pre></div>
<p>The training is now launched. When it is finished, we can use the best model in terms of AUROC on the test set and evaluate it on a bigger random set of tweets.</p>
</div>
<div id="evaluation-on-the-random-set" class="section level2" number="13.2">
<h2><span class="header-section-number">13.2</span> Evaluation on the random set</h2>
<p>While evaluating the model on the test set is useful to determine which classifier is best suited for the task, this performance might not be representative of the performance on a random set of tweets as the imbalance on the latter is much more extreme than on the test set. In this case, we needed to find a way to evaluate our classifiers in real-world settings. To do so, we used the best model from our training iterations to infer the confidences scores of each tweet out of a large random sample of 100 million tweets. We then ranked the tweets based on their confidence score in a descending way and sampled tweets along the confidence score distribution, overweighting the top of the distribution. In total, we sampled 200 tweets out of the 100 million random sample with ranks ranging from 1 to 1 million. The function used to do this sampling is as follows:</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="finetuning-bert-based-models.html#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_sampled_indices(n_sample<span class="op">=</span><span class="dv">10</span>, n_cutoff<span class="op">=</span><span class="dv">6</span>):</span>
<span id="cb44-2"><a href="finetuning-bert-based-models.html#cb44-2" aria-hidden="true" tabindex="-1"></a>    sampled_points <span class="op">=</span> []  <span class="co"># index of scores around which we sample n_sample tweets</span></span>
<span id="cb44-3"><a href="finetuning-bert-based-models.html#cb44-3" aria-hidden="true" tabindex="-1"></a>    sampled_ranks <span class="op">=</span> []  <span class="co"># ranks of sampled tweets</span></span>
<span id="cb44-4"><a href="finetuning-bert-based-models.html#cb44-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> point, rank <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">sorted</span>(<span class="bu">set</span>([<span class="bu">int</span>(x) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_cutoff) <span class="cf">for</span> x <span class="kw">in</span> np.logspace(i, i <span class="op">+</span> <span class="dv">1</span>, i <span class="op">+</span> <span class="dv">1</span>)]))):</span>
<span id="cb44-5"><a href="finetuning-bert-based-models.html#cb44-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> point:</span>
<span id="cb44-6"><a href="finetuning-bert-based-models.html#cb44-6" aria-hidden="true" tabindex="-1"></a>            new_ranks <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(rank, rank <span class="op">+</span> n_sample))</span>
<span id="cb44-7"><a href="finetuning-bert-based-models.html#cb44-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb44-8"><a href="finetuning-bert-based-models.html#cb44-8" aria-hidden="true" tabindex="-1"></a>            new_ranks <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(rank <span class="op">+</span> <span class="dv">1</span>, rank <span class="op">+</span> n_sample <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb44-9"><a href="finetuning-bert-based-models.html#cb44-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">&#39;Index of sampled point:&#39;</span>, point)</span>
<span id="cb44-10"><a href="finetuning-bert-based-models.html#cb44-10" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">&#39;Sampled ranks:&#39;</span>, new_ranks)</span>
<span id="cb44-11"><a href="finetuning-bert-based-models.html#cb44-11" aria-hidden="true" tabindex="-1"></a>        sampled_points.extend([point] <span class="op">*</span> n_sample)</span>
<span id="cb44-12"><a href="finetuning-bert-based-models.html#cb44-12" aria-hidden="true" tabindex="-1"></a>        sampled_ranks.extend(new_ranks)</span>
<span id="cb44-13"><a href="finetuning-bert-based-models.html#cb44-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sampled_points, sampled_ranks</span></code></pre></div>
<p>We then labelled these sampled tweets, which allowed us to get the percentage of positive tweets for given points in the rank distribution. Below is an example of an evaluation plot for the class <code>job_search</code> with the tweet rank as x-axis and the percentage of positive tweets for the class <code>job_search</code> as y-axis.</p>
<div class="figure">
<img src="figures/job_search.png" alt="" />
<p class="caption">Precision (y-axis) as a function of tweet rank based on confidence score i.e. positive label probability output by the model (x-axis).</p>
</div>
</div>
<div id="active-learning" class="section level2" number="13.3">
<h2><span class="header-section-number">13.3</span> Active learning</h2>
<p>While determining an initial list of n-grams to build a labelled set from is a good strategy to get a decent classification performance, it is unlikely to capture all the linguistic subtleties humans use on social media to talk about their labor market situation. In this case, we ideally need to label more tweets and need to determine which are the most informative to respect our labeling budget constraint.</p>
<p>When faced with a classification task in which the minority class is extremely rare, active learning allows to minimize the number of labels required for a classifier to achieve good performance. At each iteration, a trained model is used to query new samples expected to lead to high improvements in validation accuracy. These samples are, in turn, labeled by humans and then used for training in the next iteration of the model. There are two main approaches to identifying the most informative samples: uncertainty sampling (Lewis and Gale, 1994) and diversity sampling (Cohn et al., 1994), which have been coined as the ``two faces of active learning’’(Dasgupta, 2011). While uncertainty sampling defines the most informative samples as the ones the model is the most uncertain about (e.g. in a binary context, this boundary is at 0.5), diversity sampling consists of selecting examples to label from different homogeneous clusters of the feature space.</p>
<p>Here, we provide an example on how to select the most informative tweets to label using uncertainty sampling on uncalibrated BERT confidence scores. In this setting, we sample tweets with BERT confidence scores around 0.5. Having the data stored in a dataframe <code>df</code> containing the confidence scores in the <code>score</code> column, we create a <code>modified_score</code> which is the confidence score minus 0.5. We then select 50 tweets with the smallest positive <code>modified_score</code> and 50 tweets with the highest negative <code>modified_score</code>.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb45-1"><a href="finetuning-bert-based-models.html#cb45-1" aria-hidden="true" tabindex="-1"></a>df[<span class="st">&#39;modified_score&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;score&#39;</span>] <span class="op">-</span> <span class="fl">0.5</span></span>
<span id="cb45-2"><a href="finetuning-bert-based-models.html#cb45-2" aria-hidden="true" tabindex="-1"></a>above_threshold_df <span class="op">=</span> df.loc[df[<span class="st">&#39;modified_score&#39;</span>] <span class="op">&gt;</span> <span class="dv">0</span>].nsmallest(<span class="dv">50</span>, <span class="st">&#39;modified_score&#39;</span>)</span>
<span id="cb45-3"><a href="finetuning-bert-based-models.html#cb45-3" aria-hidden="true" tabindex="-1"></a>below_threshold_df <span class="op">=</span> df.loc[df[<span class="st">&#39;modified_score&#39;</span>] <span class="op">&lt;</span> <span class="dv">0</span>].nlargest(<span class="dv">50</span>, <span class="st">&#39;modified_score&#39;</span>)</span>
<span id="cb45-4"><a href="finetuning-bert-based-models.html#cb45-4" aria-hidden="true" tabindex="-1"></a>sample_df <span class="op">=</span> pd.concat([above_threshold_df, below_threshold_df]).reset_index(drop<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<p>In the end, the <code>sample_df</code> are sent to labelling. When labelled, the new samples are added to the existing labels and a new train-test split is applied. The training and evaluation then takes place as described earlier.</p>
<p>In practice, there exists many different active learning strategies. In Tonneau et al. (2021), we reviewed several active learning strategies in our extremely imbalanced setting and showed that active learning does improve performance in terms of precision, expansion and diversity but that no active learning strategy was systematically better than others for our problem.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="training-data-preparation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="unemployment-indicators.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/worldbank/SDG-big-data/12-model_training_eval.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["big-daa.pdf", "big-daa.epub"],
"search": {
"engine": "lunr",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
