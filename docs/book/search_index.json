[["index.html", "Big Data Analytics Chapter 1 Introduction 1.1 GPS Analytics 1.2 Labor market analysis with Twitter data 1.3 News analysis with Twitter data", " Big Data Analytics Development Data Group - Data Analytics and Tools 2022-02-01 Chapter 1 Introduction This book is comprised by three main chapters in which we detail the use of big data for mobility analysis using individual GPS information as well as NLP tools to build labor market indicators from Twitter data as well as understand the sentiment around news. 1.1 GPS Analytics We analyzed GPS data from 6 different countries to understand the differences in “stay at home” mandates for different socioeconomic groups. A summary plot of our results is displayed in the interactive chart below. The first panel represents the percentage change of the total time spent at home by socioeconomic group, the second one is the percentage change of daily users commuting to work also by socioeconomic group and the last one shows the percentage change of work commutes in the low-income group by the wealth of the neighborhood where the work is located. 1.2 Labor market analysis with Twitter data Online social networks, such as Twitter, play a key role in the diffusion of information on jobs. For instance, companies and job aggregators post job offers while users disclose their labor market situation seeking emotional support or new job opportunities through their online network. In this context, Twitter data can be seen as a complementary data source to official statistics as it provides timely information about labor market trends. In this project, we leverage state-of-the-art language models (Devlin et al, 2018) to accurately identify disclosures on personal labor market situations as well as job offers. The methodology is presented in this IC2S2 2020 presentation and detailed in Tonneau et al. (2021) (in review). Aggregating this individual information at the city and country levels, we then built Twitter-based labor market indexes and used them to better predict future labor market trends. The indicators resulting from this study can be found below, for the US, Brazil and Mexico. The x-axis corresponds to the share of Twitter users we inferred in a given month to: - have found a job - be unemployed - have shared a job offer - be looking for a job - have lost their job 1.3 News analysis with Twitter data We take news articles focusing on a specific country and compute the sentiment associated with the article. The methodology is detailed in Fraiberger et al. (2021) with an application to understanding international asset prices. A more detailed explanation of these results and the procedure to obtained them is described in the following chapters. "],["gps.html", "Chapter 2 GPS 2.1 Data 2.2 Geocode 2.3 Finding Stops 2.4 Defining Home and Work Locations 2.5 Mobility Patterns 2.6 Migration Patterns 2.7 Location labeling and parameter optimization (PART*) Appendix 2.8 Wealth Index", " Chapter 2 GPS In latest years new sources of data have been proven to be valuable assets to understand human behavior and thus help in the creation of public policies based on evidence. Among those lies mobile phone data which is commonly used to study spatio-temporal patterns in the form of mobility or social connections. This type of data usually comes in two flavors; Call Detail Record (CDR), which is basicaly the information generated by a telecom transaction between at least two devices, i.e. phone calls or SMS and it can be used to determine connections between users and a location proxy given the connection of the device to the nearest antenna. The second type is GPS data. In present times mobile phones, particularely smart phones, which global penetration is estimated to be almost 50%, are used for much more than Pier to Pier (P2P) transactions as they are capable of connecting to the internet through apps. There are companies that partner up with app creators in order to store information about the usage of their products, this way when used in a phone they store a token with a unique device ID and time stamp plus any extra information that the user has consent to share, such as geolocation which is obtained via the high precision GPS anthenas the phones are equiped with plus WiFi triangulation in some cases to upscale it. We are interested in the latter source of information as the aim of our study is to untangle the universal disparities, if any, in mobility reduction during the COVID-19 pandemic between socioeconomic groups. We focused our study in 6 middle-income countries (Brazil, Colombia, Indonesia, Mexico, Philippines and South Africa). 2.1 Data The complex task of analysing mobility data given the socioeconomic background of the user requires multiple data sources because we need a type of data that allow us to study individual spatio-temporal behavior, another one with demographic and economic information and one to serve as a bridge to the other two. 2.1.1 GPS Data Anonymous raw GPS points are provided at a global scale by the Veraset data company in the framework of the World Bank “Monitoring COVID-19 policy response through human mobility data” project. Veraset Movements data consists of raw GPS point information from devices all around the World. In contrast to the majority of GPS data providers, Verasets Movements are sourced from thousands of Software Development Kit (SDK) to reduce sample biases. It features a coverage of about 5% of the global population. In our work, unique-device GPS points are processed and analysed as a proxy of individuals mobility and patterns of visits. The data at our disposal spans over a period of 17 months, starting from the start of Jan 2020 until the end of May 2021. In this work multiple countries were analysed to assess the impact of the pandemic and the policy responses across a vast spectrum of potentially different behaviours. 2.1.2 Wealth Index Data Aiming for universal behaviors is a hard endeavour as there are cultural and economic differences between every country. This is why we need an homologated number or index that can describe the level of marginalization or deprivation of goods that comprise the universal definition of poverty. In order to do this we gathered the latest census data from each country and summarized the level of education, the access to health services and household characteristics and the finest administrative unit level available. Then we take all of these values and embed them in a new coordinate system given by a Principal component Analysis (PCA) and take the normalized value of the first component as a proxy to wealth. This approach yields a continuous variable which we later discretize by quantiles. 2.1.3 Administrative Boundaries Data The country-wise geographical information is given in the form of spatial databases that represent certain areas with polygons made of vertices in a given cartographic projection, commonly represented by their longitude and latitude coordinate pair. These areas have labels that can be linked to the census data allowing us to get a spatial representation of the previously mentioned Wealth Index. These files are usually given by the Statistics or Geography Institute of each nation. Using Geographical Information Software (GIS), in this case Sedona for Spark, we are able to assign a geographical area to each one of the GPS points given by the Veraset dataset and which will allow us to determine a probable home location inside an administrative polygon to each user and assign a wealth category to them. 2.2 Geocode One of the first step in our pipeline is to associate to each point of a gps trace for a user or an user’s stop location the smallest administrative unit available in our shapefiles for one country. This will allow us to join the gps data with the census and other indices based on the same shapefiles, and to group by larger administrative units (such as city or region for more general analysis. We will refer to the process of associating a (latitude,longitude) tuple to a shape (corresponding usually to the smallest availabe admin division) as geocoding. In this example, we will show how this can be done for the single pings that compose an user’s GPS trace, but the procedure is exactly the same for the stop locations case described in 2.3 // The data we are utilizing was provided by Veraset. If a user has installed one of the Veraset’s // partners’ apps on his or her mobile phone, the users’ position and position’s accuracy is sent to the Veraset server where // it is recorded at regular intervals. 2.2.1 Efficient spatial joining and Geospatial Indexing To be able to deal with billions of points, we need an efficient way to perform spatial queries. Since it is not feasible to join directly thousands of shapes with the billions of points, for computational reasons, we need an efficient way to reduce the computational cost by checking only few shapes that are in the surroundings of the point we want to join. As geo-spatial index we use the H3 indexing created by Uber which partitions the World in hexagons. Since the admin shapes cannot be exactly covered by hexagons that are included in the shape. The hexagon should have a size that is reasonable for a quick spatial join and hence be quite large: this implies we run the risk of mis-joining pings that end up close to the borders of the shapes. // TODO: put an image or create one To overcome this problem we perform two joins: the first one will use a geospatial index, and will get the shapes that are close enough to the points we want to geocode, after buffering the shapes to be sure that any hexagon that is around the border of a shape is included in the join. The second join will be only between these subsets of shapes selected and the actual point. This greatly reduces the computational cost and makes this problem tractable. Figure 2.1: H3 indexing representation. Figure taken from the H3 website referenced above. 2.2.2 Code 2.2.2.1 Loading administrative units We first load the administrative files containing the polygons and the wealth index information, then, using Apache Sedona (previously named GeoSpark) we can transform the Well-Known Text representation of the polygons (WKT) into geometries and create a buffer around them. var admin = spark.read.option(&quot;header&quot;,&quot;true&quot;).csv(c(&quot;admin_path&quot;) + &quot;/&quot; + c(&quot;country&quot;) + &quot;/admin.csv&quot;) admin.createOrReplaceTempView(&quot;admin&quot;) var query = &quot;SELECT geom_id AS geom_id, ST_GeomFromText(geometry) as polygon FROM admin&quot; admin = spark.sql(query) admin.createOrReplaceTempView(&quot;admin_with_polygons&quot;) query = &quot;SELECT *, ST_Buffer(polygon, 0.005) as polygon_buff FROM admin_with_polygons&quot; admin = spark.sql(query) 2.2.2.2 Loading ping data and h3 indexing We will also be needing the ping data from Veraset. In our case we’ll load them from the following table depending on the country. val table = s&quot;pings.${c(&quot;source&quot;)}_${c(&quot;country&quot;)}&quot; Then we’re able to proceed indexing the pings and the buffered geometries. In order to do this we only take those pings that have correct longitude and latitude values and threshold the accuracy (provided in the data) of the GPS point location. That last part is a confidence estimate of the real location of the device, we can usually see this in our phone when using a geolocation app which gives you information such as “accuracy &lt; 3m” which means that the actual position could be anywhere in a 3m radius circle with center at the position given. When reading the pings, we first filter the ones having wrong coordinates or an accuracy &gt; than 1km, and add to the spark table a column containing the county’s offset (if a country has only one timezone), since we want to compare different countries for the same time of the days, we will use this offset to align all the timestamps. We then index each buffered admin shape and ping using h3. var pings = spark.sql(s&quot;SELECT lat, lon, accuracy, timestamp AS time, device_id AS user_id FROM ${c(&quot;input_table&quot;)} WHERE country = &#39;${c(&quot;country&quot;)}&#39;&quot;) pings.filter($&quot;lat&quot; &gt; -90 &amp;&amp; $&quot;lat&quot; &lt; 90 &amp;&amp; $&quot;lon&quot; &gt; -180 &amp;&amp; $&quot;lon&quot; &lt; 180 &amp;&amp; $&quot;accuracy&quot; &gt;= 0 &amp;&amp; $&quot;accuracy&quot; &lt;= 1000) if (c(&quot;country&quot;) == &quot;CO&quot;){ pings = pings.withColumn(&quot;offset&quot;, lit(-5*60*60)) //colombia offset pings.printSchema() } val res = 10 pings = pings.withColumn(&quot;time&quot;, col(&quot;time&quot;) + col(&quot;offset&quot;)).withColumn(&quot;h3index&quot;, geoToH3(col(&quot;lat&quot;), col(&quot;lon&quot;), lit(res))) val adminH3 = admin.withColumn(&quot;h3index&quot;, multiPolygonToH3(col(&quot;polygon_buff&quot;), lit(res))).select(&quot;geom_id&quot;, &quot;h3index&quot;).withColumn(&quot;h3index&quot;, explode($&quot;h3index&quot;)) pings.createOrReplaceTempView(&quot;pingsH3&quot;) adminH3.createOrReplaceTempView(&quot;adminH3&quot;) 2.2.3 Initial coarse geo-spatial join with shapefiles We then perform the first approximated spatial join using the spatial index and write it on an intermediate table val query = &quot;&quot;&quot;SELECT p.time , p.user_id , p.lat , p.lon , p.accuracy , s.geom_id FROM pingsH3 AS p INNER JOIN adminH3 AS s ON (p.h3index = s.h3index)&quot;&quot;&quot; var pings_geocoded = spark.sql(query) val out_table_temp = s&quot;pings.${c(&quot;source&quot;)}_${c(&quot;country&quot;)}_geocoded_temp&quot; pings_geocoded.write.format(&quot;delta&quot;).mode(&quot;overwrite&quot;).saveAsTable(out_table_temp) 2.2.4 Final exact join with a subset of the shapefiles Finally perform the accurate spatial join on the reduced subset of shapes and pings without the spatial index and save it. val out_table_temp = s&quot;pings.${c(&quot;source&quot;)}_${c(&quot;country&quot;)}_geocoded_temp&quot; val admin = spark.sql(s&quot;select geom_id, polygon from admin_with_polygon&quot;) val pings_geo = spark.read.table(out_table_temp) var pings_geo_pol = pings_geo.join(broadcast(admin), on=&#39;geom_id&#39;) pings_geo_pol.createOrReplaceTempView(&quot;pings_geo&quot;) val query = &quot;SELECT *, ST_Point(cast(lon as Decimal(13,10)), cast(lat as Decimal(13,10))) as point FROM pings_geo&quot; pings_geo_pol = spark.sql(query) pings_geo_pol.createOrReplaceTempView(&quot;pings_geo&quot;) val query = &quot;SELECT * FROM pings_geo WHERE ST_Intersects(point, polygon)&quot; pings_geo_pol = spark.sql(query).drop(&quot;polygon&quot;, &quot;point&quot;, &quot;valid&quot;) val out_table_temp = s&quot;pings.${c(&quot;source&quot;)}_${c(&quot;country&quot;)}_geocoded&quot; pings_geo_pol.write.format(&quot;delta&quot;).mode(&quot;overwrite&quot;).saveAsTable(out_table) 2.3 Finding Stops 2.3.1 Definition Pings recorded from the gps devices are often noisy: unless an user is perfectly still in a place, the mobile will record slightly different positions. As an example, if an user is at home and he moves between the different rooms of the house, we want to group all the pings under a single label, home, to analyze the time spent at each location. This would not be possible by considering individual pings. A standard procedure to reduce the noise is to compute stop locations, by spatially and temporally clustering the raw pings. ### Infostop In order to do this we need both the spatial and the temporal data from each ping. This is, how far are consecutive pings from each other in time and space. We rely on the first part of the infostop algorithm to detect stop-locations or stays. Infostop is based in some heuristics: \\(r_1\\): The maximum distance between any pair of points within the same stop. \\(n_{min}\\): The minimum number of points that make up a stop. \\(t_{min}\\): The minimum duration of a stop. \\(t_{max}\\): The maximum time difference between two consecutive pings. Then in order to have a stop we would need to satisfy that some consecutive pings are all less than \\(r_1\\) meters apart, the time span between the first and the last pings is more than \\(t_{min}\\) seconds, there’s no more than \\(t_{max}\\) seconds between consecutive pairs and each stop cannot be composed of less than \\(n_{min}\\) pings. Particularly we use the Python function get_stationary_events() in the Infostop package, that is a wrapper around an efficient c++ implementation for computing stationary events, i.e. points of a trace that are close in time and space and that can be grouped together under a single label. Each time we find a stationary event, we associate it to a stop location, with as coordinates the centroid of the points that form the event and as duration the difference in time between the first point of the event and the last point of the event. Differently from infostop, we choose to use DBSCAN instead of Infomap to cluster stop locations. ### Code #### Data preparation Once we load the required data, we make sure that they meet the timezone metadata requirements and that their coordinates are valid geographical points. filter_string = f&quot;accuracy &gt;=0 AND accuracy &lt;= 200 AND lat &gt; -90 AND lat &lt; 90 AND lon &gt; -180 AND lon &lt; 180&quot; if not tz: # add a check on TZ_OFFSET pings = spark.sql(f&quot;SELECT device_id AS user_id, lat, lon, accuracy, timestamp, TZ_OFFSET_SEC FROM default.veraset_{c.country}_tz WHERE country = &#39;{c.country}&#39; AND {filter_string}&quot;) pings = (pings .withColumn(&#39;epoch_time&#39;, col(&quot;timestamp&quot;) + col(&quot;TZ_OFFSET_SEC&quot;).cast(&quot;long&quot;)) .drop(&quot;TZ_OFFSET_SEC&quot;, &quot;timestamp&quot;)) elif tz: pings = spark.sql(f&quot;SELECT device_id AS user_id, lat, lon, accuracy, timestamp FROM default.veraset_primary_1 WHERE country = &#39;{c.country}&#39; AND {filter_string}&quot;) pings = (pings .withColumn(&#39;time&#39;, F.to_timestamp(&#39;timestamp&#39;)) .withColumn(&#39;new_time&#39;, F.from_utc_timestamp(&#39;time&#39;, tz)) .withColumn(&#39;epoch_time&#39;, F.unix_timestamp(&#39;new_time&#39;)) .drop(&#39;timestamp&#39;, &#39;time&#39;, &#39;new_time&#39;)) else: raise Exception (&quot;Undefined time zone in config or tz_offset in input table&quot;) 2.3.1.1 Get stop locations Now we have the data ready to identify stops, we only need to sort pings by their timestamp for each user. sl = (pings .orderBy(&quot;epoch_time&quot;) .groupBy(&quot;user_id&quot;) .apply(get_stop_location, args=(radius, stay_time, min_pts_per_stop_location, max_time_stop_location, max_accuracy, db_scan_radius)) .dropna()) Where the user defined function get_stop_location() composed by other functions which at the end will yield a data frame with user id, stop beginning timestamp, stop end timestamp, a centroid coordinate pair from the original pings, a cluster label from a DBSCAN clustering algorithm, the median accuracy of the pings and the total number of pings that compose the given stop. For DBSCAN we also use \\(\\epsilon=50m\\) and the minimum accuracy for us to keep a ping is \\(100m\\) def compute_intervals(centroids, labels, timestamps, accuracy, input_data): # if the label is -1 it means that the point doesn&#39;t belong to any cluster. Otherwise there should be at least 2 points for a stop locations # and they should # assert (len(centroids) == len(community_labels)) i = 0 seen = 0 trajectory = [] while i &lt; len(labels): if labels[i] == -1: i += 1 else: start_index = i while (i + 1 &lt; len(labels)) and (labels[i] == labels[i + 1]): i += 1 trajectory.append((timestamps[start_index], timestamps[i], *centroids[seen], np.median(accuracy[start_index: i]), i - start_index + 1)) seen += 1 i += 1 return trajectory def run_infostop(data, r1, min_staying_time, min_size, max_time_between, distance_metric): data_assertions(data) centroids, stat_labels = get_stationary_events( data[:, :3], r1, min_size, min_staying_time, max_time_between, distance_metric) return compute_intervals(centroids, stat_labels, data[:, 2], data[:, 3], data) schema_df = StructType([ StructField(&#39;user_id&#39;, StringType(), False), StructField(&#39;t_start&#39;, LongType(), False), StructField(&#39;t_end&#39;, LongType(), False), StructField(&#39;lat&#39;, DoubleType(), False), StructField(&#39;lon&#39;, DoubleType(), False), StructField(&#39;cluster_label&#39;, LongType(), True), StructField(&#39;median_accuracy&#39;, DoubleType(), True), StructField(&#39;total_pings_stop&#39;, LongType(), True), ]) @pandas_udf(schema_df, PandasUDFType.GROUPED_MAP) def get_stop_location(df, radius, stay_time, min_pts_per_stop_location, max_time_stop_location, max_accuracy, db_scan_radius): identifier = df[&#39;user_id&#39;].values[0] df.sort_values(by=&#39;epoch_time&#39;, inplace=True) # shouldnt be necessary data = df[[&quot;lat&quot;, &quot;lon&quot;, &#39;epoch_time&#39;, &quot;accuracy&quot;]].values res = run_infostop(data, r1=radius, min_staying_time=stay_time, min_size=min_pts_per_stop_location, max_time_between=max_time_stop_location, distance_metric=&#39;haversine&#39;) df = pd.DataFrame(res, columns=[ &quot;t_start&quot;, &quot;t_end&quot;, &quot;lat&quot;, &quot;lon&quot;, &quot;median_accuracy&quot;, &quot;total_pings_stop&quot;]) # new filtering step based on median accuracy df = df[df[&#39;median_accuracy&#39;] &lt; max_accuracy] df[&#39;user_id&#39;] = identifier if not df.empty: # df[&#39;cluster_label&#39;] = get_labels(df[[&#39;lat&#39;, &#39;lon&#39;]]) # notice that we don&#39;t have noise here, since any point that we consider is a stop location and hence has been already pre filtered by run_infostop (min_samples = 1 =&gt; no label =-1) db = DBSCAN(eps=db_scan_radius, min_samples=1, metric=&#39;haversine&#39;, algorithm=&#39;ball_tree&#39;).fit(np.radians(df[[&#39;lat&#39;, &#39;lon&#39;]].values)) df[&#39;cluster_label&#39;] = db.labels_ else: df[&#39;cluster_label&#39;] = None return df 2.3.2 Clustering recurrent stops The clustering part is where our approach differs from the Infostop one, as the assign cluster labels by building a network of stops if they’re located closer than a distance \\(r_2\\) and then use the Infomap community detection algorithm while we proceed by clustering with DBSCAN as previously discussed. It is important to notice that these clusters don’t depend on the time but only on the spatial distribution as we are interested in finding recurrent visited locations. Finally we split those stops that span multiple days into mutiple stops that are within a day: i.e. if we have a stop location that spans from 22pm to 1am, we will split it into a stop location from 22pm to 23.59pm and another stop location from 00.00am to 1am. sl = (sl .withColumn(&quot;total_duration_stop_location&quot;, F.col(&quot;t_end&quot;) - F.col(&quot;t_start&quot;)) .withColumn(&#39;my_list&#39;, make_list(F.to_timestamp(col(&#39;t_start&#39;)), F.to_timestamp(col(&#39;t_end&#39;)))) .drop(&#39;t_start&#39;, &#39;t_end&#39;) .withColumn(&quot;tmp&quot;, F.explode(&quot;my_list&quot;)) .withColumn(&quot;t_start&quot;, F.col(&quot;tmp&quot;).t_start) .withColumn(&quot;t_end&quot;, F.col(&quot;tmp&quot;).t_end) .drop(&quot;tmp&quot;, &quot;my_list&quot;) .withColumn(&quot;duration&quot;, F.col(&quot;t_end&quot;) - F.col(&quot;t_start&quot;))) 2.3.3 Appending pings from recent dates GPS data is passively gathered and as so it’s continiously growing, this translates in the need for contiously find stops and assign cluster labels to them. This whole process is very computationally expensive and running it on the whole data would be highly inefficient, this is why we keep old stops and only compute new ones, but there’s no way around the clustering part as the labels can change every time. Fortunately the number of stops is several order magnitudes lower than the pings as the smallest stop has at least two of them and we drop those that didn’t make up a stop. 2.3.4 Stops geocoding Now that our data is composed by new aggregated points we replicate the indexing from 2.2 but with the stops instead of the pings. 2.4 Defining Home and Work Locations From the census data we have socioeconomic information for every geographical region in each country and we used that to assign a Wealth Index value to them. A good proxy of the demographic of the users can be obtained by the location of their home which we can infeer from the recurrent stop locations we did in the previous chapter 2.3 given some regularity patterns and use the administrative information of the geometry where it falls into. Moreover we can do this for every stops’ cluster. Our focus lies on the home and the work locations as they’re likely to be the places where indiviuals spend more time and define the largest bulk of their mobility patterns. 2.4.1 Seasonal patterns Figure 2.2: Number of stops by weekday and start hour at home location Figure 2.3: Number of stops by weekday and start hour at work location As we can see from 2.2 and 2.3 usually a person’s mobility patterns have seasonal structure so we label each stop with it’s corresponding day of the week. For this reason we need to label each stop with the weekday value when it was created. 2.4.2 Code df = df.withColumn(&quot;t_start_hour&quot;, F.hour( F.to_timestamp(&quot;t_start&quot;))) .withColumn(&quot;t_end_hour&quot;, F.hour( F.to_timestamp(&quot;t_end&quot;))) .withColumn(&#39;weekday&#39;, F.dayofweek( F.to_timestamp(&quot;t_start&quot;))) .withColumn(&quot;date&quot;, F.to_timestamp(&quot;t_start&quot;)) .withColumn(&quot;date_trunc&quot;, F.date_trunc(&quot;day&quot;, F.col(&quot;date&quot;))) Then we use the following function to label the home and work candidates. Notice that as this is a user defined function, the schema at the top contains the output variables. schema_df = StructType([ StructField(&#39;user_id&#39;, StringType(), False), StructField(&#39;t_start&#39;, LongType(), False), StructField(&#39;t_end&#39;, LongType(), False), StructField(&#39;duration&#39;, LongType(), False), StructField(&#39;lat&#39;, DoubleType(), False), StructField(&#39;lon&#39;, DoubleType(), False), StructField(&#39;total_duration_stop_location&#39;, LongType(), False), StructField(&#39;total_pings_stop&#39;, LongType(), False), StructField(&#39;cluster_label&#39;, LongType(), False), StructField(&#39;median_accuracy&#39;, DoubleType(), False), StructField(&#39;location_type&#39;, StringType(), True), StructField(&#39;home_label&#39;, LongType(), True), StructField(&#39;work_label&#39;, LongType(), True), StructField(&#39;geom_id&#39;, StringType(), False), StructField(&#39;date&#39;, TimestampType(), True), StructField(&#39;t_start_hour&#39;, IntegerType(), True), StructField(&#39;t_end_hour&#39;, IntegerType(), True), StructField(&quot;date_trunc&quot;, TimestampType(), True) ]) @pandas_udf(schema_df, PandasUDFType.GROUPED_MAP) def compute_home_work_label_dynamic(user_df, start_hour_day, end_hour_day, min_pings_home_cluster_label, work_activity_average): # We start by assuming every cluster falls into the &quot;Other&quot; category. # Meaning they&#39;re not home nor work. user_df[&#39;location_type&#39;] = &#39;O&#39; user_df[&#39;home_label&#39;] = -1 user_df[&#39;work_label&#39;] = -1 # HOME # filter candidates as night-time stops home_tmp = user_df[(user_df[&#39;t_start_hour&#39;] &gt;= end_hour_day) | ( user_df[&#39;t_end_hour&#39;] &lt;= start_hour_day)].copy() # restrictive version of daytimes if home_tmp.empty: # if we don&#39;t have at least a home location, we return and not attemp to compute work location return remove_unused_cols(user_df) first_day = home_tmp[&#39;date_trunc&#39;].min() last_day = home_tmp[&#39;date_trunc&#39;].max() # work on clusters with &quot;duration&quot; attribute per day (&quot;date_trunc&quot;) home_tmp = home_tmp[[&#39;cluster_label&#39;, &#39;date_trunc&#39;, &#39;duration&#39;, &#39;total_pings_stop&#39;]].groupby( [&#39;cluster_label&#39;, &#39;date_trunc&#39;]).sum().reset_index().sort_values(&#39;date_trunc&#39;) # computer cumulative duration of candidates over &quot;period&quot; window home_tmp = home_tmp.merge(home_tmp[[&#39;date_trunc&#39;, &#39;cluster_label&#39;, &#39;duration&#39;, &#39;total_pings_stop&#39;]].groupby( [&#39;cluster_label&#39;]).apply(home_rolling_on_date).reset_index(), on=[&#39;date_trunc&#39;, &#39;cluster_label&#39;], suffixes=(&#39;&#39;, &#39;_cum&#39;)) print(home_tmp.columns) ###### home_tmp = home_tmp[home_tmp.total_pings_stop_cum &gt; min_pings_home_cluster_label].drop(&#39;total_pings_stop_cum&#39;, axis=1) # filter out nan rows, equivalent to filter on min_days home_tmp = home_tmp.dropna(subset=[&#39;duration_cum&#39;]) if home_tmp.empty: # if we don&#39;t have at least a home location, we return and not attemp to compute work location return remove_unused_cols(user_df) ##################### date_cluster = home_tmp.drop_duplicates([&#39;cluster_label&#39;, &#39;date_trunc&#39;])[ [&#39;date_trunc&#39;, &#39;cluster_label&#39;]].copy() date_cluster = date_cluster.drop_duplicates([&#39;date_trunc&#39;]) home_label = list(zip(date_cluster.cluster_label, date_cluster.date_trunc)) # creating a multinidex over which locating tuples of &quot;date_trunc&quot; and &quot;home_label&quot; idx = pd.MultiIndex.from_frame(user_df[[&#39;cluster_label&#39;, &#39;date_trunc&#39;]]) user_df.loc[idx.isin(home_label), &#39;home_label&#39;] = user_df.loc[idx.isin( home_label), &#39;cluster_label&#39;] ##################### base_dates = pd.date_range(start=first_day, end=last_day) date_cluster = date_cluster.sort_values( by=&#39;date_trunc&#39;).set_index(&#39;date_trunc&#39;) date_cluster = date_cluster.reindex(base_dates) if pd.notna(date_cluster[&#39;cluster_label&#39;]).sum() &gt; 1: date_cluster = date_cluster.interpolate( method=&#39;nearest&#39;).ffill().bfill() else: date_cluster = date_cluster.ffill().bfill() date_cluster.index.name = &#39;date_trunc&#39; date_cluster = date_cluster.reset_index() home_label = list(zip(date_cluster.cluster_label, date_cluster.date_trunc)) # creating a multindex over which locating tuples of &quot;date_trunc&quot; and &quot;home_label&quot; idx = pd.MultiIndex.from_frame(user_df[[&#39;cluster_label&#39;, &#39;date_trunc&#39;]]) user_df.loc[idx.isin(home_label), &#39;location_type&#39;] = &#39;H&#39; home_list = home_tmp.cluster_label.unique() if home_list.size == 0: return remove_unused_cols(user_df) ######## # WORK # ######## work_tmp = user_df[~(user_df[&#39;cluster_label&#39;].isin(home_list))].copy() if work_tmp.empty: # if we can&#39;t compute work location we return return remove_unused_cols(user_df) # if daytime: ######don&#39;t like it work_tmp = work_tmp[((work_tmp[&#39;t_start_hour&#39;] &gt;= start_hour_day+4) &amp; (work_tmp[&#39;t_end_hour&#39;] &lt;= end_hour_day-6)) &amp; (~work_tmp[&#39;weekday&#39;].isin([1, 7]))] # restrictive version of daytimes if work_tmp.empty: # if we can&#39;t compute work location we return return remove_unused_cols(user_df) first_day = work_tmp[&#39;date_trunc&#39;].min() # drop duplicates, smooth over &quot;period&quot; time window work_tmp = work_tmp[[&#39;date_trunc&#39;, &#39;cluster_label&#39;, &#39;duration&#39;]].groupby( [&#39;cluster_label&#39;, &#39;date_trunc&#39;]).sum().reset_index() work_tmp = work_tmp.merge(work_tmp[[&#39;date_trunc&#39;, &#39;cluster_label&#39;, &#39;duration&#39;]] .groupby([&#39;cluster_label&#39;]) .apply(work_rolling_on_date) .reset_index(), on=[&#39;date_trunc&#39;, &#39;cluster_label&#39;], suffixes=(&#39;&#39;, &#39;_average&#39;)) # filter out candidates which on average on the period do not pass the constraint work_tmp = work_tmp[(work_tmp.duration_average &gt;= work_activity_average)] # Select work clusters candidate: the clusters that passed the previous criteria are selected as work for the day if work_tmp.empty: # if we can&#39;t compute work location we return return remove_unused_cols(user_df) ##################### work_label = list(zip(work_tmp.cluster_label, work_tmp.date_trunc)) idx = pd.MultiIndex.from_frame(user_df[[&#39;cluster_label&#39;, &#39;date_trunc&#39;]]) work_list = work_tmp.cluster_label.unique() if work_list.size == 0: return remove_unused_cols(user_df) # add cluster label to work_label on the day on which it is found to be work_location only user_df.loc[idx.isin(work_label), &#39;work_label&#39;] = user_df.loc[idx.isin( work_label), &#39;cluster_label&#39;] ##################### # add work labels to all user dataset work_label = work_tmp[&#39;cluster_label&#39;].unique() idx = pd.Index(user_df[&#39;cluster_label&#39;]) user_df.loc[idx.isin(work_label), &#39;location_type&#39;] = &#39;W&#39; return remove_unused_cols(user_df) 2.5 Mobility Patterns Individual mobility analysis represent a key ingredient to understand and timely follow changes in their behavioural patterns. In this book we relie on heavily tested and robust GPS traces preprocessing to measure and quantify individual mobility patterns. 2.5.1 Socio-economic groups Particular attention is devoted to different socio-economig groups. Groups are assigned to individuals based on the administrative unit where their principal pre-pandemic home-location is detected. Three main socio-economic groups are analysed here: the 40% with lowest index value (the 0% to 40% administrative units with lowest wealth index value), the middle ones (from 40% to 80%), and the top 20% (the wealthiest 80 to 100 percentiles). Socio-economic groups are drawn both by considering the entire country wealth index percentiles and by considering percentiles relatively to the metropolitan area administrative units are part of. The computation is performed by loading and grouping administrative information “admin.csv” files for each country using pyspark. def process_admin(country,admin_path): &#39;&#39;&#39; INPUT: country ISO code, path to the admin files (saved as &quot;{country}/admin.csv&quot; files) OUTPUT: three pandas dataframes - &quot;admins_by_country&quot;: all country administrative units with socio-economic group assigned based on entire country - &quot;admins_by_metro_area&quot;: wealth groups computed relatively to the emtropolitan area of each unit - total population of the metropolitan areas &#39;&#39;&#39; cols = [&#39;geom_id&#39;, &#39;metro_area_name&#39;, &#39;pop&#39;, &#39;wealth_index&#39;] admin = spark.read.option(&#39;header&#39;, &#39;true&#39;).csv(admin_path+f&#39;{country}/admin.csv&#39;).toPandas() admins = admins[[cols]] admins = admins.rename(columns={&#39;metro_ar_1&#39;: &#39;metro_area_name&#39;, &#39;wealth_ind&#39;: &#39;wealth_index&#39;}) admins_by_country = admins[[&#39;geom_id&#39;, &#39;pop&#39;, &#39;wealth_index&#39;]].dropna( ).sort_values(by=[&#39;wealth_index&#39;], ascending=[False]).reset_index(drop=True) admins_by_country[&#39;pct_wealth&#39;] = admins_by_country[&#39;pop&#39;].cumsum().divide( admins_by_country[&#39;pop&#39;].sum()) admins_by_metro_area = admins[[&#39;geom_id&#39;, &#39;metro_area_name&#39;, &#39;pop&#39;, &#39;wealth_index&#39;]].dropna( ).sort_values(by=[&#39;metro_area_name&#39;, &#39;wealth_index&#39;], ascending=[True, False]).reset_index(drop=True) admins_by_metro_area[&#39;pct_wealth&#39;] = admins_by_metro_area.groupby( &#39;metro_area_name&#39;)[&#39;pop&#39;].apply(lambda x: x.cumsum()/x.sum()) pop_metro_areas = admins_by_metro_area.groupby( &#39;metro_area_name&#39;)[&#39;pop&#39;].sum().sort_values(ascending=False) return admins_by_country, admins_by_metro_area, pop_metro_areas 2.5.2 Individual’s selection Following the preprocessing pipeline description presented in the previous sections of the book, here we introduce an additional step aiming at refinining the pool of individuals analysed. Our intent is to measure mobility patterns only for individuals whose activity level is sufficiently high to ensure a minimum level of representativeness of the data. Individuals with recods over only a small number of days cannot be considered as a complete and correct representation fairly proxying the mobility features of an individual. To this extent we use the duration of each individuals records, which have been computed during the pipeline run. The selection imposes two requirements on individuals: i) each individual should be active (i.e. should have at least one stop record) for a minimum number of days during the pre-pandemic period (which is choose to run from the beginning of January 2020 until March 15, 2020; ii) each individual should be active for a minimum fraction of days after the pre-pandemic period until the end of 2020. This selection process is obtained invoking the following python function which returns a list of the users to be included during the analyses. def get_active_list(durations, country, activity_level): &#39;&#39;&#39; For each country invoke the following function to get a list of all active individuals. INPUT: spark dataframe with precomputed individual stops&#39; durations; country ISO code; minimum activity level required OUTPUT: list of active individuals &quot;user_id&quot; &#39;&#39;&#39; # Indonesia experiences a major dropout from the service during January 2020. For this reason, a specific pre-pandemic period was adopted if country == &#39;ID&#39;: durations_2 = durations.where(col(&#39;date_trunc&#39;) &gt;= &#39;2020-02-01&#39;) else: durations_2 = durations durations_2 = durations_2.where(col(&#39;date_trunc&#39;) &lt; &#39;2021-01-01&#39;) active_days = (durations_2 .withColumn(&#39;pandemic&#39;, F.when(col(&#39;date_trunc&#39;) &lt; &#39;2020-03-15&#39;, &#39;pre&#39;).otherwise(&#39;post&#39;)) .groupby(&#39;user_id&#39;, &#39;pandemic&#39;) .agg(F.countDistinct(&#39;date_trunc&#39;).alias(&#39;n_days&#39;))) active_days.cache() max_days_pre = (active_days .where(col(&#39;pandemic&#39;) == &#39;pre&#39;) .agg(F.max(&#39;n_days&#39;).alias(&#39;max_days_pre&#39;)) .toPandas().loc[0, &#39;max_days_pre&#39;]) max_days_all = (active_days .groupby(&#39;user_id&#39;) .agg(F.sum(&#39;n_days&#39;).alias(&#39;n_days&#39;)) .agg(F.max(&#39;n_days&#39;).alias(&#39;max_days_all&#39;)) .toPandas().loc[0, &#39;max_days_all&#39;]) active_users = (active_days .groupby(&#39;user_id&#39;) .pivot(&#39;pandemic&#39;) .agg(F.first(&#39;n_days&#39;)) .fillna(0) .withColumn(&#39;tot&#39;, col(&#39;pre&#39;)+col(&#39;post&#39;)) .where(col(&#39;pre&#39;) &gt;= activity_level*max_days_pre) .where(col(&#39;tot&#39;) &gt;= activity_level*max_days_all)) active_days.unpersist() return active_users 2.5.3 Measures The focus on socio-economic groups require an aggregation process which is performed for each individuals based on their home-location administrative unit. Both stop-locations and duration data processed during the pipeline run are used here. def compute_durations_and_admins(country, data_date, stop_path, activity_level=0, hw=28, ww=28, wa=900, mph=10, mpw=7): &#39;&#39;&#39; INPUT: country ISO code, date of the data to consider, path to the stop locations data, minimum activity level for individuals to be included, **{pipeline computation specific parameters} OUTPUT: spark dataframe containing &quot;date&quot; (with daily frequency), &quot;user_id&quot;, &quot;H&quot; (time spent at home), &quot;W&quot; (time spent at work), &quot;R&quot; (1 if not leaving home), &quot;C&quot; (1 if commuting to work), &quot;O&quot; (time spent in locations which are neither home nor work) &#39;&#39;&#39; personal_nf = f&quot;personal_stop_location_hw{hw}_ww{ww}_wa{wa}_mph{mph}_mpw{mpw}&quot; stops = spark.read.parquet(f&quot;{stop_path}{country}/accuracy100_maxtimestop3600_staytime300_radius50_dbscanradius50/date{data_date}/&quot; + personal_nf) fname_nf = f&#39;durations_window_hw{hw}_ww{ww}_wa{wa}_mph{mph}_mpw{mpw}&#39; durations_path_nf = f&#39;{stop_path}{country}/accuracy100_maxtimestop3600_staytime300_radius50_dbscanradius50/date{data_date}/&#39; + fname_nf durations = spark.read.parquet(durations_path_nf) # aggregate day/night durations = (durations .groupby(&#39;date_trunc&#39;, &#39;user_id&#39;) .agg(F.sum(&#39;H&#39;).alias(&#39;H&#39;), F.sum(&#39;W&#39;).alias(&#39;W&#39;), F.sum(&#39;O&#39;).alias(&#39;O&#39;))) active_users = get_active_list(durations, country, activity_level) durations = durations.join(active_users.select( &#39;user_id&#39;), on=&#39;user_id&#39;, how=&#39;inner&#39;) # create binary column for commuters durations = durations.withColumn( &#39;C&#39;, F.when(col(&#39;W&#39;).isNull(), 0).otherwise(1)) # create binary column for people who don&#39;t leave home, aka recluse durations = durations.withColumn(&#39;R&#39;, F.when( (col(&#39;W&#39;).isNull()) &amp; (col(&#39;O&#39;).isNull()), 1).otherwise(0)) # compute H and W id for wealth labels w = Window.partitionBy(&#39;user_id&#39;) user_h_id = (stops .where(col(&#39;location_type&#39;) == &#39;H&#39;) .where(col(&#39;date_trunc&#39;) &lt;= &#39;2020-03-15&#39;) .groupby(&#39;user_id&#39;, &#39;geom_id&#39;) .agg(F.countDistinct(&#39;date_trunc&#39;).alias(&#39;n_days&#39;)) .withColumn(&#39;max_days&#39;, F.max(&#39;n_days&#39;).over(w)) .where(col(&#39;n_days&#39;) == col(&#39;max_days&#39;)) .groupby(&#39;user_id&#39;) .agg(F.first(&#39;geom_id&#39;).alias(&#39;geom_id_home&#39;))) user_w_id = (stops .where(col(&#39;location_type&#39;) == &#39;W&#39;) # .where(col(&#39;date_trunc&#39;) &lt;= &#39;2020-03-15&#39;) .groupby(&#39;user_id&#39;, &#39;geom_id&#39;) .agg(F.countDistinct(&#39;date_trunc&#39;).alias(&#39;n_days&#39;)) .withColumn(&#39;max_days&#39;, F.max(&#39;n_days&#39;).over(w)) .where(col(&#39;n_days&#39;) == col(&#39;max_days&#39;)) .groupby(&#39;user_id&#39;) .agg(F.first(&#39;geom_id&#39;).alias(&#39;geom_id_work&#39;))) durations_and_admins = (durations .withColumnRenamed(&#39;date_trunc&#39;, &#39;date&#39;) .select(&#39;date&#39;, &#39;user_id&#39;, &#39;H&#39;, &#39;R&#39;, &#39;W&#39;, &#39;C&#39;, &#39;O&#39;) .join(user_h_id, on=&#39;user_id&#39;, how=&#39;left&#39;) .join(user_w_id, on=&#39;user_id&#39;, how=&#39;left&#39;)) return durations_and_admins The aggregation process takes the output of “compute_durations_and_admins” and compute the weighted average of the different metrics of interest. Before focusing on the aggregated metrics produced, it is important to highlight that during this process metrics are reweighted based on the fraction of population living in administrative units in which at least one individual in our dataset is living. This step rebalance the data satial-coverage biases giving more importance to individuals living in administrative units with larger population. def compute_durations_normalized_by_wealth_home(durations_and_admins, admins, labels_wealth, bins_wealth): &#39;&#39;&#39; INPUT: df output of &quot;compute_durations_and_admins&quot;, df (first) output of &quot;process_admins&quot;, wealth labels, wealth bins OUTPUT: df with weight column dataframe &#39;&#39;&#39; admins[&#39;wealth_label&#39;] = pd.cut( admins[&#39;pct_wealth&#39;], bins_wealth, labels=labels_wealth) admins[&#39;geom_id&#39;] = admins[&#39;geom_id&#39;].astype(str) admins[&#39;wealth_label&#39;] = admins[&#39;wealth_label&#39;].astype(str) # get admin info for home and work location tmp1 = spark.createDataFrame( admins[[&#39;geom_id&#39;, &#39;pop&#39;, &#39;pct_wealth&#39;, &#39;wealth_label&#39;]].rename(columns=lambda x: x+&#39;_home&#39;)) out1 = (durations_and_admins .join(tmp1, on=&#39;geom_id_home&#39;, how=&#39;inner&#39;)) geom_users = (out1 .groupby(&#39;geom_id_home&#39;) .agg(F.countDistinct(&#39;user_id&#39;).alias(&#39;n_users&#39;))) out = (out1 .join(geom_users, on=&#39;geom_id_home&#39;, how=&#39;inner&#39;) .withColumn(&#39;weight&#39;, col(&#39;pop_home&#39;)/col(&#39;n_users&#39;))) return out def output(out, column): &#39;&#39;&#39; INPUT: df output of &quot;compute_durations_normalized_by_wealth_home&quot;, measure to aggregate OUTPUT: aggregated and normalized dataframe &#39;&#39;&#39; # compute aggregate measures out = (out .fillna(0, subset=column) .groupby(&#39;date&#39;, &#39;wealth_label_home&#39;) .agg((F.sum(col(column)*col(&#39;weight&#39;))/F.sum(col(&#39;weight&#39;))).alias(&#39;mean&#39;), F.stddev(column).alias(&#39;std&#39;), F.count(column).alias(&#39;n&#39;), F.countDistinct(&#39;user_id&#39;).alias(&#39;n_unique&#39;)) .withColumn(&#39;sem&#39;, col(&#39;std&#39;)/F.sqrt(col(&#39;n&#39;))) .drop(&#39;std&#39;)) durations_normalized_by_wealth_home = out.toPandas( ).set_index([&#39;wealth_label_home&#39;, &#39;date&#39;]) return durations_normalized_by_wealth_home 2.5.3.1 Measuring mobility patterns We can now go back and discuss the metrics coded so far which we selected as proxies of different aspect of individuals mobility behaviour. “H” (time spent at home“),”W\" (time spent at work), “O” (time spent in locations which are neither home nor work) are quantities directly computed as the weighted average of the time spent in three diferent location categories, namely, the home-locations of an individuals, its work locations, and all the other locations which are neither home nor work. Additionally, we also compute the fraction of people not leaving their home-location on an entire day, “R,” and the fraction of commuters going to their workplace on a specific date, “C.” All these quantities are computed and aggregated following the wealth bins provided (e.g. “low-income”: 0-40% wealth-index percentile, “middle income”: 40-80% wealth-index percentile, “high-income”: 80-100% wealth-index percentile). To further specialize on most fragile individuals’ groups, we focus our attention on i) the “low-income” individuals and their fraction of commuters, and ii) the “low-income” individuals and thir time spent at their work places. These two quantities aims at providing in-dept insights over the working behaviour and its changes during the pandeimc. Here we report an example code using the functions presented in the previous sections and the data computed by running the preprocessing pipeline. import os import pandas as pd import matplotlib.pyplot as plt import matplotlib.dates as mdates import numpy as np import seaborn as sns import pyspark.sql.functions as F from pyspark.sql.functions import col, desc, lit from pyspark.sql import Window spark.conf.set(&quot;spark.sql.shuffle.partitions&quot;, 1500) start_baseline = &#39;2020-02-01&#39; end_baseline = &#39;2020-03-01&#39; start_date = &#39;2020-02-01&#39; end_date = &#39;2021-04-15&#39; results_dir = &quot;SAVE_RESULTS_HERE&quot; admin_path = &quot;FIND_ADMIN_DATA_HERE&quot; # country specific admin files should be stored as &quot;{admin_path}/{country}/admin.csv&quot; stop_path = &quot;FIND_PIPELINE_STOP_DATA_HERE&quot; # define wealth groups bins_wealth = [0, 0.4, 0.8, 1] bins_dist = [0, 0.4, 0.8, 1] # select countries for which produce metric results countries = [&#39;ID&#39;, &#39;PH&#39;, &#39;BR&#39;, &#39;CO&#39;, &#39;MX&#39;, &#39;ZA&#39;] c_dates = {&#39;BR&#39;: &#39;2021-05-16&#39;, &#39;CO&#39;: &#39;2021-05-16&#39;, &#39;ID&#39;: &#39;2021-05-16&#39;, &#39;MX&#39;: &#39;2021-05-16&#39;, &#39;PH&#39;: &#39;2021-05-16&#39;, &#39;ZA&#39;: &#39;2021-05-16&#39;, &#39;AR&#39;: &#39;2021-05-16&#39;} activity_level = 0.2 hw = 49 ww = 49 wa = 3600 mph = 0.2 mpw = 0.2 # smoothing window size ma = 28 # weath-group names labels_wealth = [x+&#39; (&#39;+str(int(bins_wealth[i]*100))+&#39;%-&#39;+str(int(bins_wealth[i+1]*100)) +&#39;%)&#39; for i, x in zip(range(len(bins_wealth)-1), [&#39;Low&#39;, &#39;Medium&#39;, &#39;High&#39;])] labels_dist = [x+&#39; (&#39;+str(int(bins_dist[i]*100))+&#39;%-&#39;+str(int(bins_dist[i+1]*100)) +&#39;%)&#39; for i, x in zip(range(len(bins_dist)-1), [&#39;Low&#39;, &#39;Medium&#39;, &#39;High&#39;])] # largest metropolis names country_capitals = {&#39;MX&#39;: &#39;Mexico City&#39;, &#39;BR&#39;: &#39;São Paulo&#39;, &#39;CO&#39;: &#39;Bogota&#39;, &#39;PH&#39;: &#39;Quezon City [Manila]&#39;, &#39;ID&#39;: &#39;Jakarta&#39;, &#39;ZA&#39;: &#39;Johannesburg&#39;, &#39;AR&#39;: &#39;GBuenos Aires&#39;} # initialize dictionary where to store results results = {} for country in countries: print(country) admins_by_country, admins_by_metro_area, pop_metro_areas = process_admin(country,admin_path) capital_geomid = admins_by_metro_area[admins_by_metro_area.metro_area_name == country_capitals[country]].geom_id.unique().tolist() durations_and_admins = compute_durations_and_admins(country, c_dates[country], stop_path, activity_level=activity_level, hw=hw, ww=ww, wa=wa, mph=mph, mpw=mpw) durations_and_admins.cache() out1 = compute_durations_normalized_by_wealth_home(durations_and_admins, admins_by_country, labels_wealth, bins_wealth) out2 = compute_durations_normalized_by_wealth_home_wealth_work(durations_and_admins, admins_by_country, labels_wealth, bins_wealth) results[(country, &#39;t_home&#39;)] = output(out1, column=&#39;H&#39;) results[(country, &#39;t_work&#39;)] = output(out1, column=&#39;W&#39;) results[(country, &#39;t_other&#39;)] = output(out1, column=&#39;O&#39;) results[(country, &#39;rec&#39;)] = output(out1, column=&#39;R&#39;) results[(country, &#39;comms&#39;)] = output(out1, column=&#39;C&#39;) results[(country, &#39;comms_hw&#39;)] = output_hw(out2, column=&#39;C&#39;) results[(country, &#39;t_work_hw&#39;)] = output_hw(out2, column=&#39;W&#39;) durations_and_admins.unpersist() # save all countries results in a single .csv file res2 = pd.DataFrame(columns=[&#39;state&#39;, &#39;measure&#39;, &#39;wealth_label_home&#39;, &#39;date&#39;, &#39;mean&#39;, &#39;sem&#39;]) for key, res in results.items(): res_tmp = res.reset_index().copy() res_tmp[&#39;state&#39;], res_tmp[&#39;measure&#39;] = key res2 = res2.append(res_tmp, ignore_index=True) res2.to_csv(results_dir+&#39;all_hw_weighted.csv&#39;) 2.5.3.2 Measuring change It is of particular interest, due to the dramatic societal impact the pandemic had at a global scale, to study these quantities in relation to the pre-pandemic periods. A comparison bethween individuals mobility behaviour from the months preceding the explosive diffusion of the SARS-ncov virus represent an important indicator of how much each socio-economic group was affected by the local regulations and how individuals were forced to reshape their daily mobility patterns in response to the pandemic threat. To this extent, we implemented a change metric which directly compare the levels of the metrics discussed in terms of the period going from the beginning of January 2020 and March 15, 2020. Change is computed on a daily basis, comparing the level of a specific date with the day-of-the-week median value of the same metric. This accounts for weekly periodical effects, accounting, for example, for the physiological reduction in the number of commuters during weekends. def google_change_metric(df_original, start_baseline, end_baseline,other_groups=[]): &#39;&#39;&#39; INPUT: dataframe with (at least) 2 columns named &quot;mean&quot; and &quot;sem&quot; OUTPUT: dataframe with the values of the two columns converted to google change metric NOTE: Google uses as baseline period the 5-weeks period from Jan 3 to Feb 6 &#39;&#39;&#39; df = df_original.copy() # compute weekday baseline values baseline = df.loc[start_baseline:end_baseline,[&#39;mean&#39;,&#39;sem&#39;]+other_groups].copy() baseline[&#39;weekday&#39;] = list(baseline.index.dayofweek.values) baseline = baseline.groupby([&#39;weekday&#39;]+other_groups,dropna=False,as_index=False).mean() df[&#39;weekday&#39;] = list(df.index.dayofweek.values) date = df.index.copy() df = df.merge(baseline, on=[&#39;weekday&#39;]+other_groups, how=&#39;left&#39;, =(&#39;&#39;, &#39;_baseline&#39;)) # compute &quot;mean&quot; change with respect to weekday baseline values df[&#39;mean&#39;] = (df[&#39;mean&#39;]- df[&#39;mean_baseline&#39;]) / np.abs(df[&#39;mean_baseline&#39;]) df[&#39;sem&#39;] = np.abs(df[&#39;sem&#39;]/df[&#39;mean_baseline&#39;]) df.index = date # return input dataframe with &quot;mean&quot; and &quot;sem&quot; column now expressing the relative change and its error return df.drop([&#39;weekday&#39;,&#39;mean_baseline&#39;],axis=1,errors=&#39;ignore&#39;) 2.5.3.3 Home-isolated, commuters and low-wealth commuters In this section we present a summary of the results for three different metrics computed for six different countries. Countries are selected among middle-low income countries to include at least one state on three different continents: America, Asia and Africa. Similar pattern changes are found between socio-economics groups across the globe. Figure 2.4: Results for six different countries (rows): Brazil, Colombia, Indonesia, Mexico, Philippines, and South Africa. Measuring (columns): relative change in the fraction of people not leaving home, relative change of the number of people commuting from home to workplace, relative change of individuals from living in low-wealth admin units. The three coloured lines distinguishes the metric changes for the three different socio-economic groups. 2.6 Migration Patterns 2.6.1 Description A great amount of media attention has been focusing on how large cities are being empitied out by the pandemic. We are also interested in this behavior and used similar process as the described in the previous chapter 2.5, and as the home labeling process we did allowed for multiple homes in a certain time window then we could use a change of administrative unit in the residential location of the users. For this study we focused on the largest city of each country: Country Code City Brazil BR São Paulo Colombia CO Bogota Indonesia ID Jakarta Mexico MX Mexico City Philippines PH Quezon City (Manila) South Africa ZA Johannesburg 2.6.2 User selection Here, we restrict our sample 2.5.2 of individuals to only active users (as defined in the previous chapter) who had a home location in the city at any given time. It’s important to note that although we use the term city we are using the whole extent of their metropolitan area. In order to determine a wealth group for them we take the administrative unit they had at the moment when their home location was in the city and in case they had more than one we only kept the one where the most time was spent. The wealth percentiles represent the cummulative population of the administrative units ordered by their wealth index. This means that users that fall into the Low [0%-40%] group are the ones with their house in the areas with the lowest wealth index that make up to \\(40\\%\\) of the population. 2.6.3 Code We select the largest city and break their wealth index in 3 bins. And define a column to discern from urban and rural places. The bins and labels are the same as the ones from the previous chapter 2.5. def read_admin(country): admin_path = f&#39;/mnt/Geospatial/admin/{country}/admin.csv&#39; admin = spark.read.option(&#39;header&#39;, &#39;true&#39;).csv(admin_path) admin = (admin .withColumn(&#39;urban/rural&#39;, F.when(col(&#39;metro_area_name&#39;).isNull(), lit(&#39;rural&#39;)).otherwise(lit(&#39;urban&#39;))) .select(&#39;geom_id&#39;, &#39;urban/rural&#39;)) return admin admins, admins_by_metro_area, pops = process_admin(country) admins[&#39;geom_id&#39;] = admins[&#39;geom_id&#39;].astype(str) admins_by_metro_area[&#39;wealth_label&#39;] = pd.cut(admins_by_metro_area[&#39;pct_wealth&#39;], bins_wealth, labels=labels_wealth) admins_by_metro_area[&#39;geom_id&#39;] = admins_by_metro_area[&#39;geom_id&#39;].astype(str) admins_by_metro_area[&#39;wealth_label&#39;] = admins_by_metro_area[&#39;wealth_label&#39;].astype(str) admins_by_metro_area = admins_by_metro_area.loc[admins_by_metro_area.metro_area_name == pops.reset_index().metro_area_name[0]] admins = spark.createDataFrame(admins_by_metro_area).select(&#39;geom_id&#39;, &#39;wealth_label&#39;) admin_rural = read_admin(country) Then we select our user sample, which are the ones with a minimum percentage of active days (\\(20\\%\\)) using the same function get_active_list() and select the users with a home in the city. # get list of active users personal_nf = f&quot;personal_stop_location_hw{hw}_ww{ww}_wa{wa}_mph{mph}_mpw{mpw}&quot; stops = spark.read.parquet(f&quot;{stop_path}{country}/accuracy100_maxtimestop3600_staytime300_radius50_dbscanradius50/date{data_date}/&quot; + personal_nf) fname_nf = f&#39;durations_window_hw{hw}_ww{ww}_wa{wa}_mph{mph}_mpw{mpw}&#39; durations_path_nf = f&#39;{stop_path}{country}/accuracy100_maxtimestop3600_staytime300_radius50_dbscanradius50/date{data_date}/&#39; + fname_nf durations = spark.read.parquet(durations_path_nf) active_users = get_active_list(durations, country, activity_level) # read stops, filter actives, get most frequented daily geom id, and get rural/urban info admins_by_country, admins_by_metro_area, pop_metro_areas = process_admin(country) metro = (admins_by_metro_area .loc[admins_by_metro_area.metro_area_name == pop_metro_areas .reset_index() .head(1) .metro_area_name .to_list()[0]][&quot;geom_id&quot;] .to_list()) users_metro = stops.where(col(&#39;location_type&#39;) == &#39;H&#39;).filter(F.col(&quot;geom_id&quot;).isin(metro)).select(&#39;user_id&#39;).distinct() stops = stops.join(users_metro, on=&#39;user_id&#39;, how=&#39;inner&#39;).join(active_users, on=&#39;user_id&#39;, how=&#39;inner&#39;) w = Window.partitionBy(&#39;user_id&#39;) user_geom = (stops .where(col(&#39;location_type&#39;) == &#39;H&#39;) .filter(F.col(&quot;geom_id&quot;).isin(metro)) .groupby(&#39;user_id&#39;, &#39;geom_id&#39;) .agg(F.countDistinct(&#39;date_trunc&#39;).alias(&#39;n_days&#39;)) .withColumn(&#39;max_days&#39;, F.max(&#39;n_days&#39;).over(w)) .where(col(&#39;n_days&#39;) == col(&#39;max_days&#39;)) .groupby(&#39;user_id&#39;) .agg(F.first(&#39;geom_id&#39;).alias(&#39;geom_id&#39;)) .join(admins, on=&#39;geom_id&#39;) .drop(&#39;geom_id&#39;)) usrs = (user_geom .join(active_users, on=&#39;user_id&#39;, how=&#39;inner&#39;) .toPandas()) usrs[&quot;country&quot;] = country usr_df = pd.concat([usr_df, usrs]) w = Window.partitionBy(&#39;user_id&#39;, &#39;date_trunc&#39;) h_stops = (stops .where(col(&#39;location_type&#39;) == &#39;H&#39;) .join(active_users, on=&#39;user_id&#39;, how=&#39;inner&#39;) .groupby(&#39;user_id&#39;, &#39;date_trunc&#39;, &#39;geom_id&#39;) .agg(F.sum(&#39;duration&#39;).alias(&#39;duration&#39;)) .withColumn(&#39;max_duration&#39;, F.max(&#39;duration&#39;).over(w)) .where(col(&#39;duration&#39;) == col(&#39;max_duration&#39;)) .groupby(&#39;user_id&#39;, &#39;date_trunc&#39;) .agg(F.first(&#39;geom_id&#39;).alias(&#39;geom_id&#39;)) .join(admin_rural, on=&#39;geom_id&#39;, how=&#39;inner&#39;) .join(user_geom, on=&#39;user_id&#39;, how=&#39;inner&#39;)) Now that we have defined the home locations of each user we compare the location of the homes from one day to the one a day before. We are interested in migrations to rural areas, so this time we’re not taking into account if they moved to a different urban area. Then we put a flag on each user every day to tell if they moved or not and if they did, knowing if they moved from an urban area to a rural one or the other way around. # look-up previous geom id to identify migrations with direction w = Window.partitionBy(&#39;user_id&#39;).orderBy(&#39;date_trunc&#39;) h_stops = (h_stops .withColumn(&#39;prev_geom_id&#39;, F.lag(&#39;geom_id&#39;, offset=1).over(w)) .withColumn(&#39;prev_urban/rural&#39;, F.lag(&#39;urban/rural&#39;, offset=1).over(w)) .withColumn(&#39;prev_date&#39;, F.lag(&#39;date_trunc&#39;, offset=1).over(w)) .where(col(&#39;prev_geom_id&#39;).isNotNull()) .withColumn(&#39;change&#39;, F.when(col(&#39;urban/rural&#39;) == col(&#39;prev_urban/rural&#39;), &#39;no change&#39;) .otherwise(F.when(col(&#39;urban/rural&#39;) == &#39;urban&#39;, &#39;rural to urban&#39;) .otherwise(F.when(col(&#39;urban/rural&#39;) == &#39;rural&#39;, &#39;urban to rural&#39;)))) .withColumn(&#39;gap&#39;, F.datediff(col(&#39;date_trunc&#39;), col(&#39;prev_date&#39;))) .withColumn(&#39;rand_gap&#39;, (-1*F.rand()*(col(&#39;gap&#39;)-1)).astype(IntegerType())) .withColumn(&#39;new_date&#39;, F.expr(&quot;date_add(date_trunc, rand_gap)&quot;)) .withColumn(&#39;date_trunc&#39;, col(&#39;new_date&#39;))) Finally we only group the number of changes by date and wealth group. # aggregate by day and change and return as pandas df out = (h_stops .groupby(&#39;date_trunc&#39;, &#39;wealth_label&#39;, &#39;change&#39;) .agg(F.countDistinct(&#39;user_id&#39;).alias(&#39;n_users&#39;)) .withColumnRenamed(&#39;date_trunc&#39;, &#39;date&#39;) .toPandas()) out[&#39;country&#39;] = country Finally, we compute the cumulative percentage change of the difference between the individuals going to rural areas and the ones to urban areas. This way we get the net rural migration percentage. Figure 2.5: Results for six different cities São Paulo, Bogota, Jakarta, Mexico City, Quezon City (Manila), and Johannesburg. Measuring: Net rural migration in the fraction of people who lived in the city at any point in time. The three coloured lines distinguishes the metric changes for the three different socio-economic groups. 2.7 Location labeling and parameter optimization One of the crucial steps allowing us to perform the analyses presented in this book is the ability to infer individual home locations and workplaces. Additional information, based on the individual pattern of visits to recurrent locations, are added to each stop location to distinguish their roles and importance. This process has been extensively presented in Chapter 2.4. Here, we discuss how we converged on a specific parameter configuration to be shared by all the different countries analysed in the book. 2.7.1 Ground truth 2.7.1.1 Construction The first element needed to optimize the process is a ground truth to which each labeling output can be compared with. To this extent, we selected a sample of 100 active individuals for each country. Active individuals were randomly selected from six activity-based buckets: High, medium, and low activity during the pre-pandemic period; high, medium, and low activity after the pre-pandemic period (buckets combined the combination of the different class, e.g. high-medium or low-low activity buckets). Activity classification was performed splitting in percentile groups with similar size. The uniform selection of individuals acroos different activity buckets provided a balanced set of data ensuring algorithmic stability across a differentiated spectra of mobile phone usage. We stress that, even if activity groups were constructed using groups of individuals their stop locations were not aggregated in the same group, but each individual locations were separately and independently subjected to the same validation procedure. 2.7.1.2 Validation After the sampling step, individuals’ stop locations are submitted to a validator whose task is to label locations into three different categories: home location, work location, and others. Validators were allowed to label multiple home locations per individual as well as multiple work locations. Validators were required to use all the information at their disposal (both the maps and the distribution of time spent at locations over time-of-the-day, day-of-the-week, and the day-of-the-year). The stop location of same individuals were submitted to two independent validators simultaneously. The results from the first round of manual labeling were, then, submitted to a third validator responsible of solving conflicts between the first two validators. For each step all the validators were provided with the same information presented in the same way through the dashboard presented in figure. Figure 2.6: Screenshot of the dashboard used by the validators to manually assign labels to locations. Validators have two maps sourced from different providers (OpenStreetMap at the top, and GoogleMaps at the bottom) to explore. Both maps provide different point of interest information in support to the decisionmaking. The bottom map also allows the validator to switch to a satellate imagery mode. On the right side of the dashboard, validators are provided with summary information for all the locations visited by the individual. In particular, the average distribution of time-at-location during the daytime is provided at the top. Similarly, the middle panel provide interactive distribution for the weekly patters (distribution of time spent on a given weekday at a specific location), while the bottom right panel provide panel data spanning over the entire 2020 period. Data presented in this figure are synthetic in compliance to the individual privacy preserving guidelines from the data provider. 2.7.2 Parameter optimization The result of the validation procedure are stored as .csv files for each country containing the label by the first independent validator in the “FirstOpinion” column, the label of the second independent validator in the “SecondOpinion” column, and the final label assigned by the conflict-breaker validator in the “FinalOpinion” column. The final labels were compared with the results of the labeling running the labeling step of the pipeline described in 2.4 joining the two over the individual unique identifier. The labeling results produced by the algorithm were restructured using the following function, which returns a dataframe of containing the individual id, stop location unique (for each individual) identifier, and the label returned by the algorithm. def get_algorithm_labels(**args): &quot;&quot;&quot; To load computed labels the following are required: country: country of interest home_period_window: size of the window for home work_period_window: size of the window for work min_periods_over_window: minimum days/win_size on which candidate should appear to be labelled as H min_periods_over_window_work: minimum days/win_size on which candidate should appear to be labelled as W work_activity_average: average time spent at work in a day over the window time_fraction_work: fraction of user time in records spent at work fpath: path to the label results (country dependent) OUTPUT: dataframe of user_id,cluster_label,AlgorithmOpinion &quot;&quot;&quot; country=args[&quot;country&quot;] home_period_window=args[&quot;home_period_window&quot;] work_period_window=args[&quot;work_period_window&quot;] min_periods_over_window=args[&quot;min_periods_over_window&quot;] min_periods_over_window_work=args[&quot;min_periods_over_window_work&quot;] work_activity_average=args[&quot;work_activity_average&quot;] time_fraction_work=args[&quot;time_fraction_work&quot;] fname = f&#39;hpw{home_period_window}_wpw{work_period_window}_mpow{min_periods_over_window}_mpoww{min_periods_over_window_work}_waa{work_activity_average}_tfw{time_fraction_work}_v10.csv&#39; df_hw = pd.read_csv(os.path.join(fpath, fname),usecols=[&quot;user_id&quot;, &quot;location_type&quot;, &quot;cluster_label&quot;]) df_hw = df_hw[df_hw.cluster_label.notna()] df_hw[&#39;location_type&#39;] = df_hw.location_type.fillna(&#39;O&#39;) df_hw = df_hw.drop_duplicates().reset_index(drop=True) df_hw = df_hw[df_hw.location_type!=&#39;O&#39;].rename(columns={&#39;location_type&#39;:&#39;AlgorithmOpinion&#39;}) return df_hw The arguments required by this functions are those over which the parameter optimization is performed. In particular, we performed a grid-search optimization for each different combination of the following parameter configurations: iter_params = { &quot;home_period_window&quot;:np.arange(14,84,7), &quot;work_period_window&quot;:np.arange(14,84,7), &quot;min_periods_over_window&quot;:np.around(np.linspace(0.1, 0.4, 5), decimals=1), &quot;min_periods_over_window_work&quot;:np.around(np.linspace(0.1, 0.4, 5), decimals=1), &quot;work_activity_average&quot;:[1800*hours for hours in range(0,5)], &quot;time_fraction_work&quot;:[0.0, 0.2, 0.4, 0.6] } 2.7.2.1 Performance indicators In the code presented below we provide multiple performance indicators. Each one of those can be used to assess different aspects. For example, the intra-validators agreement can be compared with the average agreement between one of the two “first-step” validators and the algorithm (we do this using the Choen’s kappa statistic) to get a direct comparison between validators and algorithm accord. Focusing on direct performance metrics, we instead use the F1-score computed between the final validator’s opinion and the algorithm result looking for all locations correctly labelled as either “home-location” or “work-location.” The F1-score is computed for each one of the two labels independently and then aggregated averaging the two results. # To speed up computations remove all locations labelled as &quot;O&quot; (other) from the data. def filter_o(df,col1,col2): return df[(df[col1]!=&#39;O&#39;)|(df[col2]!=&#39;O&#39;)] def compute_sample_metrics(df_sample,col_true,col_eval1): a1 = &#39;FirstOpinion&#39; a2 = &#39;SecondOpinion&#39; dfte = filter_o(df_sample,col_true,col_eval1) df1e = filter_o(df_sample,a1,col_eval1) df2e = filter_o(df_sample,a2,col_eval1) df12 = filter_o(df_sample,a1,a2) kappa = ((skm.cohen_kappa_score(df1e[a1],df1e[col_eval1], labels=[&#39;H&#39;,&#39;W&#39;])+ skm.cohen_kappa_score(df2e[a2],df2e[col_eval1], labels=[&#39;H&#39;,&#39;W&#39;]))/2) accuracy = skm.accuracy_score(dfte[col_true],dfte[col_eval1]) f1 = skm.f1_score(dfte[col_true],dfte[col_eval1],labels=[&#39;H&#39;,&#39;W&#39;],average=&#39;macro&#39;) f1H = skm.f1_score(dfte[col_true],dfte[col_eval1],labels=[&#39;H&#39;],average=&#39;macro&#39;) f1W = skm.f1_score(dfte[col_true],dfte[col_eval1],labels=[&#39;W&#39;],average=&#39;macro&#39;) # these are metrics computed one annotator against the other. kappa_ann = skm.cohen_kappa_score(df12[a1],df12[a2], labels=[&#39;H&#39;,&#39;W&#39;]) accuracy_ann = skm.accuracy_score(df12[a1],df12[a2]) f1_ann = skm.f1_score(df12[a1],df12[a2],labels=[&#39;H&#39;,&#39;W&#39;],average=&#39;macro&#39;) f1_annH = skm.f1_score(df12[a1],df12[a2],labels=[&#39;H&#39;],average=&#39;macro&#39;) f1_annW = skm.f1_score(df12[a1],df12[a2],labels=[&#39;W&#39;],average=&#39;macro&#39;) return {&#39;accuracy&#39;:accuracy,&#39;f1&#39;:f1,&#39;kappa&#39;:kappa, &#39;accuracy_ann&#39;:accuracy_ann,&#39;f1_ann&#39;:f1_ann,&#39;kappa_ann&#39;:kappa_ann, &#39;f1_annH&#39;:f1_annH,&#39;f1_annW&#39;:f1_annW,&#39;f1_H&#39;:f1H,&#39;f1_W&#39;:f1W} 2.7.2.2 Performance error estimates and bootstrapping As an estimate for the performance variability induced by the selected sample of individuals we use a standard bootstrapping procedure, resampling with replacement for 100 different times the individuals from those present in the validation set. Resampling is performed in such a way that guarantees the balance across activity-buckets. def sample_shuffled_fraction_by_group(df_group,fraction=1,replace=True): size = int(df_group.user_id.unique().size*fraction) sampled_users = np.random.choice(df_group.user_id.unique(),replace=replace,size = size) df_res = df_group.set_index(&#39;user_id&#39;) return df_res.loc[sampled_users].reset_index() def balanced_bootstrapping(df_man, num_sample=100, metrics=True, col_true=&#39;FinalOpinion&#39;, col_eval1=&#39;FirstOpinion&#39;): if not metrics: df_res = pd.DataFrame(columns=df_man.columns+[&#39;sample&#39;]) else: df_res = pd.DataFrame(columns=[&#39;sample&#39;,&#39;accuracy&#39;,&#39;f1&#39;,&#39;kappa&#39;]) for i_sample in range(num_sample): df_samp_res = df_man.groupby(&#39;act_buck&#39;,as_index=False).apply(sample_shuffled_fraction_by_group).reset_index(drop=True) df_samp_res[&#39;sample&#39;] = i_sample+1 if not metrics: df_res = df_res.append(df_samp_res) else: df_res = df_res.append({&#39;sample&#39;:i_sample+1,**compute_sample_metrics(df_samp_res,col_true,col_eval1)},ignore_index=True) return df_res For each parameter configuration bootstrap is performed and results are structured and stored in the form of a dictionary. def get_multi_config_metrics(**args): # Initialize variables from passed &quot;args&quot; dict country=args[&quot;country&quot;] manual_labels=args[&quot;manual_labels&quot;] act_buckets = args[&#39;activity_bucket&#39;] df_hw = get_algorithm_labels(**args) df_hw[&#39;act_buck&#39;] = df_hw.user_id.map(act_buckets) col_true = args[&#39;col_true&#39;] col_eval1 = args[&#39;col_eval1&#39;] col_eval2 = args[&#39;col_eval2&#39;] numer_of_samples = args[&#39;numer_of_samples&#39;] df_compare = (pd.merge(manual_labels, df_hw, how=&quot;outer&quot;, on=[&quot;user_id&quot;, &quot;cluster_label&quot;,&quot;act_buck&quot;]) .sort_values(by=[&quot;user_id&quot;,&quot;cluster_label&quot;,&quot;FinalOpinion&quot;,&quot;AlgorithmOpinion&quot;]) .drop_duplicates(subset=[&quot;user_id&quot;,&quot;cluster_label&quot;]) .reset_index(drop=True) .fillna({&#39;Country&#39;:country,&#39;FirstOpinion&#39;:&#39;O&#39;,&#39;SecondOpinion&#39;:&#39;O&#39;,&#39;AlgorithmOpinion&#39;:&#39;O&#39;,&#39;FinalOpinion&#39;:&#39;O&#39;})) df_compare = df_compare[(df_compare.FinalOpinion!=&#39;O&#39;)|(df_compare.AlgorithmOpinion!=&#39;O&#39;)] # Compute performance metrics accuracy = skm.accuracy_score(df_compare[col_true], df_compare[col_eval1]) kappa = (skm.cohen_kappa_score(df_compare[&#39;FirstOpinion&#39;], df_compare[col_eval1])+skm.cohen_kappa_score(df_compare[&#39;SecondOpinion&#39;], df_compare[col_eval1]))/2 recall = skm.recall_score(df_compare[col_true], df_compare[col_eval1],labels=[&#39;H&#39;,&#39;W&#39;], average=&quot;macro&quot;) precision = skm.precision_score(df_compare[col_true], df_compare[col_eval1],labels=[&#39;H&#39;,&#39;W&#39;], average=&quot;macro&quot;) f1 = skm.f1_score(df_compare[col_true], df_compare[col_eval1],labels=[&#39;H&#39;,&#39;W&#39;], average=&quot;macro&quot;) cf = skm.confusion_matrix(df_compare[col_true], df_compare[col_eval1]) # Estimate performance errors via Bootstrapping res = balanced_bootstrapping(df_compare,num_sample=numer_of_samples,col_true=col_true,col_eval1=col_eval1) m = res.mean().iloc[1:] m = m.sort_index() m.index=[&#39;fold_acc&#39;,&#39;fold_annotators_acc&#39;,&#39;fold_f1&#39;,&#39;fold_f1H&#39;,&#39;fold_f1W&#39;,&#39;fold_annotators_f1&#39;,&#39;fold_annotators_f1H&#39;,&#39;fold_annotators_f1W&#39;,&#39;fold_kap&#39;,&#39;fold_annotators_kap&#39;] s = res.std().iloc[1:] s = s.sort_index() s.index=[&#39;fold_acc_std&#39;,&#39;fold_annotators_acc_std&#39;,&#39;fold_f1_std&#39;,&#39;fold_f1H_std&#39;,&#39;fold_f1W_std&#39;,&#39;fold_annotators_f1_std&#39;,&#39;fold_annotators_f1H_std&#39;,&#39;fold_annotators_f1W_std&#39;,&#39;fold_kap_std&#39;,&#39;fold_annotators_kap_std&#39;] # Collect and structure results into a dictionary the_dict = {&quot;country&quot;:country, &quot;home_period_window&quot;:int(args[&#39;home_period_window&#39;]), &quot;work_period_window&quot;:int(args[&#39;work_period_window&#39;]), &quot;min_periods_over_window&quot;:float(args[&#39;min_periods_over_window&#39;]), &quot;min_periods_over_window_work&quot;:float(args[&#39;min_periods_over_window_work&#39;]), &quot;work_activity_average&quot;:float(args[&#39;work_activity_average&#39;]), &quot;time_fraction_work&quot;:float(args[&#39;time_fraction_work&#39;]), &quot;f1&quot;:f1,&quot;accuracy&quot;:accuracy,&quot;kappa&quot;:kappa, &quot;cf&quot;:cf,**m,**s} return the_dict 2.7.2.3 Parallel bootstrapping and optimization result storage The enormous number of combination of parameter configuration we have to explore can be speeded up by multiprocess computation of the different configurations. Here, we present a possible solution which uses the [python “joblib” library]{https://joblib.readthedocs.io/en/latest/}. Parameter configurations are stored as a list of dictionaries which is then passed to the “Parallel” function. from joblib import Parallel,delayed results_save_path = &quot;SAVING_PATH_FOR_CONFIGURATION_PERFORMANCE_EVALUTATION&quot; numer_of_samples = 100 c_dates = {&#39;BR&#39;: &#39;2021-01-01&#39;, &#39;CO&#39;: &#39;2021-01-01&#39;, &#39;ID&#39;: &#39;2021-01-01&#39;, &#39;ZA&#39;: &#39;2021-01-01&#39;, &#39;MX&#39;: &#39;2020-12-01&#39;, &#39;PH&#39;: &#39;2021-01-01&#39;} countries = [&#39;CO&#39;,&#39;PH&#39;,&#39;MX&#39;,&#39;BR&#39;,&#39;ID&#39;] result_list = [] for country in countries: print(country) df_man = get_labels(country) activity_bucket = df_man.drop_duplicates([&#39;user_id&#39;,&#39;act_buck&#39;]).set_index(&#39;user_id&#39;).act_buck.dropna().to_dict() init_params = {&quot;country&quot;:[country],&quot;c_dates&quot;:[c_dates]} parameters = {**init_params, **iter_params} additional_params = {&quot;numer_of_samples&quot;:numer_of_samples,&quot;activity_bucket&quot;:activity_bucket,&quot;manual_labels&quot;:df_man, &#39;col_true&#39;:&#39;FinalOpinion&#39;,&#39;col_eval1&#39;:&#39;AlgorithmOpinion&#39;,&#39;col_eval2&#39;:None} results_df = pd.DataFrame(columns=[&quot;country&quot;, &quot;home_period_window&quot;, &quot;work_period_window&quot;, &quot;min_periods_over_window&quot;, &quot;min_periods_over_window_work&quot;, &quot;work_activity_average&quot;, &quot;time_fraction_work&quot;, &quot;accuracy&quot;, &quot;recall&quot;, &quot;precision&quot;, &quot;f1&quot;, &quot;fold_f1&quot;,&quot;fold_acc&quot;,&quot;fold_kap&quot;,&quot;fold_f1_sem&quot;,&quot;fold_acc_sem&quot;,&quot;fold_kap_sem&quot;, &quot;cf&quot;]) keys = list(parameters) keyvals = [] for values in itertools.product(*map(parameters.get, keys)): keyval = dict(zip(keys, values)) keyval = {k:str(v) for k, v in keyval.items()} keyval = {**keyval,**additional_params} keyvals.append(keyval) result_list += Parallel(n_jobs=32)(delayed(get_multi_config_metrics)(**keyval) for j,keyval in enumerate(keyvals[:])) results_df = pd.DataFrame.from_records(result_list) results_df[&#39;samples&#39;] = numer_of_samples results_df.to_csv(results_save_path+f&quot;final_statistics_with_{numer_of_samples}_folding_v10.csv&quot;, index=False) 2.7.3 Parameter configuration selection While for each country a different optimal configuration can be found, leveraging over the performance error estimates, it is possible to check for common configuartions across different countries which are compatible (i.e. the difference between the optimal configuration F1 minus its standard deviation is smaller than the shared configuration plus its standard deviation) with the country specific best configuration. Best configuration is selected among those compatible for all states by means of the harmonic average of each country F1 score. def get_country_compatibilities(res_country): &#39;&#39;&#39; INPUT: pandas dataframe resulting from bootstrapped optimization OUTPUT: pandas dataframe filtered to keep only configurations which are compatible with the best one &#39;&#39;&#39; arg_max = res_country.loc[res_country.fold_f1.idxmax()] results_df_fold_f1_max = arg_max.fold_f1 results_df_fold_f1_mstd = arg_max.fold_f1_std return res_country[res_country.fold_f1+res_country.fold_f1_std&gt;=results_df_fold_f1_max-results_df_fold_f1_mstd] We report here, an example to get (if any) a common best parameter configuration for five different countries, namely, Mexico, Brazil, Indonesia, Philippines, and Colombia. countries = [&#39;MX&#39;,&#39;BR&#39;,&#39;ID&#39;,&#39;PH&#39;,&#39;CO&#39;] numer_of_samples = 100 results_df = pd.read_csv(results_save_path+f&quot;final_statistics_with_{numer_of_samples}_folding_v10.csv&quot;) res_opt = results_df.groupby(&#39;country&#39;,as_index=False).apply(get_country_compatibilities) res_opt = res_opt.set_index([&#39;country&#39;,&quot;home_period_window&quot;,&quot;work_period_window&quot;, &quot;min_periods_over_window&quot;,&quot;min_periods_over_window_work&quot;, &quot;work_activity_average&quot;,&quot;time_fraction_work&quot;]) ### Find the best common optimum (optimization function is the max of the average f1 score) res_tmp = results_df.drop_duplicates([&quot;country&quot;,&quot;home_period_window&quot;, &quot;work_period_window&quot;, &quot;min_periods_over_window&quot;, &quot;min_periods_over_window_work&quot;, &quot;work_activity_average&quot;,&quot;time_fraction_work&quot;]) res = res_tmp.groupby([&quot;home_period_window&quot;, &quot;work_period_window&quot;, &quot;min_periods_over_window&quot;, &quot;min_periods_over_window_work&quot;, &quot;work_activity_average&quot;,&quot;time_fraction_work&quot;]).mean() res[&#39;fold_f1_harmonic&#39;] = res_tmp.groupby([&quot;home_period_window&quot;, &quot;work_period_window&quot;, &quot;min_periods_over_window&quot;, &quot;min_periods_over_window_work&quot;, &quot;work_activity_average&quot;,&quot;time_fraction_work&quot;]).fold_f1.apply(hmean) res = res.sort_values(&#39;fold_f1_harmonic&#39;,ascending=False) res.reset_index(inplace=True) i=0 while True: test_index = res.iloc[i][[&quot;home_period_window&quot;, &quot;work_period_window&quot;, &quot;min_periods_over_window&quot;, &quot;min_periods_over_window_work&quot;, &quot;work_activity_average&quot;,&quot;time_fraction_work&quot;]].tolist() testing=[] j=0 for country in countries: test = tuple([country]+test_index) if test in res_opt.index.tolist(): j+=1 if j == len(countries): print(f&quot;Best common config found: {i}: {test_index}&quot;); break else: i+=1 if i&gt;10000: print(&#39;Maximum iterations Exceeded. No common optimum found, increase/modify acceptance region!&#39;); break In our case, the best common configiguration found is: - home_period_window: 49.0, - work_period_window: 49.0, - min_periods_over_window: 0.2, - min_periods_over_window_work: 0.2, - work_activity_average: 3600.0, - time_fraction_work: 0.0 These values are the one used to label the locations in the analysis presented in the book. (PART*) Appendix 2.8 Wealth Index In order to explain how the Wealth Index was built we’re going to exemplify the methodology used for Mexico. For the rest of the countries it was an analogous procedure, but the variables names are different. 2.8.1 Variables to estimate the Social Gap Index in Mexico To estimate the “Social Gap Index” for Mexico at localidad level, we followed the methodology of Coneval (National Council of Social Policy Evaluation) in Mexico. This methodology creates a unique indicator of wealth by reducing the dimensions of a vector of characteristics related to education, health, basic services and quality of home infrastructure. To do this, we used principal components using the following variables: Education Percentage of illiterate population aged 15 or more Percentage of population aged 6-14 that does not attend to school Percentage of population aged 15 or more years without complete elementary school Health services access Percentage of population with no access to health services Household characteristics Log of average occupants per room Percentage of with dirt floor Percentage of households with no toilet Percentage of households with no piped water Percentage of households with no sewer system Percentage of households with no electricity Percentage of households with no washing machine Percentage of inhabited private households with no fridge Then, we estimated the variables of interest as follows: 2.8.2 Education Percentage of illiterate population aged 15 or more p15ym_an: Population 15-130 years old who cannot read or write p_15ymas: Total population older than 15 illiterate = p15ym_an/p_15ymas x 100 Percentage of population aged 6-14 that does not attend school p6a11_noa: Population aged 6-11 that do not attend to school p12a14noa: Population aged 12-14 that do not attend to school p_6a11: Population aged 6-11 p_12a14: Population aged 12-14 not_attend_school = (p6a11_noa + p12a14noa)/ (p_6a11 + p_12a14) x 100 Percentage of population aged &gt; 15 years old without full primary education p15pri_in: Population 15-130 years old who completed 5th grade as maximum. It includes people who did not specify the grade passed in the this education level p15pri_co: Population 15-130 years old who completed 6th grade as maximum. It includes people who did not specify the grade passed in the this education level p15ym_se: Population 15-130 who do not passed any grade of schooling or that only have pre-school p15sec_in: Population 15-130 years old who did not complete secondary school p_15ymas: Total population older than 15 no_primary_educ = p15pri_in+ p15pri_co + p15ym_se + p15sec_in / p_15ymas x 100 2.8.2.1 Health Services Access Percentage of population with no access to health services p_sinder: Total population who are not entitled to receive medical services in any public or private institution pobtot: Total population no_access_health = psinder/pobtot x 100 2.8.3 Household Characteristics Percentage of Households with dirt floor vph_pisodt: Households with dirt floor. It includes households for which the characteristics of the house were captured, classified as independent house, apartment in building, housing or room in neighborhood and housing or rooftop room and to which they did not specify housing class vivpar_hab: Total inhabited homes. dirt_floor = vph_pisodt/ vivpar_hab x 100 Average occupants per room Result of dividing the number of people residing in inhabited homes by the number of rooms in those homes. LOG(pro_ocup_c) It includes households for which the characteristics of the house were captured, classified as an independent house, apartment in building, housing or room in neighborhood and housing or rooftop room and to which they did not specify housing class. Percentage of homes with no toilet vph_excsa: Total number of homes with toilet. vivpar_hab: Total inhabited homes. no_toilet = 1 - (vph_excsa)/(vivpar_hab) x 100 Percentage of homes with no piped water vph_aguafv: Households that have water availability from a public or hydrant tap, anotherdwelling, a pipe, a well, a river, a stream, a lake or another. It includes households for which the characteristics of the house were captured, classified as an independent house, apartment in building, housing or room in neighborhood and housing or rooftop room and to which they did not specify housing class. vivpar_hab: Total inhabited homes no_water = (vph_aguafv) /(vivpar_hab) x 100 Percentage of homes with no sewer system vph_nodren: Households that have no drainage connected to the public network, septic tank, canyon, crack, river, lake or sea. It includes households for which the characteristics of the house were captured, classified as independent house, apartment in building, housing or room in neighborhood and housing or rooftop room and to which they did not specify housing class vivpar_hab: Total inhabited homes no_sewing = (vph_nodren) /(vivpar_hab) x 100 Percentage of homes with no electricity vph_s_elec: Inhabited private homes that do not have electricity. It includes households for which the characteristics of the house were captured, classified as an independent house, apartment in building, housing or room in neighborhood and housing or rooftop room and to which they did not specify housing class. vivpar_hab: Total inhabited homes no_electricity = (vph_s_elec) /(vivpar_hab) x 100 2.8.4 Assets Ownership Percentage of Households with no washing machine vph_lavad: Households that have washing machine. It includes households for which the characteristics of the house were captured, classified as an independent house, apartment in building, housing or room in neighborhood and housing or rooftop room and to which they did not specify housing class. vivpar_hab: Households of any kind: independent house, apartment in building, housing or room in neighborhood, housing or rooftop room, premises not built for room, mobile home, shelters or unspecified class. Includes households without occupant information. no_washingmachine = 1 - (vph_lavad) / (vivpar_hab) Percentage of Households with no fridge vph_refri: Households that have a refrigerator. It includes households for which the characteristics of the house were captured, classified as an independent house, apartment in building, housing or room in neighborhood and housing or rooftop room and to which they did not specify housing class. vivpar_hab: Private dwellings inhabited of any kind: independent house, apartment in building, housing or room in neighborhood, housing or rooftop room, premises not built for room, mobile home, shelters or unspecified class. Includes households without occupant information. no_fridge = 1 - (vph_refri) / vivpar_hab "],["twitter.html", "Chapter 3 Labor Market 3.1 Labor market analysis with Twitter data 3.2 Training data preparation 3.3 Finetuning BERT-based models 3.4 Unemployment indicators", " Chapter 3 Labor Market 3.1 Labor market analysis with Twitter data Online social networks, such as Twitter, play a key role in the diffusion of information on jobs. For instance, companies and job aggregators post job offers while users disclose their labor market situation seeking emotional support or new job opportunities through their online network. In this context, Twitter data can be seen as a complementary data source to official statistics as it provides timely information about labor market trends. In this project, we leverage state-of-the-art language models (Devlin et al, 2018) to accurately identify disclosures on personal labor market situations as well as job offers. In practice, we intend to recognize disclosures about being unemployed, losing one’s job or being hired, searching for a job as well as job offers. We focus on three countries for this study: the USA, Mexico and Brazil. The methodology is presented in this IC2S2 2020 presentation and detailed in Tonneau et al. (2021, in submission). Aggregating this individual information at the city and country levels, we then built Twitter-based labor market indexes and used them to better predict future labor market trends. In this book, we present the following: - building a training set allowing to recognize disclosures on personal job situation in tweets - finetuning and evaluating a BERT-based classifier - improving a classifier’s performance through active learning - building unemployment indicators using the classifier’s output 3.2 Training data preparation 3.2.1 Sampling The first step in order to train classifiers to detect disclosures of labor market situation is to sample informative tweets and have them labelled. Owing to the very low share of tweets containing these disclosures, a random sampling would yield very few positive examples which would not allow to train a good-performing classifier. We instead decided to opt for stratified sampling, namely by defining a list of n-grams, both specific to the labor market context and frequent enough, and sampling tweets containing these n-grams. In practice, we wrote code in PySpark as we had to handle big amounts of data. Here’s a snippet of the code we used: after loading the data in the dataframe df and having text_lowercase as the column where the lowercased text is stored, we define the list of ngrams to sample from and then sample from it. ngram_list = [[&#39; i &#39;, &#39;fired &#39;], [&#39;fired me&#39;], [&#39;laid off&#39;], [&#39;lost my job&#39;]] for ngram in ngram_list: if len(ngram) == 1: df_ngram = df.filter(df.text_lowercase.contains(ngram[0])) elif len(ngram) == 2: regex = f&quot;{ngram[0]}[.\\w\\s\\d]*{ngram[1]}&quot; df_ngram = df.filter(df.text_lowercase.rlike(regex)) share = min(float(150 / df_ngram.count()), 1.0) df_ngram_sample = df_ngram.sample(False, share, seed=0) We sample 150 tweets per n-gram for each class and language. In total, we end up with approximately 5000 samples for each language. A detailed list of n-grams for each language and class can be found in Tonneau et al. (2021). 3.2.2 Labelling After sampling informative tweets, we have them labelled by crowdworkers. We use the crowdsourcing platform Amazon Mechanical Turk. This platform has the advantage of having an international workforce speaking several languages, including Spanish and Brazilian Portuguese on top of English. 3.2.2.1 Creating a Qualtrics survey The first step was to create a survey we would then send to crowdworkers. For this, we use Qualtrics which implies having a Qualtrics API token and a datacenter ID. We build beforehand a survey template one can create manually on Qualtrics that we can then load questions from for the different labelling sessions. With this information, we are able to create a new blank survey for which we can define the name SurveyName and the language: def create_survey(SurveyName, apiToken, dataCenter, language): baseUrl = &quot;https://{0}.qualtrics.com/API/v3/survey-definitions&quot;.format( dataCenter) headers = { &quot;x-api-token&quot;: apiToken, &quot;content-type&quot;: &quot;application/json&quot;, &quot;Accept&quot;: &quot;application/json&quot; } data = { &quot;SurveyName&quot;: SurveyName, &quot;Language&quot;: language, &quot;ProjectCategory&quot;: &quot;CORE&quot; } response = requests.post(baseUrl, json=data, headers=headers) if json.loads(response.text)[&quot;meta&quot;][&quot;httpStatus&quot;] != &#39;200 - OK&#39;: print(json.loads(response.text)[&quot;meta&quot;][&quot;httpStatus&quot;]) SurveyID = json.loads(response.text)[&#39;result&#39;][&#39;SurveyID&#39;] DefaultBlockID = json.loads(response.text)[&#39;result&#39;][&#39;DefaultBlockID&#39;] return SurveyID, DefaultBlockID We can then retrieve questions from our template, by specifying the QuestionID from the question we want to retrieve and the SurveyID from the template. def get_question(QuestionID, SurveyID, apiToken, dataCenter): baseUrl = &quot;https://{0}.qualtrics.com/API/v3/survey-definitions/{1}/questions/{2}&quot;.format( dataCenter, SurveyID, QuestionID) headers = { &quot;x-api-token&quot;: apiToken, } response = requests.get(baseUrl, headers=headers) if json.loads(response.text)[&quot;meta&quot;][&quot;httpStatus&quot;] != &#39;200 - OK&#39;: print(json.loads(response.text)[&quot;meta&quot;][&quot;httpStatus&quot;]) return json.loads(response.text)[&quot;result&quot;] The survey template contains questions that don’t need to be modified, such as asking for a participant’s MTurk ID. For the labelling question though, we need to update the question’s text with the tweet to be labelled. We do this the following way, with tweet being the tweet in string format: def update_question(QuestionData, QuestionID, SurveyID, apiToken, dataCenter): baseUrl = &quot;https://{0}.qualtrics.com/API/v3/survey-definitions/{1}/questions/{2}&quot;.format( dataCenter, SurveyID, QuestionID) headers = { &#39;accept&#39;: &quot;application/json&quot;, &#39;content-type&#39;: &quot;application/json&quot;, &quot;x-api-token&quot;: apiToken, } response = requests.put(baseUrl, json=QuestionData, headers=headers) if json.loads(response.text)[&quot;meta&quot;][&quot;httpStatus&quot;] != &#39;200 - OK&#39;: print(json.loads(response.text)[&quot;meta&quot;][&quot;httpStatus&quot;]) QuestionData = get_question(QuestionID=QuestionID, SurveyID=SurveyID, apiToken=apiToken, dataCenter=dataCenter) QuestionData[&#39;QuestionText&#39;] = tweet update_question(QuestionData=QuestionData, QuestionID=QuestionID, SurveyID=SurveyID, apiToken=apiToken, dataCenter=dataCenter) The entire code we used to create a Qualtrics survey using Python and the Qualtrics API is available at src/1-training_data_preparation/qualtrics/get_training_set_to_qualtrics_API_classification.py. After loading the questions and embedding the data into them, the survey is ready to be sent to crowdworkers. 3.2.2.2 Sharing the survey on MTurk To share the Qualtrics survey on MTurk, we use the Python package boto3. After collecting our access and secret access keys (respectively access_key_id and secret_access_key), we initiate the client: mturk = boto3.client(&#39;mturk&#39;, aws_access_key_id=access_key_id, aws_secret_access_key=secret_access_key, region_name=&#39;us-east-1&#39;, endpoint_url=&#39;https://mturk-requester.us-east-1.amazonaws.com&#39; ) To create an MTurk task, usually called a HIT, we need to provide an XML file containing all of the HIT’s information and content. We therefore prepare dictionaries containing specific content, such as HIT title or description, for each language. For instance, for the HIT title, we create this dictionary with ntweets being the number of tweets: title_dict = { &#39;US&#39;: &#39;Read %d English Tweets and answer a few questions&#39; % (ntweets), &#39;MX&#39;: &#39;Lea %d Tweets en español mexicano y responda algunas preguntas&#39; % (ntweets), &#39;BR&#39;: &#39;Leia %d tweets em português e responda algumas perguntas&#39; % (ntweets) } After preparing all of these dictionaries, we can then feed them to the question_generator function which will load a template previously created in HTML format (template.html), adapt it to the related survey and prepare the content of the related XML file in string format: def question_generator(country_code, survey_link, instructions_dict, survey_link_text_dict, worker_input_text_dict, submit_dict): xml_wrapper_begin = &quot;&quot;&quot; &lt;HTMLQuestion xmlns=&quot;http://mechanicalturk.amazonaws.com/AWSMechanicalTurkDataSchemas/2011-11-11/HTMLQuestion.xsd&quot;&gt; &lt;HTMLContent&gt;&lt;![CDATA[ &lt;!-- YOUR HTML BEGINS --&gt; &lt;!DOCTYPE html&gt; &quot;&quot;&quot; xml_wrapper_end = &quot;&quot;&quot; &lt;!-- YOUR HTML ENDS --&gt; ]]&gt; &lt;/HTMLContent&gt; &lt;FrameHeight&gt;0&lt;/FrameHeight&gt; &lt;/HTMLQuestion&gt; &quot;&quot;&quot; with open( &quot;template.html&quot;, &quot;r&quot;) as f: content = f.read() content = content.replace(&quot;${INSTRUCTIONS}&quot;, instructions_dict[country_code]) content = content.replace(&quot;${SURVEY_LINK}&quot;, survey_link) content = content.replace(&quot;${SURVEY_LINK_TEXT}&quot;, survey_link_text_dict[country_code]) content = content.replace(&quot;${WORKER_INPUT_TEXT}&quot;, worker_input_text_dict[country_code]) content = content.replace(&quot;${SUBMIT}&quot;, submit_dict[country_code]) return xml_wrapper_begin + content + xml_wrapper_end Afterwards, when the HIT content is ready, we need to preselect the crowdworkers to make sure they live in the same country as the authors from the tweets that they will label. That way, we make sure they speak the same language but also can better understand linguistic subtleties, such as humor or specific words from the region of origin. For example, to make sure the crowdworkers working on our HIT are based in the U.S., we create this QualificationRequirements_list: QualificationRequirements_list = [ { &#39;QualificationTypeId&#39;: &#39;00000000000000000071&#39;, # Worker_Locale &#39;Comparator&#39;: &#39;EqualTo&#39;, &#39;LocaleValues&#39;: [{ &#39;Country&#39;: &#39;US&#39;}], &#39;RequiredToPreview&#39;: True, &#39;ActionsGuarded&#39;: &#39;PreviewAndAccept&#39; }] Finally, we can create the HIT. At this step, we can specify specific parameters, such as the maximum number of workers allowed to take the survey n_workers or the amount of money they get to complete the survey money_for_hit. new_hit = mturk.create_hit( MaxAssignments=n_workers, AutoApprovalDelayInSeconds=172800, LifetimeInSeconds=259200, AssignmentDurationInSeconds=10800, Reward=str(money_for_hit), Title=f&#39;{title_dict[args.country_code]} v{args.version_number}&#39;, Description=description_dict[args.country_code], Keywords=keywords_dict[args.country_code], QualificationRequirements=QualificationRequirements_list if create_hits_in_production else list(), Question=question ) The HIT is now online and crowdworkers are able to take it. You can check the progress on Qualtrics by looking at the number of replies to a given survey. When the number of replies reaches n_workers, the HIT is finished. The labelled data can then be extracted from Qualtrics. 3.3 Finetuning BERT-based models In the past few years, pretrained language models have revolutionized the field of NLP by achieving state-of-the-art results in a variety of natural language understanding tasks (Peters et al., 2018; Devlin et al., 2019). These models improve on existing word embedding methods, such as Word2Vec (Mikolov et al., 2013), by learning stable embedding representations from massive text corpora. One of the models leading this revolution is the Bidirectional Encoder Representations from Transformers model (BERT, Devlin et al., 2019), which allowed for bi-directionality, through masked language modeling, and leveraged self-attention through its Transformer-based architecture (Vaswani et al., 2017). The model was pretrained on BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2.5B words) using two unsupervised tasks, namely masked language modeling and next-sentence prediction. This model can later be fine-tuned on a variety of downstream tasks, including text classification, achieving high performance. In this part, we show how to fine-tune a BERT-based model for tweet classification. To do so, we mostly rely on the Python package simpletransformers, built on top of the famous transformers Python package developed by Hugging Face. 3.3.1 Training the model After loading the set of labelled tweets in a dataframe df, we perform a train-test split (70-30) on it: train_df = df.sample(frac=0.7,random_state=0) val_df = df.drop(train_df.index).reset_index(drop=True) We then define specific arguments for the fine-tuning such as the batch size train_batch_size, the number of epochs num_train_epochs or the output path output_dir. The models are evaluated at every epoch in terms of AUROC on the test set. The model with the highest AUROC on the test set is considered the best model and saved at best_model_dir. We also use early stopping which consists in stopping the training if the AUROC on the test set has not improved after early_stopping_patience epochs. A complete list of the training arguments can be found here. classification_args = { &#39;train_batch_size&#39;: 8, &#39;overwrite_output_dir&#39;: True, &#39;evaluate_during_training&#39;: True, &#39;save_model_every_epoch&#39;: True, &#39;save_eval_checkpoints&#39;: True, &#39;output_dir&#39;: path_to_store_model, &#39;best_model_dir&#39;: path_to_store_best_model, &#39;evaluate_during_training_verbose&#39;: True, &#39;num_train_epochs&#39;: num_train_epochs, &quot;use_early_stopping&quot;: True, &quot;early_stopping_delta&quot;: 0, &quot;early_stopping_metric&quot;: &quot;auroc&quot;, &quot;early_stopping_metric_minimize&quot;: False, &#39;early_stopping_patience&#39;: 3} Once the classification arguments are defined, we can initiate the fine-tuning. In our case, for English tweet classification, we use a version of BERT that was further pretrained on English tweets by Deep Pavlov, therefore enhancing the classification performance in the Twitter context. We need to define the model_name, which is bert in our case but can also be other more sophisticated architectures such as roberta. The model_type refers to the model name on the Hugging Face model hub. As we only cover binary classification here, the number of labels num_labels is set to 2. from simpletransformers.classification import ClassificationModel model = ClassificationModel(model_name=&#39;bert&#39;, model_type=&#39;DeepPavlov/bert-base-cased-conversational&#39;, num_labels=2, use_cuda=True, args=classification_args) Once the model has been loaded, we can launch the fine-tuning: model.train_model(train_df=train_df, eval_df=eval_df, output_dir=path_to_store_model) The training is now launched. When it is finished, we can use the best model in terms of AUROC on the test set and evaluate it on a bigger random set of tweets. 3.3.2 Evaluation on the random set While evaluating the model on the test set is useful to determine which classifier is best suited for the task, this performance might not be representative of the performance on a random set of tweets as the imbalance on the latter is much more extreme than on the test set. In this case, we needed to find a way to evaluate our classifiers in real-world settings. To do so, we used the best model from our training iterations to infer the confidences scores of each tweet out of a large random sample of 100 million tweets. We then ranked the tweets based on their confidence score in a descending way and sampled tweets along the confidence score distribution, overweighting the top of the distribution. In total, we sampled 200 tweets out of the 100 million random sample with ranks ranging from 1 to 1 million. The function used to do this sampling is as follows: def get_sampled_indices(n_sample=10, n_cutoff=6): sampled_points = [] # index of scores around which we sample n_sample tweets sampled_ranks = [] # ranks of sampled tweets for point, rank in enumerate(sorted(set([int(x) for i in range(n_cutoff) for x in np.logspace(i, i + 1, i + 1)]))): if not point: new_ranks = list(range(rank, rank + n_sample)) else: new_ranks = list(range(rank + 1, rank + n_sample + 1)) print(&#39;Index of sampled point:&#39;, point) print(&#39;Sampled ranks:&#39;, new_ranks) sampled_points.extend([point] * n_sample) sampled_ranks.extend(new_ranks) return sampled_points, sampled_ranks We then labelled these sampled tweets, which allowed us to get the percentage of positive tweets for given points in the rank distribution. Below is an example of an evaluation plot for the class job_search with the tweet rank as x-axis and the percentage of positive tweets for the class job_search as y-axis. Precision (y-axis) as a function of tweet rank based on confidence score i.e. positive label probability output by the model (x-axis). 3.3.3 Active learning While determining an initial list of n-grams to build a labelled set from is a good strategy to get a decent classification performance, it is unlikely to capture all the linguistic subtleties humans use on social media to talk about their labor market situation. In this case, we ideally need to label more tweets and need to determine which are the most informative to respect our labeling budget constraint. When faced with a classification task in which the minority class is extremely rare, active learning allows to minimize the number of labels required for a classifier to achieve good performance. At each iteration, a trained model is used to query new samples expected to lead to high improvements in validation accuracy. These samples are, in turn, labeled by humans and then used for training in the next iteration of the model. There are two main approaches to identifying the most informative samples: uncertainty sampling (Lewis and Gale, 1994) and diversity sampling (Cohn et al., 1994), which have been coined as the ``two faces of active learning’’(Dasgupta, 2011). While uncertainty sampling defines the most informative samples as the ones the model is the most uncertain about (e.g. in a binary context, this boundary is at 0.5), diversity sampling consists of selecting examples to label from different homogeneous clusters of the feature space. Here, we provide an example on how to select the most informative tweets to label using uncertainty sampling on uncalibrated BERT confidence scores. In this setting, we sample tweets with BERT confidence scores around 0.5. Having the data stored in a dataframe df containing the confidence scores in the score column, we create a modified_score which is the confidence score minus 0.5. We then select 50 tweets with the smallest positive modified_score and 50 tweets with the highest negative modified_score. df[&#39;modified_score&#39;] = df[&#39;score&#39;] - 0.5 above_threshold_df = df.loc[df[&#39;modified_score&#39;] &gt; 0].nsmallest(50, &#39;modified_score&#39;) below_threshold_df = df.loc[df[&#39;modified_score&#39;] &lt; 0].nlargest(50, &#39;modified_score&#39;) sample_df = pd.concat([above_threshold_df, below_threshold_df]).reset_index(drop=True) In the end, the sample_df are sent to labelling. When labelled, the new samples are added to the existing labels and a new train-test split is applied. The training and evaluation then takes place as described earlier. In practice, there exists many different active learning strategies. In Tonneau et al. (2021), we reviewed several active learning strategies in our extremely imbalanced setting and showed that active learning does improve performance in terms of precision, expansion and diversity but that no active learning strategy was systematically better than others for our problem. 3.4 Unemployment indicators 3.4.1 Building unemployment indicators from individual tweets After reaching good classification performance in detecting disclosures about personal labor market situations, we now need to aggregate this individual information to build timely unemployment indicators from Twitter data. The first step is to agree on a classification threshold to determine which tweets are positives and negatives. To choose this threshold, we rely on the labelled tweets sampled along the confidence score distribution, as described in the previous part. To define the cutoff, we identify the two points in the rank distribution between which the share of positives goes from over 0.5 to below 0.5. We take the average together the confidence scores at these two points and consider the result as our cutoff. Once the cutoff is defined, for a given class, we can isolate tweets considered as positives, that is with a confidence score above the cutoff score previously defined. Using this information, we can then determine the number of users with positive tweets for a given month and year, location, gender, etc… After storing the confidence scores for each tweet in the dataframe tweet_scores and for a given class class_ and cutoff: tmp = tweet_scores.filter(tweet_scores[class_] &gt;= cutoff).groupby(&#39;year&#39;, &#39;month&#39;, &#39;user_location&#39;, &#39;gender&#39;).agg( F.countDistinct(&quot;user_id&quot;).alias(class_)) More information about these unemployment indicators, including data and code, can be found in SDG-big-data/twitter-analytics/twitter-indicator. "],["news.html", "Chapter 4 News Analytics 4.1 News articles 4.2 Global vs. local news sentiment indicators 4.3 News-sentiment measures 4.4 Stylized facts", " Chapter 4 News Analytics We take news articles focusing on a specific country and compute the sentiment associated with the article. The methodology is detailed in Fraiberger et al. (2021) with an application to understanding international asset prices. 4.1 News articles Our dataset of news comes from Factiva.com. Articles are indexed using region and subject tags. Each article is annotated with topics and geographic tags generated by Factiva using a proprietary algorithm. Note that an article can be tagged with multiple locations and topics. In Fraiberger et al. (2021), we focused on English articles published by Reuters between 1991 and 2015 and tagged with either economic news or financial market news as well as with one of the 25 countries in our sample (9 AE and 16 EM). In this chapter, we provide an updated version of the news sentiment index based on Reuters articles published between 1991 and 2019. Overall, our dataset covers a wide range of economic topics (e.g. economic policy, government finance, etc.), financial topics (e.g. commodity markets, equity markets, forex, etc.), as well as corporate and political news. 4.2 Global vs. local news sentiment indicators Two types of articles constitute our corpus: local news and multi-country news. Local news is tagged with only one country and isolates local news from country-specific news articles. A typical local article is the one discussing labor market laws in Argentina reported above. A typical multi-country article is one reported below, entitled “Fears of Brazilian devaluation hit emerging markets.” The article mentions multiple countries and their interrelations: Title: Fears of Brazilian devaluation hit emerging markets Timestamp: 1998-09-11 Text: LONDON, Sept 11 (Reuters)—Emerging market currencies braced for further knocks on Friday amid fears that Brazil might give in to devaluation pressure and unleash a fresh onslaught around the globe. The rouble continued to gain ground in thin trade amid hopes of an imminent end to Russia’s political deadlock. But the Hungarian forint and Polish zloty slid on global bearishness after Thursday’s huge stock market falls in Latin America. Most Asian currencies held steady, helped by the firmer yen as the dollar sagged on President Bill Clinton’s political woes and speculation about an impending U.S. interest rate cut. The Indonesian rupiah rebounded from Thursday’s sharp fall. With the market discounting the near-certainty that Russia’s parliament would approve Yevgeny Primakov as prime minister later on Friday, attention focused mainly on whether Brazilian markets would see another hammering after Thursday’s collapse. “It’s like a tidal wave waiting offshore, and everybody’s hoping it’ll go in the other direction. If it hits Rio it’ll hit everywhere else,” said Nigel Rendell, an emerging markets strategist at Santander Investment Bank in London. A huge exodus of dollars on Thursday from Brazil’s foreign exchange markets, estimated at over $2 billion, panicked the key Sao Paulo stock market into a plunge of nearly 16 percent, its biggest one-day drop for nearly 11 years. The rout sparked similar slides across the region and fed general fears of a world economic slowdown, prompting steep market falls in Japan and Hong Kong early on Friday. Latin American currencies are little traded in London, and analysts said the market was waiting for direction from Wall Street’s opening and the start of New York currency trade. As an early indication of sentiment, the region’s most liquid unit, the Mexican peso, lost further ground from New York’s closing levels. By 1215 GMT it was 10.65 bid to the dollar, just off Thursday’s historic low of 10.685. Brazil, heavily dependent on capital inflows to support a pronounced short- term debt burden, has come under particular pressure from the flight investment capital from emerging markets. The central bank hiked its key interest rate overnight by 20 points to nearly 50 percent to try to halt the massive outflows. Analysts say it is touch and go whether Brazil will devalue the real before presidential elections on October 4, although officials have repeatedly denied devaluation is on the cards. “It does think it is likely. The only question is whether it will come before or after the election,” said David Boren, an emerging market strategist at Daiwa Europe in London. Analysts say Brazil still has enough reserves—now around $50 billion—to continue propping up the real but delaying what many see as the inevitable may leave the country financially depleted and less able to engineer an orderly devaluation in uncertain global market conditions. If Brazil devalues, it will almost certainly spark a fresh wave of pressure on emerging market currencies worldwide. Analysts said Argentina would be among the first in line, although the country had sufficient reserves in relation to its money supply to defend its currency board system. “With market focus on possible devaluations in Latam, China’s currency stance may again come under market scrutiny,” Standard Chartered Bank said on Friday in a note to clients. China has vowed not to devalue, and news on Thursday of a 23 percent rise in the country’s trade surplus in the first eight months of the year eased selling pressure on the yuan to the extent that the central bank was spotted buying dollars. Analysts said Hong Kong’s currency board would also come under more pressure if the real fell. Other potential victims included South Africa and even the stronger Central European countries such as Poland and Hungary, possibly forcing Budapest to widen its 4.5 percent wide trading band for the forint. The forint was glued with to the bottom of its target band on Friday. The zloty also swung sharply lower and was was quoted only 1.31/1.03 percent above its target basket parity at 1215 GMT, compared with Thursday’s fixing of 3.97 percent above parity. The rouble firmed to around 10.5 bid to the dollar from late Thursday levels of 12.5, buoyed partly by hopes of some political stability. But volume remained very thin, and analysts said the rally was unlikely to last as the new government looked set to print money to clear wage and pension arrears. FOREX MARKET SNAPSHOT. The following is a snapshot of emerging markets currency rates. * ASIA AFX=) * Chinese yuan CNY=) at 8.279 vs 8.2798 on Thursday * New Taiwanese dollar TWD=) 34.47 vs 34.4 * Indonesian rupiah IDR=) 11,600 vs 11,900 * Thai baht THB=TH) at 40.65 per dollar vs 40.7 * Philippine peso PHP=) 43.4 per dollar vs 43.6 * South Korean won KRW=) at 1,365 per dollar vs 1,367 * Indian rupee INR=) 42.41 per dollar vs 42.4 * EUROPE EUROPEFX= * Russian rouble RUB=) on MICEX Selt electronic trading system at 10.51/13.15 per dollar vs average rate of 12.375 on Thursday. EMTA indicative rate at 11.238. * Zloty 1.31 percent above target basket parity vs 3.97 percent at Thursday’s fixing. * Mark/Czech crown DEMCZK=) at 18.03 bid vs 17.838 * Hungarian forint DEMHUF=) unchanged from Thursday at 2.25 percent below parity against a target basket * Slovak crown DEMSKK=) fixed at 5.35 percent below target basket vs 5.80 percent on Thursday * Ukrainian hryvnia UAH=) unchanged at 3.10 per dollar * Romanian leu ROL=) at 9,045 per dollar vs 9,025 * AFRICA AFRICAFX= &amp; MIDEAST MEFX=) * Israeli shekel ILS=) 3.8508 bid on dollar from Thursday’s 3.8568 * South African rand ZAR=) 6.3 per dollar vs 6.2555 * Kenyan shilling KES=) at 59.8 per dollar vs 59.9 * LATIN AMERICA LATAMFX= * Mexican peso MXN=) at 10.65 per dollar vs 10.48 * Brazil’s real BRL=) at 1.1786 per dollar vs 1.1789 * Venezuela bolivar VEB=) unchanged at 586.9 per dollar. (C) 1998. Topics: Money/Forex Markets, Foreign Exchange News, Commodity/Financial Market News Locations: Africa, Argentina, Asia, Brazil, Central America, China, Emerging Market Countries, Eastern Asia, European Union Countries, Central/Eastern Europe, Europe, Hong Kong, Hungary, Indonesia, Japan, Latin America, Mexico, North America, Poland, Russia, South Africa, South America, Southeast Asia, Southern Africa, United Kingdom, United States, Arizona, CIS Countries, Western U.S., Western Europe The presence of multi-country news mechanically increases the co-movement between our country-specific sentiment indices, suggesting that our previous estimates confound the impact of local and multi-country news. In the following sections, we distinguish the sentiment conveyed in local news from that of multi-country news. 4.2.1 Local news sentiment indicator Local news sentiment isolates local news from country-specific news articles. Specifically, we recompute the sentiment index of each country after excluding any article mentioning any other country. Applying this filter allows us to capture the sentiment of purely local news. insheet using &quot;..\\..\\replication\\data\\features-1991-1996-missing-countries.csv&quot;, comma clear compress drop v1 tempfile _tmp save `_tmp&#39; insheet using &quot;..\\..\\replication\\data\\news.csv&quot;, comma clear compress d append using `_tmp&#39;, gen(source) * Tags of topic-specific articles cap ren *commodity* *commdty* cap ren *monetary* *money* cap ren *external* *extrnl* cap ren *political* *polit* cap ren *general* *gen* cap ren *policy* *pol* cap ren *performance* *perform* cap ren *corporate* *corp* cap ren *derivative* *deriv* cap ren *market* *mkt* cap ren *economic* *ecn* cap ren *government* *govt* cap ren *indicators* *ind* cap ren *financial* *fin* cap ren *payments* *pay* cap ren *securities* *scrty* cap ren *industrial* *indl* cap ren *forex* *fx* cap ren *pay* ** cap ren *mkts* *mkt* cap ren *bond* ** cap ren *perform* ** cap ren *commdty* *comm* cap ren *finance* *fin* * Local baseline sentiment excluding topics with no signal gen sentiment_local = positive - negative if local==1 &amp; /// !(table==1 | tables==1 | tradeextrnl==1 | ecnind==1 | debtmkt==1) gen n_articles_local = (words!=0) &amp; local==1 &amp; /// !(table==1 | tables==1 | tradeextrnl==1 | ecnind==1 | debtmkt==1) ren date str_date gen date = date(str_date, &quot;YMD&quot;, 2000), after(str_date) format date %tdnn/dd/CCYY keep country date sentiment* n_articles* collapse (mean) sentiment* (sum) n_articles*, by(country date) compress gen iso2 = &quot;&quot;, after(country) replace iso2 = &quot;AR&quot; if country==&quot;Argentina&quot; replace iso2 = &quot;BR&quot; if country==&quot;Brazil&quot; replace iso2 = &quot;CL&quot; if country==&quot;Chile&quot; replace iso2 = &quot;CN&quot; if country==&quot;China&quot; replace iso2 = &quot;FR&quot; if country==&quot;France&quot; replace iso2 = &quot;DE&quot; if country==&quot;Germany&quot; replace iso2 = &quot;GR&quot; if country==&quot;Greece&quot; replace iso2 = &quot;IN&quot; if country==&quot;India&quot; replace iso2 = &quot;ID&quot; if country==&quot;Indonesia&quot; replace iso2 = &quot;IE&quot; if country==&quot;Ireland&quot; replace iso2 = &quot;IT&quot; if country==&quot;Italy&quot; replace iso2 = &quot;JP&quot; if country==&quot;Japan&quot; replace iso2 = &quot;KR&quot; if country==&quot;Korea&quot; replace iso2 = &quot;MY&quot; if country==&quot;Malaysia&quot; replace iso2 = &quot;MX&quot; if country==&quot;Mexico&quot; replace iso2 = &quot;PE&quot; if country==&quot;Peru&quot; replace iso2 = &quot;PH&quot; if country==&quot;Philippines&quot; replace iso2 = &quot;PL&quot; if country==&quot;Poland&quot; replace iso2 = &quot;PT&quot; if country==&quot;Portugal&quot; replace iso2 = &quot;RU&quot; if country==&quot;Russia&quot; replace iso2 = &quot;ZA&quot; if country==&quot;South Africa&quot; replace iso2 = &quot;KR&quot; if country==&quot;South Korea&quot; replace iso2 = &quot;ES&quot; if country==&quot;Spain&quot; replace iso2 = &quot;TH&quot; if country==&quot;Thailand&quot; replace iso2 = &quot;TR&quot; if country==&quot;Turkey&quot; replace iso2 = &quot;US&quot; if country==&quot;United States&quot; tab iso2, m * Convert to z-scores within each country ds sentiment* foreach vv in `r(varlist)&#39; { bys iso2: egen _mean = mean(`vv&#39;) bys iso2: egen _sd = sd(`vv&#39;) replace `vv&#39; = (`vv&#39; - _mean) / _sd drop _mean _sd } compress d save &quot;..\\indicator\\sentiment_local.dta&quot;, replace 4.2.2 Global news sentiment indicator Global news sentiment index captures the tone of news published in the world every day. We extract a common factor from our country-specific sentiment series using a Kalman filter. Formally, we estimate a single (latent) factor model in the spirit of Stock and Watson (2016). \\[\\begin{align*} S_{i,t} &amp;= P_{i} F_{t} + u_{i,t} \\\\ F_{t} &amp;= A_{1} F_{t-1} + A_{2} F_{t-2} + \\dots + v_{t} \\\\ u_{i,t} &amp;= C_{1} u_{i,t-1} + C_{2} u_{i,t-2} + \\dots + e_{i,t}, \\end{align*}\\] where \\(S_{i,t}\\) refers to the news sentiment index in country \\(i\\) on day \\(t\\), \\(F_{t}\\) is the (unobserved) global news sentiment factor at time \\(t\\), and \\(P_{i}\\) is the country-specific factor loading. We use an AR(8) for the factor structure and estimate the model using Maximum Likelihood. The data coverage for news articles does not start at the same time for all countries. In order to ensure that all countries in our sample have the longest time coverage, we impute these missing observations with zeroes when we estimate the factor model to extract the global news sentiment. use &quot;..\\indicator\\sentiment_country.dta&quot;, clear keep iso2 date sentiment_country reshape wide sentiment_country, i(date) j(iso2) string tsset date tsfill keep if year(date) &gt; 1990 ds sentiment_country* loc vlist = &quot;`r(varlist)&#39;&quot; foreach vv in `vlist&#39; { di &quot;replace `vv&#39; = 0 if mi(`vv&#39;)&quot; replace `vv&#39; = 0 if mi(`vv&#39;) } compress dfactor ((`vlist&#39;) = , noconstant) (f = , ar(1/8)) ereturn list predict sentiment_global if e(sample), factor smethod(filter) * Convert to z-scores ds sentiment* foreach vv in `r(varlist)&#39; { bys iso2: egen _mean = mean(`vv&#39;) bys iso2: egen _sd = sd(`vv&#39;) replace `vv&#39; = (`vv&#39; - _mean) / _sd drop _mean _sd } compress save &quot;..\\indicator\\sentiment_global.dta&quot;, replace 4.3 News-sentiment measures 4.3.1 Bag-of-words model Sentiment is measured using a simple dictionary approach based on Loughran and Mcdonald (2011). To measure news sentiment, we use a bag-of-words model, allowing us to reduce complex and multi-dimensional text data into a single number. First, we combine existing lists of positive and negative words found in financial texts by Loughran and Mcdonald (2011) and in texts related to economic policy by Young and Soroka (2012). We then expand our lists by including the inflections of each word: for example, the word lose belongs to the negative list, hence we also include the words losing, loser, lost, loss, etc, leading to a final list of 7,217 negative words and 3,250 positive words. The following table shows the most frequent tonal words inour corpus. Most frequent positive (left) and negative (right) words. Positive word Fraction of positive words Fraction of articles IDF Negative word Fraction of negative words Fraction of articles IDF Strong 0.107 0.118 2.135 Crisis 0.088 0.069 2.675 Gains 0.099 0.104 2.265 Losses 0.072 0.069 2.677 Well 0.082 0.103 2.271 Deficit 0.071 0.044 3.132 Good 0.065 0.077 2.561 Weak 0.070 0.070 2.656 Help 0.061 0.074 2.603 Limited 0.063 0.062 2.774 Recovery 0.056 0.058 2.850 Concerns 0.063 0.067 2.705 Highest 0.044 0.053 2.935 Decline 0.050 0.052 2.960 Agreement 0.043 0.042 3.179 Weaker 0.048 0.049 3.007 Assets 0.042 0.042 3.159 Poor 0.047 0.049 3.017 Positive 0.041 0.051 2.973 Unemployment 0.045 0.030 3.493 Better 0.041 0.053 2.932 Lost 0.045 0.048 3.034 Gained 0.041 0.049 3.007 Fears 0.041 0.045 3.109 Boost 0.040 0.054 2.914 Dropped 0.040 0.045 3.095 Leading 0.039 0.052 2.957 Slow 0.039 0.042 3.162 Confidence 0.036 0.039 3.255 Negative 0.039 0.040 3.225 Gain 0.035 0.042 3.159 Problems 0.037 0.039 3.233 Agreed 0.034 0.042 3.179 Worries 0.037 0.040 3.210 Stronger 0.032 0.042 3.172 Hard 0.036 0.039 3.234 Worth 0.032 0.039 3.239 Recession 0.035 0.032 3.457 Opening 0.032 0.041 3.199 Loss 0.033 0.032 3.441 Note: This table presents the most frequent positive (negative) words in our corpus. For each panel, the first column reports the number of occurrences of each positive (negative) words relative to all occurrences of positive (negative) words, the second reports the fraction of articles in which the word appears, and the third column reports its inverse document frequency (IDF), which is defined below. Source: Words lists come from Loughran and Mcdonald (2011) and Young and Soroka (2012). News articles come from Factiva.com. 4.3.2 Country-specific news sentiment indicator Next, we define the sentiment of an article \\(j\\) as: \\[ s_{j} = \\dfrac{\\sum_{i} w_{ij} p_{ij} - \\sum_{i} w_{ij} n_{ij}}{\\sum_{i} w_{ij} t_{ij}} \\] where \\(p_{ij}\\) is the number of occurrences of positive word \\(i\\) in article \\(j\\), \\(n_{ij}\\) is the number of occurrences of negative word \\(i\\) in article \\(j\\), \\(t_{ij}\\) is the number of occurrences of word \\(i\\) in article \\(j\\), and \\(w_{ij}\\) is the weight associated with word \\(i\\) in article \\(j\\). In our baseline estimates, we take \\(w_{ij} = 1\\), allowing each word to contribute to the sentiment measure proportionally to its frequency of occurrence. In a robustness check, we let each word contribute to the sentiment measure proportionally to its Term Frequency-Inverse Document Frequency (TF-IDF, Manning et al. 2008) by taking: \\[ w_{ij} = \\log\\left(\\frac{N}{N_{i}}\\right) \\] where \\(N\\) is the number of articles in the corpus and \\(N_{i}\\) is the number of articles in which word \\(i\\) is present. Hence, this weighting smoothes out differences in word frequency naturally occurring in the English language by giving more weight to words that appear more rarely across documents. It is well established that the distribution of words in the English language follows a power law. For a broader discussion on power laws in Economics, see Gabaix 2016. def get_counts(idx,data=df[&#39;full_text&#39;],tones=list(tone2keywords)): # Split into words and remove non-letter characters tokens = re.sub(&quot;[^a-zA-Z]&quot;,&quot; &quot;, data.loc[idx].lower()).split() # Return Words and Their Count counter = collections.Counter(tokens) # Word Count T = sum(counter.values()) values = [T] index = [&#39;# words&#39;] for tone in sorted(tones): # Tonal Words In the Text words = list(set(counter.keys())&amp;set(tone2keywords[tone].keys())) if words: # Tonal Words Counts counts = itemgetter(*words)(counter) # Tonal Words IDFs idfs = itemgetter(*words)(tone2keywords[tone]) if len(words) &gt; 1: tf = sum(counts)/T else: tf = counts/T tfidf = np.dot(counts,idfs)/T else: tf = 0 tfidf = 0 values.append(tf) index.append(&#39;% &#39;+tone) values.append(tfidf) index.append(&#39;% &#39;+tone+&#39; tfidf&#39;) return pd.Series(values,index=index,name=idx) To illustrate our sentiment measure, we show the example of an article in which tonal words are highlighted in bold, indicating that although our sentiment measure does not capture all of the nuances in the text, it provides a good indication of its overall tone. This article contains the tags Argentina and Economic News: Title: Argentina’s Peronists defend Menem’s labor reforms. Timestamp: 1996-09-02 Text: BUENOS AIRES, Sept 2 (Reuters)-The Argentine government Monday tried to counter criticisms of President Carlos Menem’s proposals for more flexible labor laws, arguing that not just workers would contribute to new unemployment insurance. Menem angered trade unions, already in disagreement over his fiscal austerity programs, by announcing a labor reform package Friday including suspending collective wage deals and replacing redundancy payouts with unemployment insurance. Topics: Labor/Personnel Issues, Corporate/Industrial News, Economic/Monetary Policy, Economic News, Political/General News, Labor Issues, Domestic Politics. Locations: Argentina, Latin America, South America. Next, we compute a daily sentiment index for each country by taking the average sentiment across articles that are tagged with the country’s name. Finally, we normalize each country sentiment index by computing its z-score. insheet using /// &quot;..\\..\\replication\\data\\sentiments-panel-reuters-econ-fin-mkt-25-countries-1991-2019.csv&quot; /// , comma clear ren v1 country gen sentiment_country = positive - negative gen n_articles_country = 1 if words!=0 keep country year month day sentiment_country n_articles collapse (mean) sentiment* (sum) n_articles*, by(country year month day) compress gen date = mdy(month, day, year) format date %tdnn/dd/CCYY gen iso2 = &quot;&quot;, after(country) replace iso2 = &quot;AR&quot; if country==&quot;Argentina&quot; replace iso2 = &quot;BR&quot; if country==&quot;Brazil&quot; replace iso2 = &quot;CL&quot; if country==&quot;Chile&quot; replace iso2 = &quot;CN&quot; if country==&quot;China&quot; replace iso2 = &quot;FR&quot; if country==&quot;France&quot; replace iso2 = &quot;DE&quot; if country==&quot;Germany&quot; replace iso2 = &quot;GR&quot; if country==&quot;Greece&quot; replace iso2 = &quot;IN&quot; if country==&quot;India&quot; replace iso2 = &quot;ID&quot; if country==&quot;Indonesia&quot; replace iso2 = &quot;IE&quot; if country==&quot;Ireland&quot; replace iso2 = &quot;IT&quot; if country==&quot;Italy&quot; replace iso2 = &quot;JP&quot; if country==&quot;Japan&quot; replace iso2 = &quot;KR&quot; if country==&quot;Korea&quot; replace iso2 = &quot;MY&quot; if country==&quot;Malaysia&quot; replace iso2 = &quot;MX&quot; if country==&quot;Mexico&quot; replace iso2 = &quot;PE&quot; if country==&quot;Peru&quot; replace iso2 = &quot;PH&quot; if country==&quot;Philippines&quot; replace iso2 = &quot;PL&quot; if country==&quot;Poland&quot; replace iso2 = &quot;PT&quot; if country==&quot;Portugal&quot; replace iso2 = &quot;RU&quot; if country==&quot;Russia&quot; replace iso2 = &quot;ZA&quot; if country==&quot;South Africa&quot; replace iso2 = &quot;KR&quot; if country==&quot;South Korea&quot; replace iso2 = &quot;ES&quot; if country==&quot;Spain&quot; replace iso2 = &quot;TH&quot; if country==&quot;Thailand&quot; replace iso2 = &quot;TR&quot; if country==&quot;Turkey&quot; replace iso2 = &quot;US&quot; if country==&quot;United States&quot; tab iso2, m * Convert to z-scores within each country ds sentiment* foreach vv in `r(varlist)&#39; { bys iso2: egen _mean = mean(`vv&#39;) bys iso2: egen _sd = sd(`vv&#39;) replace `vv&#39; = (`vv&#39; - _mean) / _sd drop _mean _sd } compress d save &quot;..\\indicator\\sentiment_country.dta&quot;, replace 4.4 Stylized facts The following table reports standard statistics about the cross country (raw) sentiment indices: Summary statistics of the sentiment index by country. Country Variable N Mean SD Min p25 Median p75 Max AR Sentiment 6485 -0.0064 0.0218 -0.1463 -0.0174 -0.0042 0.0064 0.1250 BR Sentiment 6950 -0.0058 0.0186 -0.1908 -0.0154 -0.0044 0.0047 0.0952 CL Sentiment 5695 -0.0041 0.0276 -0.1405 -0.0200 -0.0022 0.0130 0.1250 CN Sentiment 8604 -0.0022 0.0150 -0.1200 -0.0095 -0.0014 0.0055 0.1512 DE Sentiment 8365 -0.0056 0.0167 -0.1667 -0.0138 -0.0046 0.0035 0.1277 ES Sentiment 6415 -0.0044 0.0253 -0.1547 -0.0179 -0.0018 0.0107 0.1554 FR Sentiment 7573 -0.0054 0.0194 -0.1429 -0.0148 -0.0039 0.0051 0.1549 GR Sentiment 5989 -0.0049 0.0266 -0.1493 -0.0193 -0.0014 0.0112 0.1270 ID Sentiment 7446 -0.0033 0.0199 -0.1333 -0.0134 -0.0030 0.0074 0.1091 IE Sentiment 4411 -0.0023 0.0284 -0.1695 -0.0181 0.0000 0.0143 0.1346 IN Sentiment 8493 0.0034 0.0195 -0.1713 -0.0039 0.0031 0.0112 0.1493 IT Sentiment 7140 -0.0051 0.0232 -0.1538 -0.0169 -0.0031 0.0085 0.1268 JP Sentiment 8345 -0.0039 0.0147 -0.1304 -0.0113 -0.0035 0.0037 0.1795 KR Sentiment 7776 -0.0040 0.0203 -0.1379 -0.0140 -0.0023 0.0074 0.1148 MX Sentiment 6788 -0.0028 0.0243 -0.1591 -0.0154 -0.0009 0.0114 0.1034 MY Sentiment 7506 -0.0043 0.0153 -0.1692 -0.0114 -0.0034 0.0037 0.0833 PE Sentiment 4509 -0.0016 0.0280 -0.2295 -0.0158 0.0000 0.0141 0.1505 PH Sentiment 6747 -0.0039 0.0232 -0.2041 -0.0164 -0.0029 0.0088 0.1250 PL Sentiment 6296 -0.0017 0.0219 -0.1442 -0.0126 0.0000 0.0106 0.2000 PT Sentiment 4649 -0.0039 0.0271 -0.2353 -0.0182 -0.0031 0.0121 0.1091 RU Sentiment 7055 -0.0043 0.0178 -0.1411 -0.0133 -0.0026 0.0056 0.1458 TH Sentiment 7464 -0.0031 0.0192 -0.1352 -0.0123 -0.0023 0.0069 0.1385 TR Sentiment 6355 -0.0020 0.0246 -0.2857 -0.0152 0.0000 0.0133 0.1290 US Sentiment 8825 -0.0030 0.0124 -0.1210 -0.0080 -0.0016 0.0033 0.0896 ZA Sentiment 6809 -0.0067 0.0233 -0.1707 -0.0193 -0.0055 0.0072 0.1004 AE Sentiment 61,712 -0.0043 0.0211 -0.2353 -0.0141 -0.0028 0.0062 0.1795 EM Sentiment 110,978 -0.0032 0.0213 -0.2857 -0.0135 -0.0018 0.0081 0.2000 Total Sentiment 172,690 -0.0036 0.0212 -0.2857 -0.0138 -0.0022 0.0074 0.2000 Note: This table reports summary statistics about the (raw) daily sentiment indices for each country. Sentiment denotes our baseline measure of news sentiment. The AE and EM rows report summary statistics by country groups; the Total row reports summary statistics across all countries in our sample. Overall, we find that the the mean and the median of news indices are negative, reflecting the fact that our dictionary of tonal words contains more negative words. We also find very little difference in the properties of the indices across advanced and emerging markets, or among emerging markets. Unsurprisingly, we also find that the news sentiment index takes values that are, on average, higher in “good” times. The latter is true whether we define good times using real variables (e.g. when GDP growth is positive) or financial (strong equity market performance). Figure 4.1: Behaviour of the news sentiment index in good vs. bad times. Note: This figure reports the average and median values of the (raw) sentiment index using various definitions of good and bad times. The first two columns split the sample based on periods where a country’s real GDP growth has been positive (or negative). The third and fourth columns split the sample based on each country’s stock market index being above or below its trend, where the trend is constructed using a two-sided HP filter with a smoothing parameter of 129,600, set using the Ravn and Uhlig (2002) rule for monthly data. The last two categories split the sample based on whether the country’s daily stock market return has been positive or negative. To give more intuition about the index, we also report the behaviour of the news sentiment index around key economic and financial events. We first plot the daily news sentiment index around equity crisis (left) and booms (right); where we define crisis or booms as daily variations of the local equity index greater than 3 standard deviation (on either side). As expected, we find that news sentiment collapses (or improves drastically) around equity busts (booms). Figure 4.2: Sentiment indices around equity crisis (left) and booms (left) We also zoom in on specific events that occurred in advanced and emerging markets. We report the behaviour of the US sentiment index around September 15, 2018 when Lehman brothers filed for Chapter 11 bankruptcy (left), and the behaviour of the China sentiment index around June 13, 2015 when the China’s stock market bubble popped (right). In both cases, we also find that the news sentiment index tracks closely those events and reacts as expected. Figure 4.3: Sentiment indices around the U.S. Lehman Brothers’ failure (Left) and China stock market crash (right) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
