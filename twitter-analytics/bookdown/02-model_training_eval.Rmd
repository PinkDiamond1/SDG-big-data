# Finetuning BERT-based models

In the past few years, pretrained language models have revolutionized the field of NLP by achieving state-of-the-art results in a variety of natural language understanding tasks (Peters et al., 2018; Devlin et al., 2019). These models improve on existing word embedding methods, such as Word2Vec (Mikolov et al., 2013), by learning stable embedding representations from massive text corpora.

One of the models leading this revolution is the Bidirectional Encoder Representations from Transformers model (BERT, Devlin et al., 2019), which allowed for bi-directionality, through masked language modeling, and leveraged self-attention through its Transformer-based architecture (Vaswani et al., 2017). The model was pretrained on BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2.5B words) using two unsupervised tasks, namely masked language modeling and next-sentence prediction. This model can later be fine-tuned on a variety of downstream tasks, including text classification, achieving high performance.


In this part, we show how to fine-tune a BERT-based model for tweet classification. To do so, we mostly rely on the Python package `simpletransformers`, built on top of the famous `transformers` Python package developed by Hugging Face.

## Training the model

After loading the set of labelled tweets in a dataframe `df`, we perform a train-test split (70-30) on it:

```{python eval=FALSE}
train_df = df.sample(frac=0.7,random_state=0)
val_df = df.drop(train_df.index).reset_index(drop=True)
```

We then define specific arguments for the fine-tuning such as the batch size `train_batch_size`, the number of epochs `num_train_epochs` or the output path `output_dir`. The models are evaluated at every epoch in terms of AUROC on the test set. The model with the highest AUROC on the test set is considered the best model and saved at `best_model_dir`. We also use early stopping which consists in stopping the training if the AUROC on the test set has not improved after `early_stopping_patience` epochs. A complete list of the training arguments can be found [here](https://simpletransformers.ai/docs/usage/#configuring-a-simple-transformers-model).

```{python eval=FALSE}
    classification_args = { 'train_batch_size': 8,
                            'overwrite_output_dir': True,
                            'evaluate_during_training': True,
                            'save_model_every_epoch': True,
                            'save_eval_checkpoints': True,
                            'output_dir': path_to_store_model,
                            'best_model_dir': path_to_store_best_model,
                            'evaluate_during_training_verbose': True,
                            'num_train_epochs': num_train_epochs,
                            "use_early_stopping": True,
                            "early_stopping_delta": 0,
                            "early_stopping_metric": "auroc",
                            "early_stopping_metric_minimize": False,
                            'early_stopping_patience': 3}
```

Once the classification arguments are defined, we can initiate the fine-tuning. In our case, for English tweet classification, we use a version of BERT that was further pretrained on English tweets by [Deep Pavlov](https://deeppavlov.ai), therefore enhancing the classification performance in the Twitter context. We need to define the `model_name`, which is `bert` in our case but can also be other more sophisticated architectures such as `roberta`. The `model_type` refers to the model name on the [Hugging Face model hub](https://huggingface.co/models). As we only cover binary classification here, the number of labels `num_labels` is set to 2.

```{python eval=FALSE}
from simpletransformers.classification import ClassificationModel
model = ClassificationModel(model_name='bert',
                            model_type='DeepPavlov/bert-base-cased-conversational',
                            num_labels=2,
                            use_cuda=True,
                            args=classification_args)
```

Once the model has been loaded, we can launch the fine-tuning:

```{python eval=FALSE}
model.train_model(train_df=train_df, eval_df=eval_df, output_dir=path_to_store_model)
```

The training is now launched. When it is finished, we can use the best model in terms of AUROC on the test set and evaluate it on a bigger random set of tweets.

## Evaluation on the random set

While evaluating the model on the test set is useful to determine which classifier is best suited for the task, this performance might not be representative of the performance on a random set of tweets as the imbalance on the latter is much more extreme than on the test set. In this case, we needed to find a way to evaluate our classifiers in real-world settings. To do so, we used the best model from our training iterations to infer the confidences scores of each tweet out of a large random sample of 100 million tweets. We then ranked the tweets based on their confidence score in a descending way and sampled tweets along the confidence score distribution, overweighting the top of the distribution. In total, we sampled 200 tweets out of the 100 million random sample with ranks ranging from 1 to 1 million. The function used to do this sampling is as follows:

```{python eval=FALSE}
def get_sampled_indices(n_sample=10, n_cutoff=6):
    sampled_points = []  # index of scores around which we sample n_sample tweets
    sampled_ranks = []  # ranks of sampled tweets
    for point, rank in enumerate(sorted(set([int(x) for i in range(n_cutoff) for x in np.logspace(i, i + 1, i + 1)]))):
        if not point:
            new_ranks = list(range(rank, rank + n_sample))
        else:
            new_ranks = list(range(rank + 1, rank + n_sample + 1))
        print('Index of sampled point:', point)
        print('Sampled ranks:', new_ranks)
        sampled_points.extend([point] * n_sample)
        sampled_ranks.extend(new_ranks)
    return sampled_points, sampled_ranks
```
We then labelled these sampled tweets, which allowed us to get the percentage of positive tweets for given points in the rank distribution. Below is an example of an evaluation plot for the class `job_search` with the tweet rank as x-axis and the percentage of positive tweets for the class `job_search` as y-axis.

![Precision (y-axis) as a function of tweet rank based on confidence score i.e. positive label probability output by the model (x-axis).](figures/job_search.png)

## Active learning

While determining an initial list of n-grams to build a labelled set from is a good strategy to get a decent classification performance, it is unlikely to capture all the linguistic subtleties humans use on social media to talk about their labor market situation. In this case, we ideally need to label more tweets and need to determine which are the most informative to respect our labeling budget constraint.

When faced with a classification task in which the minority class is extremely rare, active learning allows to minimize the number of labels required for a classifier to achieve good performance. At each iteration, a trained model is used to query new samples expected to lead to high improvements in validation accuracy. These samples are, in turn, labeled by humans and then used for training in the next iteration of the model. There are two main approaches to identifying the most informative samples: uncertainty sampling (Lewis and Gale, 1994) and diversity sampling (Cohn et al., 1994), which have been coined as the ``two faces of active learning''(Dasgupta, 2011). While uncertainty sampling defines the most informative samples as the ones the model is the most uncertain about (e.g. in a binary context, this boundary is at 0.5), diversity sampling consists of selecting examples to label from different homogeneous clusters of the feature space.

Here, we provide an example on how to select the most informative tweets to label using uncertainty sampling on uncalibrated BERT confidence scores. In this setting, we sample tweets with BERT confidence scores around 0.5. Having the data stored in a dataframe `df` containing the confidence scores in the `score` column, we create a `modified_score` which is the confidence score minus 0.5. We then select 50 tweets with the smallest positive `modified_score` and 50 tweets with the highest negative `modified_score`.

```{python eval=FALSE}
df['modified_score'] = df['score'] - 0.5
above_threshold_df = df.loc[df['modified_score'] > 0].nsmallest(50, 'modified_score')
below_threshold_df = df.loc[df['modified_score'] < 0].nlargest(50, 'modified_score')
sample_df = pd.concat([above_threshold_df, below_threshold_df]).reset_index(drop=True)
```

In the end, the `sample_df` are sent to labelling. When labelled, the new samples are added to the existing labels and a new train-test split is applied. The training and evaluation then takes place as described earlier.  

